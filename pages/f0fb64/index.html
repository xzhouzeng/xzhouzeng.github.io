<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>动作质量评估 | XZ Blog</title>
    <meta name="generator" content="VuePress 1.9.5">
    <link rel="icon" href="/img/favicon.ico">
    <meta name="description" content="Learning notes and sharing">
    <meta name="keywords" content="后端开发,技术文档,论文解读,AI,个人博客,markdown">
    <meta name="baidu-site-verification" content="7F55weZDDc">
    <meta name="theme-color" content="#11a8cd">
    
    <link rel="preload" href="/assets/css/0.styles.b7b742a8.css" as="style"><link rel="preload" href="/assets/js/app.9749b4f6.js" as="script"><link rel="preload" href="/assets/js/4.5da4c663.js" as="script"><link rel="preload" href="/assets/js/6.2484af9c.js" as="script"><link rel="prefetch" href="/assets/js/10.e0104b9a.js"><link rel="prefetch" href="/assets/js/11.63bdaef5.js"><link rel="prefetch" href="/assets/js/12.25c3ab74.js"><link rel="prefetch" href="/assets/js/13.248068ad.js"><link rel="prefetch" href="/assets/js/14.f4e65a02.js"><link rel="prefetch" href="/assets/js/15.3efbd5c7.js"><link rel="prefetch" href="/assets/js/16.a996b679.js"><link rel="prefetch" href="/assets/js/17.b929c3be.js"><link rel="prefetch" href="/assets/js/18.f56edfb3.js"><link rel="prefetch" href="/assets/js/19.42d6d76e.js"><link rel="prefetch" href="/assets/js/2.85cbe54a.js"><link rel="prefetch" href="/assets/js/20.4105fdfa.js"><link rel="prefetch" href="/assets/js/21.9f4fbcc5.js"><link rel="prefetch" href="/assets/js/22.0d2b66df.js"><link rel="prefetch" href="/assets/js/23.5201c1d6.js"><link rel="prefetch" href="/assets/js/24.ca69a44f.js"><link rel="prefetch" href="/assets/js/25.4a9fa551.js"><link rel="prefetch" href="/assets/js/26.355da810.js"><link rel="prefetch" href="/assets/js/27.1c48e68c.js"><link rel="prefetch" href="/assets/js/28.6f08e3bc.js"><link rel="prefetch" href="/assets/js/29.4c6ebaf4.js"><link rel="prefetch" href="/assets/js/3.cbaeda42.js"><link rel="prefetch" href="/assets/js/30.54b4d1bf.js"><link rel="prefetch" href="/assets/js/31.47c1a1ac.js"><link rel="prefetch" href="/assets/js/32.2c62c259.js"><link rel="prefetch" href="/assets/js/33.2099844d.js"><link rel="prefetch" href="/assets/js/34.4522b286.js"><link rel="prefetch" href="/assets/js/35.0f453b5e.js"><link rel="prefetch" href="/assets/js/36.04de70cf.js"><link rel="prefetch" href="/assets/js/37.306cc701.js"><link rel="prefetch" href="/assets/js/38.2f2b954f.js"><link rel="prefetch" href="/assets/js/39.a68ff272.js"><link rel="prefetch" href="/assets/js/40.ef51d7b3.js"><link rel="prefetch" href="/assets/js/41.8afb9f7b.js"><link rel="prefetch" href="/assets/js/42.a36d7260.js"><link rel="prefetch" href="/assets/js/43.760640e8.js"><link rel="prefetch" href="/assets/js/44.4d7de229.js"><link rel="prefetch" href="/assets/js/45.3d4a4655.js"><link rel="prefetch" href="/assets/js/46.1f8d2647.js"><link rel="prefetch" href="/assets/js/47.6a66d324.js"><link rel="prefetch" href="/assets/js/48.fc036730.js"><link rel="prefetch" href="/assets/js/49.cae162d4.js"><link rel="prefetch" href="/assets/js/5.11d967e7.js"><link rel="prefetch" href="/assets/js/50.7ea76738.js"><link rel="prefetch" href="/assets/js/51.9625347d.js"><link rel="prefetch" href="/assets/js/52.69c0feed.js"><link rel="prefetch" href="/assets/js/53.499bf52f.js"><link rel="prefetch" href="/assets/js/54.dd1da291.js"><link rel="prefetch" href="/assets/js/55.f079715b.js"><link rel="prefetch" href="/assets/js/56.2bb9ddbe.js"><link rel="prefetch" href="/assets/js/57.5004934c.js"><link rel="prefetch" href="/assets/js/58.3a179660.js"><link rel="prefetch" href="/assets/js/59.b72a3457.js"><link rel="prefetch" href="/assets/js/60.a3b13775.js"><link rel="prefetch" href="/assets/js/61.ec0c69b2.js"><link rel="prefetch" href="/assets/js/62.32e82d5c.js"><link rel="prefetch" href="/assets/js/63.2a6bb04f.js"><link rel="prefetch" href="/assets/js/64.bc571e83.js"><link rel="prefetch" href="/assets/js/65.a44199ce.js"><link rel="prefetch" href="/assets/js/66.2a054dfd.js"><link rel="prefetch" href="/assets/js/67.79b393ac.js"><link rel="prefetch" href="/assets/js/68.a63686a0.js"><link rel="prefetch" href="/assets/js/69.cb9f18aa.js"><link rel="prefetch" href="/assets/js/7.38e3dd4a.js"><link rel="prefetch" href="/assets/js/70.55002538.js"><link rel="prefetch" href="/assets/js/71.99c1bbec.js"><link rel="prefetch" href="/assets/js/72.60daec4d.js"><link rel="prefetch" href="/assets/js/8.21fcb95f.js"><link rel="prefetch" href="/assets/js/9.3f132d53.js">
    <link rel="stylesheet" href="/assets/css/0.styles.b7b742a8.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="/img/logo.png" alt="XZ Blog" class="logo"> <span class="site-name can-hide">XZ Blog</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link">首页</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="文档" class="dropdown-title"><a href="/doc/" class="link-title">文档</a> <span class="title" style="display:none;">文档</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/pages/b406c0/" class="nav-link">人体姿态估计</a></li><li class="dropdown-item"><!----> <a href="/pages/1186a5/" class="nav-link">2D-3D-Lifting</a></li><li class="dropdown-item"><!----> <a href="/pages/f0fb64/" aria-current="page" class="nav-link router-link-exact-active router-link-active">动作质量评估</a></li><li class="dropdown-item"><!----> <a href="/pages/1aa734/" class="nav-link">基于RGBD视觉信息的异常行为识别</a></li><li class="dropdown-item"><!----> <a href="/pages/ce88a1/" class="nav-link">基于RGB视频的行为识别</a></li><li class="dropdown-item"><!----> <a href="/pages/0dsfd5/" class="nav-link">大模型应用</a></li><li class="dropdown-item"><h4>网络结构</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/pages/xxx/" class="nav-link">Transformer</a></li><li class="dropdown-subitem"><a href="/pages/xxx/" class="nav-link">GCN</a></li><li class="dropdown-subitem"><a href="/pages/0e6e45/" class="nav-link">Graph Transformers</a></li><li class="dropdown-subitem"><a href="/pages/413fa2/" class="nav-link">Diffusion Model</a></li></ul></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="算法" class="dropdown-title"><a href="/ai/" class="link-title">算法</a> <span class="title" style="display:none;">算法</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/pages/e4e08c/" class="nav-link">深度学习</a></li><li class="dropdown-item"><!----> <a href="/pages/61f973/" class="nav-link">论文解读</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="技术" class="dropdown-title"><a href="/technology/" class="link-title">技术</a> <span class="title" style="display:none;">技术</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/pages/t6068a3/" class="nav-link">后端开发</a></li><li class="dropdown-item"><!----> <a href="/pages/1fa324/" class="nav-link">Git</a></li><li class="dropdown-item"><!----> <a href="/pages/bed179/" class="nav-link">博客搭建</a></li><li class="dropdown-item"><!----> <a href="/pages/f56caf/" class="nav-link">Debug</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="更多" class="dropdown-title"><a href="/more/" class="link-title">更多</a> <span class="title" style="display:none;">更多</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/pages/1db558/" class="nav-link">面试</a></li><li class="dropdown-item"><!----> <a href="/pages/434772/" class="nav-link">实用技巧</a></li><li class="dropdown-item"><!----> <a href="/friends/" class="nav-link">友情链接</a></li></ul></div></div><div class="nav-item"><a href="/about/" class="nav-link">关于</a></div><div class="nav-item"><a href="/pages/5c4f4b/" class="nav-link">收藏</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="索引" class="dropdown-title"><a href="/archives/" class="link-title">索引</a> <span class="title" style="display:none;">索引</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/categories/" class="nav-link">分类</a></li><li class="dropdown-item"><!----> <a href="/tags/" class="nav-link">标签</a></li><li class="dropdown-item"><!----> <a href="/archives/" class="nav-link">归档</a></li></ul></div></div> <a href="https://github.com/xzhouzeng/vuepress-theme-vdoing" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><div class="blogger"><img src="/img/head.jpg"> <div class="blogger-info"><h3>xzhouzeng</h3> <span>@渐行。</span></div></div> <nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link">首页</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="文档" class="dropdown-title"><a href="/doc/" class="link-title">文档</a> <span class="title" style="display:none;">文档</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/pages/b406c0/" class="nav-link">人体姿态估计</a></li><li class="dropdown-item"><!----> <a href="/pages/1186a5/" class="nav-link">2D-3D-Lifting</a></li><li class="dropdown-item"><!----> <a href="/pages/f0fb64/" aria-current="page" class="nav-link router-link-exact-active router-link-active">动作质量评估</a></li><li class="dropdown-item"><!----> <a href="/pages/1aa734/" class="nav-link">基于RGBD视觉信息的异常行为识别</a></li><li class="dropdown-item"><!----> <a href="/pages/ce88a1/" class="nav-link">基于RGB视频的行为识别</a></li><li class="dropdown-item"><!----> <a href="/pages/0dsfd5/" class="nav-link">大模型应用</a></li><li class="dropdown-item"><h4>网络结构</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/pages/xxx/" class="nav-link">Transformer</a></li><li class="dropdown-subitem"><a href="/pages/xxx/" class="nav-link">GCN</a></li><li class="dropdown-subitem"><a href="/pages/0e6e45/" class="nav-link">Graph Transformers</a></li><li class="dropdown-subitem"><a href="/pages/413fa2/" class="nav-link">Diffusion Model</a></li></ul></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="算法" class="dropdown-title"><a href="/ai/" class="link-title">算法</a> <span class="title" style="display:none;">算法</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/pages/e4e08c/" class="nav-link">深度学习</a></li><li class="dropdown-item"><!----> <a href="/pages/61f973/" class="nav-link">论文解读</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="技术" class="dropdown-title"><a href="/technology/" class="link-title">技术</a> <span class="title" style="display:none;">技术</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/pages/t6068a3/" class="nav-link">后端开发</a></li><li class="dropdown-item"><!----> <a href="/pages/1fa324/" class="nav-link">Git</a></li><li class="dropdown-item"><!----> <a href="/pages/bed179/" class="nav-link">博客搭建</a></li><li class="dropdown-item"><!----> <a href="/pages/f56caf/" class="nav-link">Debug</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="更多" class="dropdown-title"><a href="/more/" class="link-title">更多</a> <span class="title" style="display:none;">更多</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/pages/1db558/" class="nav-link">面试</a></li><li class="dropdown-item"><!----> <a href="/pages/434772/" class="nav-link">实用技巧</a></li><li class="dropdown-item"><!----> <a href="/friends/" class="nav-link">友情链接</a></li></ul></div></div><div class="nav-item"><a href="/about/" class="nav-link">关于</a></div><div class="nav-item"><a href="/pages/5c4f4b/" class="nav-link">收藏</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="索引" class="dropdown-title"><a href="/archives/" class="link-title">索引</a> <span class="title" style="display:none;">索引</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/categories/" class="nav-link">分类</a></li><li class="dropdown-item"><!----> <a href="/tags/" class="nav-link">标签</a></li><li class="dropdown-item"><!----> <a href="/archives/" class="nav-link">归档</a></li></ul></div></div> <a href="https://github.com/xzhouzeng/vuepress-theme-vdoing" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>人体姿态估计</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>动作质量评估</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/pages/f0fb64/" aria-current="page" class="active sidebar-link">动作质量评估</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/pages/f0fb64/#_1-综述" class="sidebar-link">1. 综述</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level4"><a href="/pages/f0fb64/#a-survey-of-video-based-action-quality-assessment-2021-insai" class="sidebar-link">A Survey of Video-based Action Quality Assessment（2021 INSAI）</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/pages/f0fb64/#_2-相关工作" class="sidebar-link">2. 相关工作</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/pages/f0fb64/#_2-1-基于人体骨骼" class="sidebar-link">2.1 基于人体骨骼</a></li><li class="sidebar-sub-header level4"><a href="/pages/f0fb64/#assessing-the-quality-of-actions-2014-eccv" class="sidebar-link">Assessing the quality of actions（2014 ECCV）</a></li><li class="sidebar-sub-header level4"><a href="/pages/f0fb64/#dynamical-regularity-for-action-analysis-2015-bmva" class="sidebar-link">Dynamical Regularity for Action Analysis（2015 BMVA）</a></li><li class="sidebar-sub-header level4"><a href="/pages/f0fb64/#skeleton-free-body-pose-estimation-from-depth-images-for-movement-analysis-2015-iccvw" class="sidebar-link">Skeleton-Free Body Pose Estimation from Depth Images for Movement Analysis（2015 ICCVW）</a></li><li class="sidebar-sub-header level4"><a href="/pages/f0fb64/#a-comparative-study-of-pose-representation-and-dynamics-modelling-for-online-motion-quality-assessment-2016-cviu" class="sidebar-link">A comparative study of pose representation and dynamics modelling for online motion quality assessment（2016 CVIU）</a></li><li class="sidebar-sub-header level4"><a href="/pages/f0fb64/#cognilearn-a-deep-learning-based-interface-for-cognitive-behavior-assessment-2017-iui" class="sidebar-link">CogniLearn: A Deep Learning-based Interface for Cognitive Behavior Assessment（2017 IUI）</a></li><li class="sidebar-sub-header level4"><a href="/pages/f0fb64/#a-mal-automatic-motion-assessment-learning-from-properly-performed-motions-in-3d-skeleton-videos-2019-iccvw" class="sidebar-link">A-MAL: Automatic Motion Assessment Learning from Properly Performed Motions in 3D Skeleton Videos（2019 ICCVW）</a></li><li class="sidebar-sub-header level4"><a href="/pages/f0fb64/#action-assessment-by-joint-relation-graphs-2019-iccv" class="sidebar-link">Action Assessment by Joint Relation Graphs（2019 ICCV）</a></li><li class="sidebar-sub-header level4"><a href="/pages/f0fb64/#efficient-and-robust-skeleton-based-quality-assessment-and-abnormality-detection-in-human-action-performance-2020-jbhi" class="sidebar-link">Efficient and Robust Skeleton-Based Quality Assessment and Abnormality Detection in Human Action Performance（2020 JBHI）</a></li><li class="sidebar-sub-header level3"><a href="/pages/f0fb64/#_2-2-基于rgb视频流" class="sidebar-link">2.2 基于RGB视频流</a></li><li class="sidebar-sub-header level4"><a href="/pages/f0fb64/#learning-to-score-olympic-events-2017-cvprw" class="sidebar-link">Learning to Score Olympic Events（2017 CVPRW）</a></li><li class="sidebar-sub-header level4"><a href="/pages/f0fb64/#scoringnet-learning-key-fragment-for-action-quality-assessment-with-ranking-loss-in-skilled-sports-2018-accv" class="sidebar-link">ScoringNet: Learning Key Fragment for Action Quality Assessment with Ranking Loss in Skilled Sports（2018 ACCV）</a></li><li class="sidebar-sub-header level4"><a href="/pages/f0fb64/#s3d-stacking-segmental-p3d-for-action-quality-assessment-2018-icip" class="sidebar-link">S3D: Stacking Segmental P3D for Action Quality Assessment（2018 ICIP）</a></li><li class="sidebar-sub-header level4"><a href="/pages/f0fb64/#end-to-end-learning-for-action-quality-assessment-2018-pcm" class="sidebar-link">End-To-End Learning for Action Quality Assessment（2018 PCM）</a></li><li class="sidebar-sub-header level4"><a href="/pages/f0fb64/#action-quality-assessment-across-multiple-actions-wacv-2019" class="sidebar-link">Action Quality Assessment Across Multiple Actions（WACV 2019）</a></li><li class="sidebar-sub-header level4"><a href="/pages/f0fb64/#manipulation-skill-assessment-from-videos-with-spatial-attention-network-2019-iccvw" class="sidebar-link">Manipulation-Skill Assessment from Videos with Spatial Attention Network（2019 ICCVW）</a></li><li class="sidebar-sub-header level4"><a href="/pages/f0fb64/#what-and-how-well-you-performed-a-multitask-learning-approach-to-action-quality-assessment-2019-cvpr" class="sidebar-link">What and How Well You Performed? A Multitask Learning Approach to Action Quality Assessment（2019 CVPR）</a></li><li class="sidebar-sub-header level4"><a href="/pages/f0fb64/#the-pros-and-cons-rank-aware-temporal-attention-for-skill-determination-in-long-videos-2019-cvpr" class="sidebar-link">The Pros and Cons: Rank-Aware Temporal Attention for Skill Determination in Long Videos（2019 CVPR）</a></li><li class="sidebar-sub-header level4"><a href="/pages/f0fb64/#a-deep-learning-framework-for-assessing-physical-rehabilitation-exercises-2020-tnsre" class="sidebar-link">A Deep Learning Framework for Assessing Physical Rehabilitation Exercises（2020 TNSRE）</a></li><li class="sidebar-sub-header level4"><a href="/pages/f0fb64/#usdl-2020-cvpr" class="sidebar-link">USDL（2020 CVPR）</a></li><li class="sidebar-sub-header level4"><a href="/pages/f0fb64/#hybrid-dynamic-static-context-aware-attention-network-for-action-assessment-in-long-videos-2020-acm-mm" class="sidebar-link">Hybrid Dynamic-static Context-aware Attention Network for Action Assessment in Long Videos（2020 ACM MM）</a></li><li class="sidebar-sub-header level4"><a href="/pages/f0fb64/#learning-to-score-figure-skating-sport-videos-2020-tcsvt" class="sidebar-link">Learning to Score Figure Skating Sport Videos（2020 TCSVT）</a></li><li class="sidebar-sub-header level4"><a href="/pages/f0fb64/#an-asymmetric-modeling-for-action-assessment-2020-eccv" class="sidebar-link">An asymmetric modeling for action assessment（2020 ECCV）</a></li><li class="sidebar-sub-header level4"><a href="/pages/f0fb64/#tsa-net-tube-self-attention-network-for-action-quality-assessment-2021-acm-mm" class="sidebar-link">TSA-Net: Tube Self-Attention Network for Action Quality Assessment（2021 ACM MM）</a></li><li class="sidebar-sub-header level4"><a href="/pages/f0fb64/#auto-encoding-score-distribution-regression-for-action-quality-assessment-2021" class="sidebar-link">Auto-Encoding Score Distribution Regression for Action Quality Assessment（2021）</a></li><li class="sidebar-sub-header level4"><a href="/pages/f0fb64/#core-2021-iccv" class="sidebar-link">CoRe（2021 ICCV）</a></li><li class="sidebar-sub-header level4"><a href="/pages/f0fb64/#improving-action-quality-assessment-using-weighted-aggregation-2022-ibpria" class="sidebar-link">Improving Action Quality Assessment using Weighted Aggregation（2022 IbPRIA）</a></li><li class="sidebar-sub-header level4"><a href="/pages/f0fb64/#finediving-a-fine-grained-dataset-for-procedure-aware-action-quality-assessment-2022-cvpr-oral" class="sidebar-link">FineDiving: A Fine-grained Dataset for Procedure-aware Action Quality Assessment（2022 CVPR Oral）</a></li></ul></li></ul></li></ul></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>基于骨骼的行为识别</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>基于RGB视频的行为识别</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>大模型应用</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>网络架构</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>视频生成</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>NLP</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>多模态</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>视频理解</span> <span class="arrow right"></span></p> <!----></section></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper "><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/" title="首页" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/doc/#文档" data-v-06225672>文档</a></li><li data-v-06225672><a href="/doc/#动作质量评估" data-v-06225672>动作质量评估</a></li></ul> <div class="info" data-v-06225672><div title="作者" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/xzhouzeng" target="_blank" title="作者" class="beLink" data-v-06225672>xzhouzeng</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2023-05-24</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABKFJREFUSA3tVl1oFVcQnrMbrak3QUgkya1akpJYcrUtIqW1JvFBE9LiQ5v6JmJpolbMg32rVrhgoYK0QiMY6i9Y6EMaW5D+xFJaTYItIuK2Kr3+BJNwkxBj05sQY3b3nM6cs2dv9t7NT/vQJw/sndk5M/PNzJkzewGerP+pAmy+ON8lLzUJgA8ZYxYIYZmGYRnctDaWvJJAmTtfP1pvXsBCCPP8QFcCaRkZYACgDZFO4stNIcBCajEOlmmC9XpJ9bAGCaPaPmzPl32dvLSVu3BWCTQs0XQQ6g0DYgwLIoAZbBCdW/i+781o1VVlm/410mw4h06Y7bIPHNyWDyL4FHkX03Q8SrzNhZTZriieckWt7cL6MM85YcLpsi/7O9/iXFT6MswI0DmmpkSaJ0qLxFIm3+i1THHB3zmBH3PYx9CcykcLOeQVVa7QtdxTgQgEleX2AjHYfwA+2ddV77ruGoJUbhGDI09YSNXyMpUt5ylOzxgbUmtOp7NmbNt8v3arjTBfYELmLUV+M+nSawNNAUqpT3ClJWg5I3BLT+cGW/DXNGCa6tx1aakCGEigArTn4TDIPdrXXYKCZNrHLMCOEPvHBlLQ99s9eHB7EB6NTki73CVPQ2F5MSx/uRQixfmq7rK0wYD8w8E905bnPDfwoWs/rfv93NWN/ZfvwsLIU7A09gxECyISeGJkHAau98L97tuw7NXnoPyNF8FcYGLGKsOs0mN3OEyec9esGW/ZEl945dTP34wlR2FZVQWU1q0Cw8Tr7p+hgLLNL0FPxx/Q35mA8aEUrH6nCgwEl0tn7wUiZYJnNRh6DK4UH/k0lfyrsBKdPVv/AriGIQcEDQZ65LBAGe2Rzui9Ybjz7XUppz1/uKBbyVPGkN3ZAeC6hr0x7Nr38N5+EqkoOm17xpoqR9ohQF55ERSvr4Dkr3chNfC3DMzGJlNBElW8w9nsGQvhNGIzDkXzCg8cLK951xHsFBlTJspJNi3ZFIMF2AeDV3q8DNOB+YHi6QTrChDIWDBRi5U5f+ZMfJLu3ccrqxtdxk4SKH336LFxSmkqefwU5T8fhdSdQf9IVKD6aNiwI/hnmcAZ91isYMJIaCUCx9W098+LgruikeTqzqqxKPUwqJyCPJiyemVVZBOijDGjD38Os0jOiSPL1z3SPjXNANbiNPXAdzTfukjjuknNBbyz3nwgTd3AVFqUJ5hpHlq9MveLnWwttUfoygBmvVjuikxND3znrhsELnZk7k+OjIGxeNEkomyLVta0xxn+HZhjBc4YZ/AFjHjz9u3xRZl2BN4aq9nFwWh16IrQ1aHHEd3j1+4/dB9OtH4e29A2H1DyHQRmOSfQZ1Fy7MHBTGB6J/Djq6p3OxyO2cB+4Car7v/o3GXgfAkj23+x9ID1Teoamo/SXcbvSf2PX7Vc8DdCmE1vN9di+32P9/5YR3vLnhCVGUWBjEkr3yh4H8v9CzmsbdhzOKzsJKM90iFdaTMjRPhGVsakRvOaRidljo6H6G7j+ctrJpsP+4COhDIl0La2+FS4+5mlocBaXY5QnGZysIBYoeSsl5qQzrSj/cgNrfuEzlWBfwA+EjrZyWUvpAAAAABJRU5ErkJggg==">动作质量评估<!----></h1> <!----> <div class="theme-vdoing-content content__default"><h1 id="action-quality-assessment"><a href="#action-quality-assessment" class="header-anchor">#</a> Action Quality Assessment</h1> <h2 id="_1-综述"><a href="#_1-综述" class="header-anchor">#</a> 1. 综述</h2> <h4 id="a-survey-of-video-based-action-quality-assessment-2021-insai"><a href="#a-survey-of-video-based-action-quality-assessment-2021-insai" class="header-anchor">#</a> A Survey of Video-based Action Quality Assessment（2021 INSAI）</h4> <ul><li><p><strong>定义</strong></p> <p><strong>行动质量评估（AQA）旨在评估特定行动的执行情况</strong>，近年来受到了越来越多的关注，因为它在许多现实世界的应用中起着至关重要的作用，包括体育、医疗和其他。</p> <p><strong>与行为识别不同，AQA更具挑战性，因为它需要模型从描述相同动作的视频中预测细粒度分数。</strong></p></li> <li><p><strong>运用领域</strong></p> <p>医疗康复、体育运动（跳水、体操、滑冰等）、日常行为或者技能评估</p></li> <li><p><strong>AQA基本架构</strong></p> <p>AQA框架由视频<strong>特征提取模块和评估模块</strong>组成：</p> <ul><li><p>在特征提取阶段，传统方法首先完成时空关键点的提取和选择，然后采用离散傅立叶变换（DFT）、离散余弦变换（DCT）或线性组合完成特征融合。然而，深度学习方法通常采用深度卷积网络（DCNs）和递归神经网络。</p></li> <li><p>评估模块的形式与评估任务的类型高度相关。</p></li></ul> <img src="/assets/img/image-20220614211002543.ae81358d.png" title="" alt="image-20220614211002543" data-align="center"></li> <li><p><strong>AQA任务分类</strong></p> <ol><li><p><strong>Regression scoring</strong></p> <p>回归评分的形式通常出现在<strong>体育运动</strong>中。由于裁判员在体育赛事AQA数据集中给出视频的地面真实分数，因此可以通过大型体育赛事的广播视频构建数据集，获取难度较小。采用支持向量回归（SVR）模型或全连通网络（FCN）直接完成分数预测。<strong>均方误差（MSE）通常用作回归评分的指标</strong>：</p> <p><img src="https://math.now.sh?from=%7B%5Ccolor%7BBlack%7D%20M%20S%20E%3D%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%5Cleft%28%5Chat%7Bs%7D_%7Bi%7D-s_%7Bi%7D%5Cright%29%5E%7B2%7D%7D%0A"></p></li> <li><p><strong>Grading</strong></p> <p>分级的形式通常出现在<strong>医疗技能操作评估任务</strong>中。操作员的操作将分为特定级别，<strong>如新手、中级和专家</strong>。因此，行动质量评估被转化为一个<strong>分类问题</strong>。通常采用<strong>分类准确度作为度量标准</strong>。</p></li> <li><p><strong>Pairwise sorting</strong></p> <p>成对排序任务从视频库中获取任意两个视频，以评估动作质量。假设视频数据集包含N个视频，那么总共有<img src="https://math.now.sh?inline=C%5E2_n" style="display:inline-block;margin:0;">个组合。标签矩阵M可以根据以下规则构造：</p> <p><img src="https://math.now.sh?from=%7B%5Ccolor%7BBlack%7DM%28i%2C%20j%29%3D%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bll%7D%0A"></p></li></ol></li></ul> <p>-1, &amp; Q\left(S_{i}\right)&lt;Q\left(S_{j}\right) \
0, &amp; Q\left(S_{i}\right)=Q\left(S_{j}\right)
\end{array}\right.}
$$</p> <div class="language- extra-class"><pre><code> 评价指标使用**pairwise accuracy (% of correctly ordered pairs)** 
</code></pre></div><ul><li><p><strong>挑战</strong></p> <ul><li>医疗服务中的动作具有高时间复杂度、语义丰富性和低容错性的特点，这对AQA系统的语义理解能力提出了更高的要求。</li> <li>运动领域的QA任务主要面临身体变形和运动模糊等问题</li> <li>在各个领域都存在一些共有常见的挑战，如效率、视图遮挡、模型可解释性等。</li></ul></li> <li><p><strong>未来工作展望</strong></p> <img title="" src="/assets/img/image-20220614225937628.76f86d6c.png" alt="image-20220614225937628" data-align="center" style="zoom:80%;"></li></ul> <h2 id="_2-相关工作"><a href="#_2-相关工作" class="header-anchor">#</a> 2. 相关工作</h2> <h3 id="_2-1-基于人体骨骼"><a href="#_2-1-基于人体骨骼" class="header-anchor">#</a> 2.1 基于人体骨骼</h3> <h4 id="assessing-the-quality-of-actions-2014-eccv"><a href="#assessing-the-quality-of-actions-2014-eccv" class="header-anchor">#</a> Assessing the quality of actions（2014 ECCV）</h4> <h4 id="dynamical-regularity-for-action-analysis-2015-bmva"><a href="#dynamical-regularity-for-action-analysis-2015-bmva" class="header-anchor">#</a> Dynamical Regularity for Action Analysis（2015 BMVA）</h4> <h4 id="skeleton-free-body-pose-estimation-from-depth-images-for-movement-analysis-2015-iccvw"><a href="#skeleton-free-body-pose-estimation-from-depth-images-for-movement-analysis-2015-iccvw" class="header-anchor">#</a> Skeleton-Free Body Pose Estimation from Depth Images for Movement Analysis（2015 ICCVW）</h4> <h4 id="a-comparative-study-of-pose-representation-and-dynamics-modelling-for-online-motion-quality-assessment-2016-cviu"><a href="#a-comparative-study-of-pose-representation-and-dynamics-modelling-for-online-motion-quality-assessment-2016-cviu" class="header-anchor">#</a> A comparative study of pose representation and dynamics modelling for online motion quality assessment（2016 CVIU）</h4> <h4 id="cognilearn-a-deep-learning-based-interface-for-cognitive-behavior-assessment-2017-iui"><a href="#cognilearn-a-deep-learning-based-interface-for-cognitive-behavior-assessment-2017-iui" class="header-anchor">#</a> CogniLearn: A Deep Learning-based Interface for Cognitive Behavior Assessment（2017 IUI）</h4> <h4 id="a-mal-automatic-motion-assessment-learning-from-properly-performed-motions-in-3d-skeleton-videos-2019-iccvw"><a href="#a-mal-automatic-motion-assessment-learning-from-properly-performed-motions-in-3d-skeleton-videos-2019-iccvw" class="header-anchor">#</a> A-MAL: Automatic Motion Assessment Learning from Properly Performed Motions in 3D Skeleton Videos（2019 ICCVW）</h4> <h4 id="action-assessment-by-joint-relation-graphs-2019-iccv"><a href="#action-assessment-by-joint-relation-graphs-2019-iccv" class="header-anchor">#</a> Action Assessment by Joint Relation Graphs（2019 ICCV）</h4> <ul><li><p><strong>出发点</strong></p> <p>以往的作品主要关注整个场景，包括表演者的身体和背景，但<strong>忽略了详细的关节互动</strong>。这对于细粒度和准确的动作评估是不够的，因为每个关节的动作质量取决于其相邻关节。因此，我们建议根据关节关系学习详细的关节运动。我们建立了可训练的关节关系图，并在其上分析关节运动。我们提出了两个新的关节运动学习模块，即<strong>关节共性模块</strong>和<strong>关节差异模块</strong>。</p> <p>相邻（局部连接）关节的运动共性反映了某个身体部位的<strong>一般运动</strong>，而相邻关节之间的运动差异反映了<strong>动作协调</strong>。一个良好的动作必须有熟练的详细动作和良好的关节协调。</p></li> <li><p><strong>模型</strong></p> <p>为了建模关节运动之间的关系，我们提出了一种基于图的动作评估网络，其中图的节点对应于关节运动。我们定义了：</p> <p><strong>两个可学习的关系图</strong>：空间关系图用于建模一个时间步内的关节关系，时间关系图用于建模两个即时时间步之间的关节关系。</p> <p>基于这两个图形，我们开发了<strong>两个运动学习模块</strong>，即关节共性模块和关节差异模块。关节通用性模块通过在空间图中聚合关节运动来提取特定时间步的身体部位动力学信息。关节差分模块通过将每个关节与其在空间图和时间图中的局部连接邻居进行比较来提取协调信息。</p> <p>输入视频被统一划分为<strong>T个时间步</strong>。我们的模型给出了每个时间步的评估结果。我们将整个场景和局部patch视频作为输入，在<strong>关节周围裁剪局部patch</strong>。我们提取了整个场景视频和局部补丁视频的特征。然后，提出的关节共性模块和关节差异模块在关系图上学习关节运动，给出四个学习特征。然后将学习到的特征反馈给回归模块。我们的模型给出了每个时间步的部分结果和整个视频的整体结果。</p> <img title="" src="/assets/img/image-20220618145741763.369387c3.png" alt="image-20220618145741763" data-align="center" style="zoom:80%;"> <ul><li><p><strong>预处理</strong></p> <p>使用基于<strong>Mask-RCNN的姿势估计方法提取人体姿势和检测框</strong>。我们利用Kinetics上的I3D预训练来提取RGB和opticalflow的联合特征。通过整体图像获得整个场景特征，通过关节周围裁剪局部面片获得关节运动特征。我<strong>们将视频分成10段，每段中均匀地抽取16帧作为I3D的输入</strong> 。</p></li> <li><p><strong>The Joint Commonality Module</strong></p> <p><strong>空间关系图表示每个邻居在每个时间步长内对某个关节的运动有多大影响。</strong> 我们将空间关系图的相邻矩阵表示为<img src="https://math.now.sh?inline=A_s%E2%88%88%20R%5E%7BJ%C3%97J%7D" style="display:inline-block;margin:0;">，其中J是骨架关节的总数。相邻矩阵中的元素As（i，j）表示第i个关节对第j个关节的影响程度。<strong>As中的元素是非负的和可学习的</strong>，但不相关的关节对的元素除外，它们被设置为零。训练开始时，可学习元素在[0，1]范围内随机初始化。</p> <p><img src="https://math.now.sh?from=%7B%5Ccolor%7BBlack%7DH_%7B1%7D%5E%7Bt%7D%3DA_%7Bs%7D%20%5Ccdot%20H_%7B0%7D%5E%7Bt%7D%7D%0A"></p><p>c∈ {0，1} 表示是否执行了图卷积。其中<img src="https://math.now.sh?inline=H_t%5E0%EF%BC%8CH_t%5E1%E2%88%88%20R%5E%7BJ%C3%97M%7D" style="display:inline-block;margin:0;">。这里，J表示关节总数，M表示隐藏状态的特征尺寸。特别地，隐藏状态包含卷积前关节的运动特征，即<img src="https://math.now.sh?inline=H_t%5E0%3DF_t" style="display:inline-block;margin:0;">，其中<img src="https://math.now.sh?inline=F_t%E2%88%88%20R%5E%7BJ%C3%97M%7D" style="display:inline-block;margin:0;">表示第t个时间步的关节运动特征。</p> <p>然后，<strong>模块将所有节点的隐藏状态聚合为通用特征</strong>，其中t是时间步长 。功能聚合是平均池，可以写为：</p> <p><img src="https://math.now.sh?from=%7B%5Ccolor%7BBlack%7D%20%5Coverline%7Bh_%7Bc%7D%5E%7Bt%7D%7D%3D%5Cfrac%7B1%7D%7BN%7D%5Cleft%28H_%7Bc%7D%5E%7Bt%5E%7BT%7D%7D%20%5Ccdot%20%5Cmathbf%7B1%7D%5Cright%29%7D%0A"></p></li> <li><p><strong>The Joint Difference Module</strong></p> <p>我们将时间关系图的邻接矩阵表示为Ap，其中<img src="https://math.now.sh?inline=A_p%E2%88%88%20R%5E%7BJ%C3%97J%7D" style="display:inline-block;margin:0;">。时间关系图还模拟相邻关节之间的关系，但跨越两个即时时间步。Ap（i，j）的元素表示对第i个关节的影响程度 <strong>（在前一时间步t− 1） 在第j个关节上有（目前为步骤t）</strong>。与As类似，相邻矩阵Ap也是非负的且可学习的。训练开始时，可训练权重在[0，1]上随机初始化。</p> <p><img src="https://math.now.sh?from=%7B%5Ccolor%7Bblack%7D%20%5Cbegin%7Baligned%7D%0A"></p></li></ul></li></ul> <p>D_{p}^{t}(i, m)=&amp; \sum_{j}\left(A_{p}(i, j) \cdot\left(F^{t}(i, m)-F^{t-1}(j, m)\right)\right) \cdot w_{j} \
&amp; 1 \leq i, j \leq J, 1 \leq m \leq M
\end{aligned}}
$$</p> <div class="language- extra-class"><pre><code>邻域聚合中的权重表示为wj，wj是可学习的，表示联合j对其他人的影响。然后通过均值池融合每个关节的聚合运动差异，形成差异特征（$\overline d^t_p与\overline d^t_s$相同）：

$$
{\color{black} \bar{d}_{s}^{t}=\frac{1}{N}\left(D_{s}^{t^{T}} \cdot \mathbf{1}\right)}
$$
</code></pre></div><ul><li><p><strong>Regression Module</strong></p> <p>输入的5个特征通过特征编码器（400x512 FC层+Relu）进行编码，然后由功能池层聚合，形成一个整体功能vt ：</p> <p><img src="https://math.now.sh?from=%7B%5Ccolor%7BBlack%7Dv%5E%7Bt%7D%3D%5Csum_%7Bi%7D%20%5Calpha_%7Bi%7D%20%5Ccdot%20%5Chat%7Bu_%7Bi%7D%5E%7Bt%7D%7D%2B%5Cbeta_%7Bi%7D%7D%0A"></p><p>最后，我们得到了两个完全连通层（第一个FC为512×128形状，具     有ReLU激活，第二个FC为128×1形状的线性层）的评估结果。总体评估结果如下：</p> <p><img src="https://math.now.sh?from=%7B%5Ccolor%7BBlack%7D%20s%3D%5Csum_%7Bt%7D%20S%5Cleft%28v%5E%7Bt%7D%5Cright%29%7D%20%0A"></p></li> <li><p><strong>实验</strong></p> <p>我们的模型的结果与最先进的方法和基线进行了比较。我们的模型实现了最先进的性能，并且在六个动作中的每一个都优于基线。 （AQA-7）</p> <img title="" src="/assets/img/image-20220618155546920.f3f2b1d5.png" alt="image-20220618155546920" data-align="center" style="zoom:80%;"> <p><strong>消融实验：</strong></p> <p>联合共性模块和联合差异模块都对模型性能有贡献。在融合学习特征的方法中，特征编码器和特征池层是必要的。</p> <img title="" src="/assets/img/image-20220618155525837.3dfea00f.png" alt="image-20220618155525837" data-align="center" style="zoom:80%;"></li></ul> <h4 id="efficient-and-robust-skeleton-based-quality-assessment-and-abnormality-detection-in-human-action-performance-2020-jbhi"><a href="#efficient-and-robust-skeleton-based-quality-assessment-and-abnormality-detection-in-human-action-performance-2020-jbhi" class="header-anchor">#</a> Efficient and Robust Skeleton-Based Quality Assessment and Abnormality Detection in Human Action Performance（2020 JBHI）</h4> <h3 id="_2-2-基于rgb视频流"><a href="#_2-2-基于rgb视频流" class="header-anchor">#</a> 2.2 基于RGB视频流</h3> <h4 id="learning-to-score-olympic-events-2017-cvprw"><a href="#learning-to-score-olympic-events-2017-cvprw" class="header-anchor">#</a> Learning to Score Olympic Events（2017 CVPRW）</h4> <h4 id="scoringnet-learning-key-fragment-for-action-quality-assessment-with-ranking-loss-in-skilled-sports-2018-accv"><a href="#scoringnet-learning-key-fragment-for-action-quality-assessment-with-ranking-loss-in-skilled-sports-2018-accv" class="header-anchor">#</a> ScoringNet: Learning Key Fragment for Action Quality Assessment with Ranking Loss in Skilled Sports（2018 ACCV）</h4> <h4 id="s3d-stacking-segmental-p3d-for-action-quality-assessment-2018-icip"><a href="#s3d-stacking-segmental-p3d-for-action-quality-assessment-2018-icip" class="header-anchor">#</a> S3D: Stacking Segmental P3D for Action Quality Assessment（2018 ICIP）</h4> <h4 id="end-to-end-learning-for-action-quality-assessment-2018-pcm"><a href="#end-to-end-learning-for-action-quality-assessment-2018-pcm" class="header-anchor">#</a> End-To-End Learning for Action Quality Assessment（2018 PCM）</h4> <h4 id="action-quality-assessment-across-multiple-actions-wacv-2019"><a href="#action-quality-assessment-across-multiple-actions-wacv-2019" class="header-anchor">#</a> Action Quality Assessment Across Multiple Actions（WACV 2019）</h4> <ul><li><p><strong>出发点</strong></p> <p><strong>三个问题：</strong></p> <ul><li><p>不同的行动之间是否有共同的行动质量要素？</p></li> <li><p>如果是这样的话，在各种行动中训练/预先训练一个模型会有帮助吗（而不是按照当前的方法训练一个特定的行动）？</p></li> <li><p>一个受过各种动作训练的模型能否衡量一个看不见的动作的质量？</p></li></ul> <p><strong>三个实验：</strong></p> <p>在本文中，<strong>新发布一个AQA数据集</strong>，包含7个动作类型。主要设计了三个实验，以观察在行动质量评估（AQA）环境中知识转移是否可行。</p> <ol><li>检查是否有可能学习一个所有行动模型，如果有，比较我们提出的所有行动模型与行动特定模型的性能 <strong>（All-Action vs. Single-Action Models）</strong></li> <li>评估所有动作模型如何量化看不见的行动类别的质量 <strong>（ Zero-Shot AQA）</strong></li> <li>评估所有动作模型对新动作类的泛化 <strong>（Fine-tuning to a Novel Action Class）</strong></li></ol></li> <li><p><strong>AQA数据集</strong></p> <ul><li><p><strong>介绍</strong></p> <ul><li>参考下文</li></ul></li> <li><p><strong>共同的动作质量元素</strong></p> <p>AQA-7运动有类似的动作元素，包括滑步和扭转。因此，行动的质量也以类似的方式进行评估。例如，在跳水和体操跳马中，裁判希望运动员在长矛姿势（执行质量方面）时双腿完全伸直，难度与扭转和空翻的次数成正比。同样，在滑雪和滑雪板大型空中项目中，难度与垂直和水平旋转的次数有关。在所有的运动项目中，都有一个完美落地的期望，最终得分会有很高的冲击力（一个飞溅或“撕裂”最小的项目就是跳水）。这些相似之处背后的原因是，在保持双腿伸直的同时，必须完成更多的翻筋斗、扭转或旋转（难度方面），并且在起跳到着陆的有限时间内，身体处于紧密的抱膝或长矛姿势（执行质量方面），这使其更难实现，因此，值得裁判多加分（相当于质量更高）。<strong>最终得分是执行质量和难度的函数</strong>。在某些情况下，如跳水，这将是结果函数，在另一种情况下，如Gymault，这将是总和，而其他动作（BigAir事件）可能使用更全面的组合方法。考虑到行动的相似性，人们相信，<strong>了解一个行动中应该重视哪些方面有助于衡量另一个行动的质量。</strong></p></li></ul></li> <li><p><strong>模型</strong></p> <ul><li><p>模型首先使用<strong>C3D网络</strong>提取特征，然后是一个<strong>256维LSTM层</strong>和一个<strong>完全连接（FC）层</strong>，输出最终AQA分数。</p></li> <li><p>视频以<strong>16帧</strong>的片段进行处理，以在FC-6层生成C3D特征，FC-6层连接到LSTM层，用于时间特征聚合。</p></li> <li><p>C3D网络在训练期间保持冻结状态，因此仅调整LSTM和最终FC层参数。将预测得分与真实得分之间的欧氏距离作为要最小化的损失函数。</p></li> <li><p>这项工作的主要区别不是为每个动作建立一个单独的模型（我们称之为<strong>特定动作或单个动作模型</strong>），而是通过使用所有/多个动作的样本进行训练来学习单个模型（我们称我们的模型为<strong>所有动作或多个动作模型</strong>）。</p> <img title="" src="/assets/img/image-20220617151512388.95f1ec28.png" alt="image-20220617151512388" data-align="center" style="zoom:50%;"></li></ul></li> <li><p><strong>实验🎯</strong></p> <ul><li><p><strong>数据准备（标准化及数据增强）</strong></p> <p>由于<strong>不同动作的得分范围不同</strong>，我们将所有动作的<strong>原始得分除以相应动作的训练标准差</strong>。在测试时，我们将预测得分乘以相应操作的标准偏差，得到最终的判断值。</p> <p>实验只在六个动作类上进行，因为它们的长度很短。所有视频在需要时，通过对第一帧进行零填充，标准化为<strong>103帧</strong>的固定大小。<strong>蹦床被排除在实验之外</strong>，因为650帧的平均长度要长得多，并且由多个“技巧”组成该模型使用803个视频进行训练，并在其余303个视频上进行测试。</p> <p>在训练过程中，使用时间数据增强来获得同一视频样本的<strong>六</strong>个不同副本，其中<strong>一个帧开始时间不同</strong>（实际上是4818个训练样本）。</p></li> <li><p><strong>实现细节</strong></p> <p>C3D网络在UCF-101上预训练，预训练后，C3D被冻结并用作特征提取器。我们使用90个clips的批量大小（15个完整视频样本）。</p></li> <li><p><strong>实验一（All-Action vs. Single-Action Models）</strong></p> <p><strong>All-Action模型</strong>的基线是<strong>Single-Action C3D-LSTM</strong>，因为两者使用相同的特征聚合方法（LSTM）和回归模块（FC层）。很明显，时空特征的使用（C3D网络、FC-6层激活）比pose+DCT有所改善。在六个动作中的五个动作中，提议的全动作模型优于单动作模型，但单板滑雪的成绩低于0.14。平均而言，所有动作模型都将Spearman的等级关联性能提高了<strong>0.03</strong>，而<strong>无需更改网络，而是利用所有动作的数据样本。</strong></p> <img title="" src="/assets/img/image-20220617152410823.d835e243.png" alt="image-20220617152410823" data-align="center" style="zoom:80%;"></li> <li><p><strong>实验二（ Zero-Shot AQA）</strong></p> <p>在<strong>Multi-action</strong>中，模型在五个动作类上进行训练，并在剩余的动作类上进行测试（按列）。在<strong>单动作模型行</strong>中，对角线条目显示同一动作的训练和测试结果。</p> <p>通过随机初始化，AQA系统无法以任何可靠性执行，如斯皮尔曼的秩相关接近零值所示。</p> <p>相比之下，全动作版本显示出一些正相关。我们认为，全动作模型性能更好的原因是，<strong>使用多个动作可以提供良好的初始化，因为存在公共/共享的动作质量元素。</strong> 此外，使用“所有动作模型”还有一个优势，即<strong>可以访问更多的培训视频进行学习</strong>。尽管在四个动作中，我们的所有动作都比随机初始化效果更好，但有两个动作——体操跳马和滑雪——似乎提供的指示很弱。</p> <img title="" src="/assets/img/image-20220617154303183.d43bb956.png" alt="image-20220617154303183" data-align="center" style="zoom:80%;"></li> <li><p><strong>实验三（Fine-tuning to a Novel Action Class）</strong></p> <p>所有动作模型都是对<strong>五个动作进行预训练</strong>，并对剩余的看不见的动作进行微调，数据点最少。对于数据丰富的动作（跳水、体操跳马、滑雪和滑雪板），使用{25、75、125}个训练样本，而对于数据贫乏的动作（同步跳水3米/10米），仅使用{15、25、35}个训练样本进行微调。对剩余的<strong>50个样本进行所有动作的测试</strong>。</p> <p>我们发现在18个案例中，有16个案例的所有动作都更好。请注意，即使在体操跳马和滑雪的情况下，似乎所有动作的初始化都很差（实验二中zero-shot AQA实验）</p> <img title="" src="/assets/img/image-20220617154845213.4923d9da.png" alt="image-20220617154845213" data-align="center" style="zoom:80%;"></li></ul></li> <li><p><strong>结论</strong></p> <p>这项工作表明，与许多其他计算机视觉任务（如对象分类或动作识别）一样，<strong>动作质量评估（AQA）可以通过跨多个动作的样本训练共享模型，从知识转移/共享中获益。</strong> 我们在新引入的数据集AQA-7上实验证明了这一点。实验表明：</p> <ol><li>通过考虑多个动作，可以更好地利用有限的每动作数据来提高每动作性能（<strong>更有效地利用数据</strong>）</li> <li><strong>多动作预训练可以更好地初始化新动作</strong>，暗示行动中质量概念的基本一致性（证明所有行动模型都比单一行动模型更具普遍性）。</li></ol></li></ul> <h4 id="manipulation-skill-assessment-from-videos-with-spatial-attention-network-2019-iccvw"><a href="#manipulation-skill-assessment-from-videos-with-spatial-attention-network-2019-iccvw" class="header-anchor">#</a> Manipulation-Skill Assessment from Videos with Spatial Attention Network（2019 ICCVW）</h4> <h4 id="what-and-how-well-you-performed-a-multitask-learning-approach-to-action-quality-assessment-2019-cvpr"><a href="#what-and-how-well-you-performed-a-multitask-learning-approach-to-action-quality-assessment-2019-cvpr" class="header-anchor">#</a> What and How Well You Performed? A Multitask Learning Approach to Action Quality Assessment（2019 CVPR）</h4> <ul><li><p><strong>出发点</strong></p> <p><strong>是否可以通过对行动及其质量的描述来提高行动质量评估（AQA）任务的绩效？</strong> 当前的AQA和技能评估方法建议学习只服务于一项任务的特征——估计最终得分。在本文中，我们建议学习解释三个相关任务的时空特征——细粒度动作识别、注释生成和AQA分数估计。收集了一个新的<strong>MTL-AQA数据集</strong>（参考下文），这是迄今为止最大的数据集，由1412个潜水样本组成。</p></li> <li><p><strong>多任务AQA概念</strong></p> <p>MTL是一种机器学习范式，其中一个模型可以满足多个任务。例如，将道路标志、道路和车辆识别在一起，而STL方法需要为每个对象类型建立单独的模型。<strong>MTL任务的选择通常是这样的，即它们彼此相关</strong>，并且它们的网络有一个共同的主体，分支成特定于任务的输出头。<strong>总网络损失是单个任务损失的总和</strong>（加权和）。当端到端优化后，网络能够在公共身体部分学习更丰富的表示，因为它必须能够服务/解释所有任务。通过使用与主任务互补的相关辅助任务，更丰富的表示往往有助于提高主任务的性能。</p> <p>总的来说，不仅仅是跳水，动作质量还取决于<strong>执行了什么动作</strong>以及<strong>该动作执行得有多好</strong>。这使得选择辅助任务变得自然：<strong>详细的动作识别是对“什么”部分的回答</strong>，而<strong>注释是一种口头描述，包含动作执行的优缺点，是对“如何”部分的回答</strong>。AQA可以看作是找到一个将输入视频映射到AQA分数的函数。Caruana将辅助任务的监督信号视为一种归纳偏差（假设）。归纳偏差可以被认为是在发现AQA函数时限制假设/搜索空间的约束。通过归纳偏差，MTL提供了比STL更好的泛化能力。</p> <img title="" src="/assets/img/image-20220615135851980.5d86ee61.png" alt="image-20220615135851980" data-align="center" style="zoom:80%;"></li></ul> <p>两个模型：</p> <ul><li><p><strong>Averaging as aggregation (C3D-AVG)</strong></p> <p><strong>主干网络</strong>：由<strong>C3D网络</strong>组成，直至第五个池层。将视频（96帧）分成小片段（16帧），然后聚合片段级表示以获得视频级描述。</p> <p><strong>聚合方案</strong>：使用平均值作为线性组合。网络针对所有三项任务进行了端到端优化。平均层以上的C3D-AVG网络可被视为编码器，它将输入视频剪辑编码为表示，平均后（在特征空间中）将对应于运动员收集的总AQA点。后续层可以看作是单个任务的解码器。</p> <p><strong>特定于任务的头部</strong>：对于动作识别和AQA任务，片段级pool-5特征按元素平均，以生成视频级表示。由于字幕是一项顺序到顺序的任务，因此在平均之前，各个片段级特征会输入到字幕分支（单个片段级特征在实践中比用于字幕的平均片段级特征工作得更好）。</p> <img title="" src="/assets/img/image-20220617182948828.8d9d1d01.png" alt="image-20220617182948828" data-align="center" style="zoom:80%;"></li> <li><p><strong>Multiscale Context Aggregation with Dilated Convolutions (MSCADC)</strong></p> <p>Nibali等人的研究表明，多尺度上下文聚合和扩展卷积（MSCADC）可以改善跳水分类。鉴于MSCADC在辅助任务上的强大性能，它被选为MTL。</p> <p><strong>主干网络</strong>：MSCADC网络基于C3D网络，并结合了一些改进，如使用批量归一化，以提供更好的正则化，这在数据非常有限的AQA中是必需的。此外，从C3D的最后两个卷积组中删除池，而是使用2的膨胀率。该主干结构在所有MTL任务之间共享。</p> <p><strong>特定于任务的头部</strong>：我们使用单独的输出头，头部由一个上下文网络和几个附加层组成。</p> <p>与C3D-AVG网络不同，我们<strong>将整个动作降采样为仅16帧的短序列</strong>（类似于关键动作快照），可以显著减少网络参数和内存的数量，而这些参数和内存可以用来提高空间分辨率。</p> <img title="" src="/assets/img/image-20220617183326563.13db114c.png" alt="image-20220617183326563" data-align="center" style="zoom:80%;"></li> <li><p><strong>实现细节</strong></p> <p>在UCF101动作识别数据集上对公共网络主干进行预训练，批次大小设置为三个样本。</p> <p>其他特定于体系结构的实现细节如下：C3D-AVG：该模型通过171×128像素输入视频的<strong>112×112</strong>中心裁剪进行端到端训练。每个潜水样本的时间标准化长度为96帧。MSCADC：由于此体系结构不包含完全连接的层，并且所有视频都降采样到16帧，因此模型参数较少，允许使用更高分辨率的视频输入。帧的大小调整为640×360像素，并使用<strong>180×180</strong>中心裁剪。</p></li> <li><p><strong>实验</strong></p> <ul><li><p><strong>Single-task vs. Multi-task approach</strong>：本实验首先考虑了STL方法对AQA任务的处理，然后测量了包含辅助任务的效果。下表总结了评估结果。我们观察到，对于这两个网络，<strong>MTL方法的性能都优于STL方法</strong>。（Cls-行为识别，Caps-评论生成。第一行显示STL结果，其余行显示MTL结果。 ）</p> <p><strong>C3D-AVG在STL和MTL方面都优于MSCADC</strong>，而MSCADC的优势是速度快，内存需求比C3D-AVG低</p> <img title="" src="/assets/img/image-20220617184759189.29649797.png" alt="image-20220617184759189" data-align="center" style="zoom:80%;"></li> <li><p><strong>本文的模型与现有的方法进行比较</strong>：我们获得了数据集上所有现有方法的结果。C3D-SVR是中表现最好的方法，但似乎没有从增加的训练样本数量中获益。在之前工作中由于训练数据量不足，C3D-LSTM的表现比C3D-SVR差。而在扩展训练数据的情况下，表现确实优于C3D-SVR。我们的MSCADC-STL比大多数现有方法工作得更好，而<strong>我们的C3D-AVG-STL比所有现有方法的性能更好。</strong></p> <img title="" src="/assets/img/image-20220617185358863.507536c0.png" alt="image-20220617185358863" data-align="center" style="zoom:80%;"></li> <li><p><strong>Generalization provided by MTL</strong>：为了确定MTL提供了更多的泛化，我们<strong>使用更少的数据点来训练C3DAVG-STL和C3D-AVG-MTL模型</strong>。我们看到，MTL始终优于STL，而且随着训练样本的减少，差距似乎也在扩大 。</p> <img title="" src="/assets/img/image-20220617185730444.514b6aec.png" alt="image-20220617185730444" data-align="center" style="zoom:80%;"></li></ul></li> <li><p><strong>结论</strong></p> <p><strong>本文将多任务学习方法引入AQA，并表明MTL的性能优于STL</strong>，因为它具有更好的泛化能力，这在AQA和技能评估中尤其重要，因为数据集很小。</p></li></ul> <h4 id="the-pros-and-cons-rank-aware-temporal-attention-for-skill-determination-in-long-videos-2019-cvpr"><a href="#the-pros-and-cons-rank-aware-temporal-attention-for-skill-determination-in-long-videos-2019-cvpr" class="header-anchor">#</a> The Pros and Cons: Rank-Aware Temporal Attention for Skill Determination in Long Videos（2019 CVPR）</h4> <h4 id="a-deep-learning-framework-for-assessing-physical-rehabilitation-exercises-2020-tnsre"><a href="#a-deep-learning-framework-for-assessing-physical-rehabilitation-exercises-2020-tnsre" class="header-anchor">#</a> A Deep Learning Framework for Assessing Physical Rehabilitation Exercises（2020 TNSRE）</h4> <h4 id="usdl-2020-cvpr"><a href="#usdl-2020-cvpr" class="header-anchor">#</a> USDL（2020 CVPR）</h4> <blockquote><p>Uncertainty-aware Score Distribution Learning for Action Quality Assessment</p></blockquote> <ul><li><p><strong>出发点</strong></p> <p>现有的大多数方法只是将AQA视为一个回归问题，以便直接预测行动得分。不幸的是，他们的表现确实有限。这种限制的根源在于，这种处理忽视了<strong>行动分数标签的潜在模糊性</strong>，这是AQA的关键问题之一。在实际中这种歧义是由<strong>动作标签的生成方式引起的</strong>。</p> <p>对于跳水比赛，当运动员以3.8的难度完成动作时，七名裁判给出的分数为{9.0、8.5、9.0、8.0、9.0、8.5、9.0}。剔除前两名和后两名得分后，最终得分计算为：sf最终=（9.0+9.0+8.5）×3.8=100.70。<strong>这表明了不同的裁判所造成的最终分数固有的不确定性</strong>。此外，<strong>每个法官的主观评价也可能给最终得分带来不确定性</strong>。除了跳水比赛外，体操跳马、花样滑雪等许多其他运动中也存在这种现象。复杂的分数不确定性使得准确的AQA非常困难。因此，需要设计一个鲁棒模型来处理AQA的不确定性。</p></li> <li><p><strong>解决思路</strong></p> <p>为了解决这个问题，我们提出了一种<strong>不确定性感知分数分布学习（USDL）方法</strong>，该方法利用<strong>不同分数的分布作为监控信号</strong>，而不是单个分数。采用的分数分布可以更好地描述AQA分数的概率，从而可以很好地处理上述不确定性问题。我们<strong>基于广泛使用的高斯函数生成真实分数分布，其中的平均值设置为分数标签</strong>。同时，将动作视频输入3D ConvNets，以生成其预测的分数分布。然后，我们优化了groundtruth得分分布和预测得分分布之间的<strong>Kullback-Leibler散度</strong>。</p> <p>此外，一旦细<strong>粒度分数标签可用</strong>（例如，一个动作的难度或来自不同评委的多个分数），我们进一步设计了一种<strong>多路径不确定性感知分数分布学习（MUSDL）方法</strong>，以充分探索最终分数的这些分离组件。在推理过程中，我们严格遵循博弈规则，将多个预测分数进行融合，得到最终分数。</p> <img title="" src="/assets/img/image-20220618164924039.f6236bbe.png" alt="image-20220618164924039" data-align="center" style="zoom:80%;"></li> <li><p><strong>Score Distribution Generation</strong></p> <p>在训练阶段，给定与标记分数s相关的视频，首先生成一个高斯函数，其平均值为s，标准偏差为σ：</p> <p><img src="https://math.now.sh?from=%7B%5Ccolor%7BBlack%7D%20g%28c%29%3D%7B%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7D%7D%5Cexp(-%7B%5Cfrac%7B(c-s)%5E%7B2%7D%7D%7B2%5Csigma%5E%7B2%7D%7D%7D)%7D%0A"></p><p>这里σ是一个超参数，用作评估行动的不确定性水平。通过<strong>将分数区间统一离散为一组分数</strong>c=[c1，c2，…，cm]，使用一个向量来描述<strong>每个分数的程度</strong>，即gc=[g（c1），g（c2），…，g（cm）]。最终得分分布标签pc=[p（c1），p（c2），…，p（cm）]通过<strong>规范化gc生成</strong>，如下所示：</p> <p><img src="https://math.now.sh?from=%7B%5Ccolor%7BBlack%7D%20p%28c_%7Bi%7D%29%3Dg(c_%7Bi%7D)%2F%5Csum_%7Bj%3D1%7D%5E%7Bm%7Dg(c_%7Bj%7D)%2C%5C%20i%3D1%2C2%2C...%2Cm%7D%0A"></p><p>在我们的实验中，我们对两个数据集中的最终总分和MTL-AQA中的七个评判分数进行了归一化。对于最终总分，由于是浮点数，我们将其归一化为：</p> <p><img src="https://math.now.sh?from=%7B%5Ccolor%7BBlack%7D%20S_%7B%5Ctext%20%7Bnormalize%20%7D%7D%3D%5Cfrac%7BS-S_%7B%5Cmin%20%7D%7D%7BS_%7B%5Cmax%20%7D-S_%7B%5Cmin%20%7D%7D%20%5Ctimes%20100%7D%0A"></p><p>此处Smax和Smin表示数据集中的最大和最小分数。对于MTL-AQA数据集中的评判分数，<strong>由于这些分数本质上是离散的，但不是整数，因此我们通过将原始分数的值加倍来对其进行归一化，以获得整数。</strong></p></li> <li><p><strong>USDL</strong></p> <ul><li>对于L帧的给定输入视频，我们利用滑动窗口将其分割为<strong>N个</strong>重叠片段，其中<strong>每个片段包含M(16)个连续帧</strong>。使用在Kinetics dataset上预训练的<strong>I3D模型</strong>作为特征提取器。它将包含16帧的动作序列作为输入，并输出<strong>1024维的特征</strong>。然后是三个完全连接的层（两个隐藏层FC（256，ReLU）和FC（128，ReLU），从而生成<strong>N个特征</strong>。</li> <li>**完全连接层的权重在不同片段之间共享。**将获得的特征通过时间池进行融合，并通过softmax层生成预测分布。我们得到的最终预测得分为<img src="https://math.now.sh?inline=s_%7Bpre%7D%3D%5Bs_%7Bpre%7D%EF%BC%88c_1%EF%BC%89%EF%BC%8Cs_%7Bpre%7D%EF%BC%88c_2%EF%BC%89%EF%BC%8C%E2%80%A6%EF%BC%8Cs_%7Bpre%7D%EF%BC%88c_m%EF%BC%89%5D" style="display:inline-block;margin:0;">。</li></ul> <img title="" src="/assets/img/image-20220618193810330.1bb83670.png" alt="image-20220618193810330" data-align="center" style="zoom:80%;"></li> <li><p><strong>训练与预测</strong></p> <p>最后，将学习损失计算为spre和pc之间的<strong>Kullback-Leibler（KL）散度：</strong></p> <p><img src="https://math.now.sh?from=%7B%5Ccolor%7BBlack%7D%20K%20L%5C%7Bp_%7Bc%7D%7C%7C%7B%5Cbf%20s%7D_%7Bp%20r%20e%7D%5C%7D%3D%5Csum_%7Bi%3D1%7D%5E%7Bm%7Dp%28c_%7Bi%7D%29%5Clog%5Cfrac%7Bp(c_%7Bi%7D)%7D%7Bs_%7Bp%20r%20e%7D(c_%7Bi%7D)%7D%7D%0A"></p><p>从分数分布推断：在推断阶段，我们将输入的测试视频转发到我们的优化模型中，以获得相应的预测分数分布spre。通过选择<strong>具有最大概率的分数</strong>获得最终评估：</p> <p><img src="https://math.now.sh?from=%7B%5Ccolor%7BBlack%7D%20s_%7Bf%20i%20n%20a%20l%7D%3D%5Carg_%7B%5Cmathrm%7Bc%7D_%7Bi%7D%7D%5Cmathrm%7Bmax~%7D%5C%7Bs_%7Bp%20r%20e%7D%28c_%7B1%7D%29%2Cs_%7Bp%20r%20e%7D(c_%7B2%7D)%2C...%2Cs_%7Bp%20r%20e%7D(c_%7Bm%7D)%5C%7D%7D%0A"></p></li> <li><p><strong>MUSDL</strong></p> <ul><li><p>在训练阶段，我们<strong>将K个评委的分数建模为不同的高斯分布</strong>，并使用类似的策略来训练包含K个子网络的模型。在测试阶段，我们根据K预测分数和运动规则获得最终评估。</p> <img title="" src="/assets/img/image-20220618200304274.3430281a.png" alt="image-20220618200304274" data-align="center" style="zoom:80%;"></li> <li><p>如图，对于每一条路径，我们使用与我们的USDL方法相同的管道。<strong>不同路径的完全连接层分别进行训练</strong>，I3D主干在路径之间共享。在训练阶段，假设我们有一组分数来自K个不同的评委。首先按照递增的顺序对分数进行排序，以训练代表不同严格程度法官的子网络。给定一个训练视频，我们首先通过I3D主干将其馈送，并获得N个特征{f 1，f 2，…f N}。然后将特征馈入K个子网络，以获得K最终预测分布，如下所示：</p> <p><img src="https://math.now.sh?from=%7B%5Ccolor%7BBlack%7D%20s_%7Bp%20r%20e%2Ck%7D%5E%7Bj%20u%20d%20g%20e%7D%3D%5Cphi_%7Bk%7D%28f_%7B1%7D%2Cf_%7B2%7D%2C...f_%7BN%7D%29%2C%5Cquad%20k%3D1%2C2%2C...K%7D%0A"></p></li> <li><p>然后，总培训损失计算为：</p> <p><img src="https://math.now.sh?from=%7B%5Ccolor%7BBlack%7D%20%5Cbegin%7Baligned%7D%0A"></p></li></ul></li></ul> <p>&amp;=\sum_{k=1}^{K} \sum_{i=1}^{m} p\left(c_{i, k}^{j u d g e}\right) \log \frac{p\left(c_{i, k}^{j u d g e}\right)}{s_{p r e, k}^{j u d g e}\left(c_{i, k}^{j u d g e}\right)}
\end{aligned}}
$$</p> <ul><li><p><strong>基于规则的多径推理</strong>：在推理阶段，我们通过我们的多径模型转发每个测试视频，并获得最终预测分数。根据跳水比赛的具体规则，我们可以得到最终分数：</p> <p><img src="https://math.now.sh?from=%7B%5Ccolor%7BBlack%7D%20s_%7Bf%20i%20n%20a%20l%7D%3DD%20D%5Ctimes%5Csum_%7Bk%5E%7B%5Cprime%7D%5Cin%20U%7Ds_%7Bf%20i%20n%20a%20l%2Ck%5E%7B%5Cprime%7D%7D%5E%7Bj%20u%20a%20d%20g%20e%7D%7D%0A"></p><p>这里，U表示{1，2，…，K}的子集（例如，跳水比赛将放弃给出前2名和最后2名分数的裁判），<strong>DD表示将提前发布的输入动作视频的难度</strong>。事实上，即使在推理期间没有提供DD，我们仍然可以通过在训练期间为模型引入侧网络分支来训练模型来预测DD。在推理过程中，预测的DD直接用于公式。🧲</p></li> <li><p><strong>实验</strong></p> <ul><li><p><strong>说明</strong></p> <p><strong>Regression</strong>：大多数现有作品都采用了这种策略。我<strong>们修改了USDL中最后一个fc层的维度，以生成一个预测分数</strong>。在训练阶段，我们优化了预测分数和地面真实分数之间的L2损失。</p> <p><strong>MUSDL and MUSDL∗</strong>：提出的方法，难度DD分别使用了测试期间的真实值和预测值。</p> <p><strong><img src="https://math.now.sh?inline=USDL_%7BDD%7D" style="display:inline-block;margin:0;"></strong>：在训练阶段，我们使用了七位评委的分数。根据跳水的得分规则，前两名和后两名将被淘汰。我们将剩下的三个评判分数相加，得到一个新的分数标签，并应用USDL学习这个新标签。在推理阶段，我们将预测得分与难度DD的基本真理相乘，生成最终结果。<strong>(分解难度与评委评分)</strong></p></li> <li><p><strong>AQA-7</strong></p> <p>我们的方法的实验结果以及与其他AQA方法的比较。</p> <img title="" src="/assets/img/image-20220618201639725.9f4f6749.png" alt="image-20220618201639725" data-align="center" style="zoom:80%;"></li> <li><p><strong>MTL-AQA</strong></p> <p>现有技术和我们的MUSDL模型相比，竞争结果优于列出的所有其他方法。这些实验结果有力地说明了我们方法的有效性。</p> <img title="" src="/assets/img/image-20220618201813835.94501368.png" alt="image-20220618201813835" data-align="center" style="zoom:80%;"></li> <li><p><strong>消融实验</strong></p> <p>MUSDL*方法在之前的MUSDL方法的基础上添加了一个额外的分支，以执行多任务学习。</p> <p>从结果中，我们可以看到，<img src="https://math.now.sh?inline=USDL_%7BDD%7D" style="display:inline-block;margin:0;">的表现优于USDL，为1.7%，这表明DD是潜水得分评估的一个重要因素。我们认为，<strong>使用DD可以提高性能的原因是它“解开”了问题，使主要管道在视频质量评估方面更加专业化</strong>。MUSDL的性能比单路径方法高出0.4%，这表明细粒度评分可以进一步提高网络的性能 。</p> <img title="" src="/assets/img/image-20220618202118777.c03cc963.png" alt="image-20220618202118777" data-align="center" style="zoom:80%;"></li></ul></li></ul> <h4 id="hybrid-dynamic-static-context-aware-attention-network-for-action-assessment-in-long-videos-2020-acm-mm"><a href="#hybrid-dynamic-static-context-aware-attention-network-for-action-assessment-in-long-videos-2020-acm-mm" class="header-anchor">#</a> Hybrid Dynamic-static Context-aware Attention Network for Action Assessment in Long Videos（2020 ACM MM）</h4> <h4 id="learning-to-score-figure-skating-sport-videos-2020-tcsvt"><a href="#learning-to-score-figure-skating-sport-videos-2020-tcsvt" class="header-anchor">#</a> Learning to Score Figure Skating Sport Videos（2020 TCSVT）</h4> <h4 id="an-asymmetric-modeling-for-action-assessment-2020-eccv"><a href="#an-asymmetric-modeling-for-action-assessment-2020-eccv" class="header-anchor">#</a> An asymmetric modeling for action assessment（2020 ECCV）</h4> <h4 id="tsa-net-tube-self-attention-network-for-action-quality-assessment-2021-acm-mm"><a href="#tsa-net-tube-self-attention-network-for-action-quality-assessment-2021-acm-mm" class="header-anchor">#</a> TSA-Net: Tube Self-Attention Network for Action Quality Assessment（2021 ACM MM）</h4> <h4 id="auto-encoding-score-distribution-regression-for-action-quality-assessment-2021"><a href="#auto-encoding-score-distribution-regression-for-action-quality-assessment-2021" class="header-anchor">#</a> Auto-Encoding Score Distribution Regression for Action Quality Assessment（2021）</h4> <h4 id="core-2021-iccv"><a href="#core-2021-iccv" class="header-anchor">#</a> CoRe（2021 ICCV）</h4> <blockquote><p>Group-aware Contrastive Regression for Action Quality Assessment</p></blockquote> <ul><li><p><strong>出发点</strong></p> <ul><li><p>由于视频之间的细微差异和分数的巨大差异，评估动作质量很有挑战性。<strong>现有的大多数方法都是通过从单个视频中回归质量分数来解决这个问题</strong>，这会受到视频间分数变化较大的影响。</p></li> <li><p>本文表明，视频之间的关系可以为训练和推理过程中更准确的动作质量评估提供重要线索。具体而言，我们<strong>将动作质量评估问题重新表述为参考另一个具有共同属性（例如类别和难度）的视频回归相对分数</strong>，而不是学习未参考的分数。</p></li></ul></li> <li><p><strong>解决思路</strong></p> <ul><li><p>我们提出了一个新的<strong>对比回归（CoRe）框架</strong>，通过成对比较来学习相对分数，该框架突出了视频之间的差异，并指导模型学习评估的关键提示。我们借鉴了对比学习的概念。对比学习旨在学习一个更好的表示空间，其中两个相似样本X，XA之间的距离dA被强制为较小，而不同样本X，XB之间的距离dB被鼓励为较大。因此，<strong>表示空间中的距离已经可以反映两个样本之间的语义关系</strong>（即，如果它们来自同一类别）。类似地，在AQA的背景下，我们的目标是学习一个模型，该模型可以将输入视频映射到分数空间，在该空间中，<strong>动作质量之间的差异可以通过相对分数来衡量(∆A.∆B）</strong> 。与以往直接预测分数的工作不同，我们建议回归输入视频和多个示例视频之间的相对分数作为参考。</p> <img title="" src="/assets/img/image-20220619110810313.42b03c2c.png" alt="image-20220619110810313" data-align="center" style="zoom:80%;"></li> <li><p>作为更准确的分数预测的一步，我们设计了一个<strong>组感知回归树（GART）</strong>，将相对分数回归转换为两个更简单的子问题：（1）<strong>从粗到细的分类</strong>。我们首先将相对分数的范围划分为几个不重叠的区间（即组），然后使用二叉树通过逐步分类将相对分数分配给某个组；（2） <strong>小间隔内的回归</strong>。我们在相对得分所在的组内进行回归，并预测最终得分。</p></li> <li><p>作为另一个贡献，我们设计了一个新的度量，称为<strong>相对L2距离（R-l2）</strong>，通过考虑类内方差来更精确地衡量行动质量评估的绩效。</p></li> <li><p>为了验证我们的方法的有效性，为了证明CoRe的有效性，我们在三个主流AQA数据集上进行了广泛的实验，包括<strong>AQA-7、MTL-AQA和JIGSAWS</strong>。我们的方法大大优于以前的方法，并在所有三个基准上建立了新的最先进水平。</p></li></ul></li> <li><p><strong>模型</strong></p> <p>我们首先根据动作的类别和难度为每个输入视频采样一个示例视频。然后，我们将视频对馈送到共享的I3D主干中，以<strong>提取时空特征，并将这两个特征与示例视频的参考分数相结合</strong>。最后，我们将组合特征传递给<strong>群体感知回归树</strong>，并获得两个视频之间的<strong>得分差异</strong>。在推理过程中，可以通过平均多个不同样本的结果来计算最终得分。</p> <img title="" src="/assets/img/image-20220619150604273.c28795a6.png" alt="image-20220619150604273" data-align="center" style="zoom:80%;"> <ul><li><p><strong>特征提取</strong></p> <p>让<img src="https://math.now.sh?inline=v_m" style="display:inline-block;margin:0;">表示输入视频，<img src="https://math.now.sh?inline=v_n" style="display:inline-block;margin:0;">表示带有分数标签<img src="https://math.now.sh?inline=s_n" style="display:inline-block;margin:0;">的示例视频。我们的目标是回归输入视频和参考视频之间的分数差异 ，<strong>回归问题</strong>可以写为：</p> <p><img src="https://math.now.sh?from=%7B%5Ccolor%7BBlack%7D%20%5Chat%7Bs%7D_%7Bm%7D%3DR%5Cmathrm%7Be%7D%28%5Cmathcal%7BF%7D_%7B%5Cpsi%7D(v_%7Bm%7D%29%2C%5Cmathcal%7BF%7D_%7B%5Cpsi%7D(v_%7Bn%7D))%2Bs_%7Bn%7D.%7D%0A"></p><p>为了使输入和示例具有可比性，我们倾向于<strong>选择与输入视频共享某些属性（例如类别和难度）的视频作为示例</strong>。形式上，给定一个输入视频vm和相应的示例vn，<strong>首先使用I3D提取特征{fn，fm}</strong>，然后将它们与示例sn的分数相加 ：</p> <p><img src="https://math.now.sh?from=%7B%5Ccolor%7BBlack%7D%20f_%7B%28n%2Cm%29%7D%3D%5Cmathrm%7Bconcat%7D(%5Bf_%7Bn%7D%2Cf_%7Bm%7D%2Cs_%7Bn%7D%2F%5Cepsilon%5D)%2C%7D%0A"></p><p><img src="https://math.now.sh?from=%7B%5Ccolor%7BBlack%7D%20%5CDelta%20s%3DR%5Cmathbf%7Be%7D%28f_%7B(n%2Cm%29%7D)%7D%0A"></p></li> <li><p><strong>Group-Aware Regression Tree</strong></p> <p>虽然对比回归框架可以预测相对得分∆s，<strong>∆s通常取值范围很广（例如，对于潜水，∆s∈ [−30, 30])</strong>。 因此，预测∆s直接还是很困难的。为此，我们设计了一个群体感知回归树（GART），以<strong>分而治之</strong>的方式解决这个问题。</p> <p>具体来说，首先划分∆s分为<img src="https://math.now.sh?inline=2%5E%7Bd-1%7D" style="display:inline-block;margin:0;">非重叠间隔（即“组”）。然后，我们构造了一个具有d层，其中叶子代表<img src="https://math.now.sh?inline=2%5E%7Bd-1%7D" style="display:inline-block;margin:0;">组，群体感知回归树的决策过程遵循从粗到细的方式：在第一层，我们判断输入视频是否比示例视频好或坏；在下面的几层中，我们逐渐对输入视频的好坏程度做出更准确的预测。一旦到达叶节点，我们就可以知道输入<strong>视频应该分类到哪一组</strong>，然后我们可以<strong>在相应的小间隔内执行回归</strong>。</p> <img title="" src="/assets/img/image-20220619152033675.92bf0297.png" alt="image-20220619152033675" data-align="center" style="zoom:80%;"> <p>我们采用二叉树结构来执行回归任务。首先，我们执行一个到f（n，m）的MLP，并将输出用作根节点特性的初始化。然后，我们以自顶向下的方式执行回归。每个节点将其父节点的输出特征作为输入，并与更新的特征一起生成二进制概率。**每个叶节点的概率可以通过将沿根路径的所有概率相乘来计算。**我们使用Sigmoid将每个叶节点的输出映射到[0，1]，这是对应组的预测得分差。</p> <div class="language-python line-numbers-mode"><pre class="language-python"><code><span class="token comment"># https://github.com/yuxumin/CoRe/blob/master/models/RegressTree.py</span>
<span class="token keyword">class</span> <span class="token class-name">RegressTree</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>in_channel<span class="token punctuation">,</span>hidden_channel<span class="token punctuation">,</span>depth<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>RegressTree<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>depth <span class="token operator">=</span> depth
        self<span class="token punctuation">.</span>num_leaf <span class="token operator">=</span> <span class="token number">2</span><span class="token operator">**</span><span class="token punctuation">(</span>depth<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>first_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_channel<span class="token punctuation">,</span> hidden_channel<span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>feature_layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>self<span class="token punctuation">.</span>get_tree_layer<span class="token punctuation">(</span><span class="token number">2</span><span class="token operator">**</span>d<span class="token punctuation">,</span> hidden_channel<span class="token punctuation">)</span> <span class="token keyword">for</span> d <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>depth <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>clf_layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>self<span class="token punctuation">.</span>get_clf_layer<span class="token punctuation">(</span><span class="token number">2</span><span class="token operator">**</span>d<span class="token punctuation">,</span> hidden_channel<span class="token punctuation">)</span> <span class="token keyword">for</span> d <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>depth <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>reg_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_leaf <span class="token operator">*</span> hidden_channel<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_leaf<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> groups<span class="token operator">=</span>self<span class="token punctuation">.</span>num_leaf<span class="token punctuation">)</span>
    <span class="token decorator annotation punctuation">@staticmethod</span>
    <span class="token keyword">def</span> <span class="token function">get_tree_layer</span><span class="token punctuation">(</span>num_node_in<span class="token punctuation">,</span> hidden_channel<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>num_node_in <span class="token operator">*</span> hidden_channel<span class="token punctuation">,</span> 
                      num_node_in <span class="token operator">*</span> <span class="token number">2</span> <span class="token operator">*</span> hidden_channel<span class="token punctuation">,</span>
                      <span class="token number">1</span><span class="token punctuation">,</span> groups<span class="token operator">=</span>num_node_in<span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
    <span class="token decorator annotation punctuation">@staticmethod</span>
    <span class="token keyword">def</span> <span class="token function">get_clf_layer</span><span class="token punctuation">(</span>num_node_in<span class="token punctuation">,</span> hidden_channel<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>num_node_in <span class="token operator">*</span> hidden_channel<span class="token punctuation">,</span> 
                         num_node_in <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">,</span> 
                         <span class="token number">1</span><span class="token punctuation">,</span> groups<span class="token operator">=</span>num_node_in<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_feature<span class="token punctuation">)</span><span class="token punctuation">:</span>
        out_prob <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>first_layer<span class="token punctuation">(</span>input_feature<span class="token punctuation">)</span>
        bs <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>depth <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            prob <span class="token operator">=</span> self<span class="token punctuation">.</span>clf_layers<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
            x <span class="token operator">=</span> self<span class="token punctuation">.</span>feature_layers<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>
            <span class="token comment"># print(prob.shape,x.shape)d</span>
            <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>out_prob<span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span>
                prob <span class="token operator">=</span> F<span class="token punctuation">.</span>log_softmax<span class="token punctuation">(</span>prob<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
                pre_prob <span class="token operator">=</span> out_prob<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>
                prob <span class="token operator">=</span> pre_prob <span class="token operator">+</span> prob
                out_prob<span class="token punctuation">.</span>append<span class="token punctuation">(</span>prob<span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                out_prob<span class="token punctuation">.</span>append<span class="token punctuation">(</span>F<span class="token punctuation">.</span>log_softmax<span class="token punctuation">(</span>prob<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bs<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 2 branch only</span>
        delta <span class="token operator">=</span> self<span class="token punctuation">.</span>reg_layer<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># leaf_prob = torch.exp(out_prob[-1].view(bs, -1))</span>
        <span class="token comment"># assert delta.size() == leaf_prob.size()</span>
        <span class="token comment"># final_delta = torch.sum(leaf_prob * delta, dim=1)</span>
        <span class="token keyword">return</span> out_prob<span class="token punctuation">,</span> delta
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br></div></div><ul><li><p><strong>划分策略</strong></p> <p>定义每个组的边界。首先，我们收集所有可能的训练视频对的得分差异列表δ=[δ1，…，δT]。然后，我们按照升序对列表进行排序，以获得δ∗ = [δ∗1.δ∗T]。给定<strong>组数R</strong>，分区算法给出每个区间I R=（ζrleft，ζrright）的界限，如下所示：</p> <p><img src="https://math.now.sh?from=%7B%5Ccolor%7BBlack%7D%20%5Cbegin%7Barray%7D%7Bl%7D%0A"></p><p><img src="https://math.now.sh?from=%0A"></p><p>{\color{Black} \begin{array}{l}
\left.\zeta_{\text {right }}^{r}=\delta^{*}\left(\left\lfloor(T-1) \times \frac{r}{R}\right\rfloor\right]\right), \forall i=1,2, \ldots, R\end{array}}</p> <p><img src="https://math.now.sh?from=%0A%E5%88%86%E5%8C%BA%E7%AD%96%E7%95%A5%E9%9D%9E%E5%B8%B8%E9%87%8D%E8%A6%81%E3%80%82%E5%A6%82%E6%9E%9C%E6%88%91%E4%BB%AC%E7%AE%80%E5%8D%95%E5%9C%B0%E5%B0%86%E6%95%B4%E4%B8%AA%E8%8C%83%E5%9B%B4%E7%BB%9F%E4%B8%80%E5%88%92%E5%88%86%E4%B8%BA%E5%A4%9A%E4%B8%AA%E7%BB%84%EF%BC%8C%E9%82%A3%E4%B9%88%E8%AE%AD%E7%BB%83%E9%9B%86%E4%B8%AD%E5%BE%97%E5%88%86%E5%B7%AE%E5%BC%82%E4%BD%8D%E4%BA%8E%E6%9F%90%E4%B8%AA%E7%BB%84%E7%9A%84%E8%A7%86%E9%A2%91%E5%AF%B9%E5%8F%AF%E8%83%BD%E6%98%AF%E4%B8%8D%E5%B9%B3%E8%A1%A1%E7%9A%84%E3%80%82%E4%B8%8D%E5%90%8C%E5%88%92%E5%88%86%E7%AD%96%E7%95%A5%E4%B8%8B%E8%AE%AD%E7%BB%83%E9%9B%86%E5%BE%97%E5%88%86%E5%B7%AE%E5%BC%82%E7%9A%84%E5%88%86%E5%B8%83%E3%80%82%EF%BC%88a%EF%BC%89%20%E7%BB%9F%E4%B8%80%E5%88%86%E5%8C%BA%E3%80%82%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E8%A7%82%E5%AF%9F%E5%88%B0%E4%B8%8D%E5%90%8C%E7%BE%A4%E4%BD%93%E4%B9%8B%E9%97%B4%E7%9A%84%E9%A2%91%E7%8E%87%E5%B7%AE%E5%BC%82%E5%BE%88%E5%A4%A7%E3%80%82%EF%BC%88b%EF%BC%89%20%E6%96%B9%E7%A8%8B%E5%BC%8F%E4%B8%AD%E6%8F%90%E5%87%BA%E7%9A%84%E5%88%86%E7%BB%84%E7%AD%96%E7%95%A5%E3%80%82%E6%AF%8F%E7%BB%84%E7%9A%84%E8%AE%AD%E7%BB%83%E5%AF%B9%E6%98%AF%E5%B9%B3%E8%A1%A1%E7%9A%84%E3%80%82%20%0A%0A%3Cimg%20title%3D%22%22%20src%3D%22.%2Fpicture%2Fimage-20220619152821840.png%22%20alt%3D%22image-20220619152821840%22%20style%3D%22zoom%3A80%25%3B%22%20data-align%3D%22center%22%3E%0A%0A"></p><p>我们将GART的深度设置为d=5，节点特征维度设置为256</p> <p><strong>输入对的真值得分差δ</strong>在第i组，即δ∈ （ζileft，ζiright）。对于分类标签{lr}和回归标签{σr}的训练数据中的每个视频对，分类任务和回归任务的目标函数可以写成：</p> <p><img src="https://math.now.sh?from=%7B%5Ccolor%7BBlack%7D%20%5Csigma_%7Bi%7D%3D%5Cfrac%7B%5Cdelta-%5Czeta_%7B%5Ctext%20%7Bleft%20%7D%7D%5E%7Bi%7D%7D%7B%5Czeta_%7B%5Ctext%20%7Bright%20%7D%7D%5E%7Bi%7D-%5Czeta_%7B%5Ctext%20%7Bleft%20%7D%7D%5E%7Bi%7D%7D%7D%0A"></p><p><img src="https://math.now.sh?from=%7B%5Ccolor%7BBlack%7D%20J_%7B%5Cmathrm%7Bcls%7D%7D%3D-%5Csum_%7Br%3D0%7D%5E%7BR%7D%5Cbig%28l_%7Br%7D%5Clog(P_%7Br%7D%29%2B(1-l_%7Br%7D)%5Clog(1-P_%7Br%7D)%5Cbig)%7D%0A"></p><p><img src="https://math.now.sh?from=%7B%5Ccolor%7BBlack%7D%20J_%7B%5Cmathrm%7Breg%7D%7D%3D%5Csum_%7Br%3D0%7D%5E%7BR%7D%20%5Cmathbb%7BI%7D%5Cleft%28l_%7Br%7D%3D1%5Cright%29%5Cleft(%5Chat%7B%5Csigma%7D_%7Br%7D-%5Csigma_%7Br%7D%5Cright)%5E%7B2%7D%7D%0A"></p><p>其中{Pr}和{<img src="https://math.now.sh?inline=%5Coverline%20%CF%83_r" style="display:inline-block;margin:0;">}是预测的叶概率和回归结果。视频对的最终目标函数是：</p> <p><img src="https://math.now.sh?from=%7B%5Ccolor%7BBlack%7D%20J%3DJ_%7B%5Cmathrm%7Bcls%7D%7D%2BJ_%7B%5Cmathrm%7Breg%7D%7D%7D%0A"></p></li> <li><p><strong>预测</strong></p> <p><img src="https://math.now.sh?from=%7B%5Ccolor%7BBlack%7D%20R_%7B%7B%5Cbf%20%5Ctheta%7D%7D%28f_%7B(n%2Cm%29%7D)%3D%5Chat%7B%5Csigma%7D_%7Br%5E%7B*%7D%7D(%5Czeta_%7B%5Cmathrm%7Bright%7D%7D%5E%7Br%5E%7B*%7D%7D-%5Czeta_%7B%5Cmathrm%7Bleft%7D%7D%5E%7Br%5E%7B*%7D%7D)%2B%5Czeta_%7B%5Cmathrm%7Bleft%7D%7D%5E%7Br%5E%7B*%7D%7D%2C%7D%0A"></p><p>此处<strong>r∗ 是概率最高的组</strong>。在我们的实现中，我们还采用了多样本投票策略。给定一个输入视频vtest，我们从训练数据中选择<strong>M个样本</strong>，使用这些M个不同的样本{vmtrain}Mm=1构建M对，其得分为{strain}Mm=1。多示例投票的过程可以总结为：</p> <p><img src="https://math.now.sh?from=%7B%5Ccolor%7BBlack%7D%20%5Cbegin%7Baligned%7D%0A"></p></li></ul></li></ul></li></ul> <p>\hat{s}<em>{\text {test }} &amp;=\frac{1}{M} \sum</em>{m=1}^{M} \hat{s}_{\text {test }}^{m}, m=1,2, \ldots, M
\end{aligned}}
$$</p> <ul><li><p><strong>实验</strong></p> <ul><li><p><strong>AQA-7</strong></p> <p>本文模型在在AQA-7的几乎所有类别上都取得了最好的结果。</p> <img title="" src="/assets/img/image-20220619154953297.9388b4b5.png" alt="image-20220619154953297" data-align="center" style="zoom:80%;"></li> <li><p><strong>回归树深度的影响 与 投票样本数量的影响</strong></p> <p>（Diving class of AQA-7 dataset）</p> <p>当深度为<strong>5和6</strong>时，我们的模型性能更好，其中组的总数为32和64。</p> <p>随着M的增加，性能变得更好，方差更低。当M超过<strong>10</strong>时，Sp.Corr.的改善不太显著。</p></li></ul> <img src="/assets/img/image-20220619155324522.5c564fcb.png" title="" alt="image-20220619155324522" data-align="center"> <ul><li><p><strong>MTL-AQA</strong></p> <p>表2显示了现有方法和我们的方法在MTL-AQA数据集上的性能。由于MTL-AQA中的潜水动作可以使用难度（DD）注释，我们还验证了<strong>DD对该数据集的影响</strong>。</p> <p><strong>使用DD标签（动作难度）训练效果变化原因推测</strong>：一个是我们可以选择更合适的样本，另一个是我们的方法可以从难度中挖掘更多关于动作的信息。</p> <img title="" src="/assets/img/image-20220619155527840.a1ed2bdc.png" alt="image-20220619155527840" data-align="center" style="zoom:80%;"></li> <li><p><strong>消融实验</strong></p> <p>通过比较I3D+MLP和I3D+GART，我们可以看到，当使用我们的组感知回归树替换MLP时，在Spearman的秩度量和R-'2度量下，性能分别提高了0.0022和0.028，这证明了GART设计的有效性。当使用我们提出的核心框架替换I3D基线时，性能得到进一步提高。<strong>以上结果证明了我们方法的两个组成部分的有效性。</strong></p> <img title="" src="/assets/img/image-20220619155908277.263c0f5c.png" alt="image-20220619155908277" data-align="center" style="zoom:80%;"></li> <li><p><strong>样例展示</strong></p> <p>基于输入和样本之间的比较，回归树从粗到细确定相对分数。回归树的第一层尝试确定哪个视频更好，下面的层尝试使预测更准确。图中的第一种情况显<strong>示输入和示例之间的差异较大时的行为</strong>，第二种情况<strong>显示差异较小时的行为</strong>。每对示例和输入视频具有相同的难度（DD）。在这两种情况下，我们的模型都能给出令人满意的预测。</p> <img title="" src="/assets/img/image-20220619160057656.17a1bb98.png" alt="image-20220619160057656" data-align="center" style="zoom:80%;"></li> <li><p><strong>可解释性探讨</strong></p> <p>为了进一步证明我们的方法的有效性，我们在MTL-AQA上使用<strong>Grad CAM</strong>可视化了基线模型（I3D+MLP）和我们的最佳模型（CoRe+GART），我们观察到，我们的方法可以聚焦于某些区域（手、身体等），这表明我们的对比回归框架可以减轻背景造成的影响，并更加关注区分部分。</p> <img title="" src="/assets/img/image-20220619160204647.190a6505.png" alt="image-20220619160204647" data-align="center" style="zoom:80%;"></li></ul></li></ul> <h4 id="improving-action-quality-assessment-using-weighted-aggregation-2022-ibpria"><a href="#improving-action-quality-assessment-using-weighted-aggregation-2022-ibpria" class="header-anchor">#</a> Improving Action Quality Assessment using Weighted Aggregation（2022 IbPRIA）</h4> <h4 id="finediving-a-fine-grained-dataset-for-procedure-aware-action-quality-assessment-2022-cvpr-oral"><a href="#finediving-a-fine-grained-dataset-for-procedure-aware-action-quality-assessment-2022-cvpr-oral" class="header-anchor">#</a> FineDiving: A Fine-grained Dataset for Procedure-aware Action Quality Assessment（2022 CVPR Oral）</h4> <blockquote><p><a href="https://www.zhihu.com/question/33632730" target="_blank" rel="noopener noreferrer">跳水的动作代码代表什么含义？ - 知乎 (zhihu.com)<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p><a href="https://github.com/xujinglin/FineDiving" target="_blank" rel="noopener noreferrer">TSA代码<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></blockquote> <ul><li><p><strong>出发点</strong></p> <ul><li><p>AQA通过分析视频中动作的表现来评估动作的执行质量。与传统的动作识别不同，AQA更具有挑战性：动作识别可以从一张或几张图像中识别一个动作，AQA则需要遍历整个动作序列来评估动作的质量。现有的大多数AQA方法都是通过视频的深度特征来回归不同的动作质量得分，然而，在相似背景下评估差异很小的不同动作之间的质量是很困难的。例如，跳水比赛通常都是在水上运动中心拍摄的，并且视频中所有运动员都执行相同的动作程式：起跳、空中动作、入水。这些动作程式的细微差别主要体现在执行空中动作时，运动员翻腾转体的周数、空中姿势以及入水情况（e.g., 水花大小）。捕捉这些细微的差异需要AQA方法不仅能够解析跳水动作的各个步骤，还要明确量化这些步骤的动作执行质量。如果我们仅通过对整个视频的深度特征的得分进行回归来判断动作质量，这将是对动作质量的混乱和不透明的评估，因为我们<strong>无法通过分析动作步骤的表现来解释最终得分</strong>。认知科学表明，人类通过引入细粒度注释和可靠的比较来学习评估动作质量。迫切需要<strong>构建一个细粒度的体育视频数据集</strong>，以鼓励更透明和可靠的AQA评分方法。为了应对这些挑战，我们构建了一个新的竞技体育视频数据集“FineDiving”。</p></li> <li><p>进一步提出了一种<strong>过程感知（procedure-aware）方法</strong>，用于评估精细潜水的动作质量。提出的框架使用新的时间分割注意模块（称为TSA）学习过程感知嵌入，并以细粒度的方式量化查询和示例之间的质量差异。</p></li></ul></li> <li><p><strong>模型</strong></p> <p><strong>整体</strong>：在给定成对查询和示例实例的情况下，我们使用<strong>I3D</strong>提取时空视觉特征，并提出一个时间分割注意力模块（TSA），通过依次完成<strong>过程分割</strong>、<strong>过程感知交叉注意力学习</strong>和<strong>细粒度对比回归</strong>来评估动作质量。时间分割注意模块由步骤转换标签和动作得分标签监督，这引导模型关注与查询步骤一致的示例区域，并量化它们的差异以预测可靠的动作得分。</p> <img src="/assets/img/2022-08-22-14-13-35-image.49386182.png" title="" alt="" data-align="center"> <ul><li><p><strong>问题构建</strong>：给定成对<strong>query X</strong>和 <strong>exemplar Z</strong>实例，我们的过程感知方法被表述为一个回归问题，通过学习新的时间分割注意模块（缩写为TSA）来预测查询视频的动作质量分数。它可以表示为：</p> <p><img src="https://math.now.sh?from=%7B%5Ccolor%7BBlack%7D%20%5Chat%7By%7D_%7BX%7D%3D%5Cmathcal%7BP%7D%28X%2C%20Z%20%5Cmid%20%5CTheta%29%2By_%7BZ%7D%7D%0A"></p><p>其中<img src="https://math.now.sh?inline=%5Cmathcal%7BP%7D%EF%BC%9D%EF%BD%9BF%EF%BC%8CT%EF%BD%9D" style="display:inline-block;margin:0;">表示包含I3D骨干F和TSA模块T的总体框架；<img src="https://math.now.sh?inline=%CE%98" style="display:inline-block;margin:0;">表示<img src="https://math.now.sh?inline=%5Cmathcal%7BP%7D" style="display:inline-block;margin:0;">的可学习参数；<img src="https://math.now.sh?inline=%5Chat%7By%7D_%7BX%7D" style="display:inline-block;margin:0;">是X的预测分数，<img src="https://math.now.sh?inline=y_%7BZ%7D" style="display:inline-block;margin:0;">是Z的地面实况分数。</p></li> <li><p><strong>Procedure Segmentation</strong>：为了将成对查询和示例动作解析为具有语义和时间对应关系的连续步骤，我们首先提出通过识别步骤从一个子动作类型切换到另一个子动作的时间转换来分段动作过程。假设需要识别L个阶跃转变，过程分割组件S通过计算以下公式预测在第t帧处发生阶跃转变的概率：</p> <p><img src="https://math.now.sh?from=%7B%5Ccolor%7BBlack%7D%20%5Cbegin%7Barray%7D%7Bc%7D%0A"></p></li></ul></li></ul> <p>\hat{t}<em>{k}=\underset{\frac{T}{L}(k-1)&lt;t \leq \frac{T}{L} k}{\arg \max } \hat{p}</em>{k}(t)
\end{array}}
$$</p> <div class="language- extra-class"><pre><code>其中$\hat{p}_{k}∈ R^T$是第k步转变的预测概率分布；$\hat{p}_{t}$表示第k步在第t帧处传递的预测概率；$\hat{t}_{k}$是第k步跃迁的预测。

给定第k步过渡的基本真值，即$t_k$，它可以被编码为二进制分布$p_k$，其中$p_k（t_k）=1$，$p_k（t_s）|_{s\ne k}=0$。通过预测$\hat{p}_{k}$和基本真值$p_k$，过程分割问题可以转换为**密集分类问题**，该问题预测每个帧是否是第k步转移的概率。我们计算$\hat{p}_{k}$和$p_k$之间的二元交叉熵损失以优化S，并找到具有最大概率的第k步跃迁的帧。
</code></pre></div><ul><li><p><strong>Procedure-aware Cross-Attention</strong>：通过过程分割，我们基于L步过渡预测，在每个动作过程中获得具有语义和时间对应性的L+1个连续步骤，即<img src="https://math.now.sh?inline=%5C%7BS_l%5EX%2CS_l%5EZ%5C%7D_%7Bl%3D1%7D%5E%7BL%2B1%7D" style="display:inline-block;margin:0;">。我们利用<strong>Transformer</strong>的序列到序列表示能力，通过交叉注意学习成对查询和示例步骤的过程感知嵌入。基于S，将查询和示例动作实例划分为L+1个连续步骤。考虑到两者连续步骤的长度可能不同，我们通过下采样或上采样将它们固定为给定大小，以满足注意模型中 <strong>“查询”和“键”的维度相同的要求</strong>。然后，我们提出了过程感知交叉注意学习来发现成对步骤之间的空间和时间对应关系，并在这两个步骤中生成新的特征。</p> <p><img src="https://math.now.sh?from=%7B%5Ccolor%7BBlack%7D%20%5Cbegin%7Barray%7D%7Bll%7D%0A"></p></li></ul> <p>S_{l}^{r}=\operatorname{MLP}\left(\mathrm{LN}\left(S_{l}^{r^{\prime}}\right)\right)+S_{l}^{r^{\prime}}, &amp; r=1 \cdots R
\end{array}}</p> <p><img src="https://math.now.sh?from=%0A-%20**Fine-grained%20Contrastive%20Regression**%EF%BC%9A%E5%9F%BA%E4%BA%8E%E5%AD%A6%E4%B9%A0%E5%88%B0%E7%9A%84%E8%BF%87%E7%A8%8B%E6%84%9F%E7%9F%A5%E5%B5%8C%E5%85%A5%24S_l%20%28%E6%8C%87%20S_l%5ER%29%24%EF%BC%8C%E6%88%91%E4%BB%AC%E9%80%9A%E8%BF%87%E5%AD%A6%E4%B9%A0%E6%88%90%E5%AF%B9%E6%AD%A5%E9%AA%A4%E7%9A%84%E7%9B%B8%E5%AF%B9%E5%BE%97%E5%88%86%E6%9D%A5%E9%87%8F%E5%8C%96%E6%9F%A5%E8%AF%A2%E5%92%8C%E7%A4%BA%E4%BE%8B%E4%B9%8B%E9%97%B4%E7%9A%84%E6%AD%A5%E9%AA%A4%E5%81%8F%E5%B7%AE%EF%BC%8C%E8%BF%99%E6%8C%87%E5%AF%BCTSA%E6%A8%A1%E5%9D%97%E9%80%9A%E8%BF%87%E5%AD%A6%E4%B9%A0%E7%BB%86%E7%B2%92%E5%BA%A6%E5%AF%B9%E6%AF%94%E5%9B%9E%E5%BD%92%E5%88%86%E9%87%8FR%E6%9D%A5%E8%AF%84%E4%BC%B0%E5%8A%A8%E4%BD%9C%E8%B4%A8%E9%87%8F%E3%80%82%E5%85%B6%E5%85%AC%E5%BC%8F%E4%B8%BA%EF%BC%9A%0A%0A"></p><p>{\color{Black} \hat{y}<em>{X}=\frac{1}{L+1} \sum</em>{l=1}^{L+1} \mathcal{R}\left(S_{l}\right)+y_{Z}}</p> <p><img src="https://math.now.sh?from=%0A-%20%E5%85%B3%E4%BA%8ETSA%E4%B8%ADexemplar%E7%9A%84%E9%80%89%E6%8B%A9%E7%AD%96%E7%95%A5%E3%80%82%E6%A0%B9%E6%8D%AEaction%20type%E4%BB%8E%E8%AE%AD%E7%BB%83%E9%9B%86%E4%B8%AD%E9%80%89%E6%8B%A9exemplar%E3%80%82%E5%9C%A8%E8%AE%AD%E7%BB%83%E9%98%B6%E6%AE%B5%EF%BC%8C%E5%AF%B9%E4%BA%8E%E6%AF%8F%E4%B8%AA%E8%AE%AD%E7%BB%83%E6%A0%B7%E6%9C%AC%EF%BC%88query%EF%BC%89%EF%BC%8C%E4%BB%8E%E5%85%B7%E6%9C%89%E7%9B%B8%E5%90%8Caction%20type%E7%9A%84%E5%85%B6%E4%BB%96%E8%AE%AD%E7%BB%83%E6%A0%B7%E6%9C%AC%E4%B8%AD%E9%9A%8F%E6%9C%BA%E9%80%89%E6%8B%A9%E4%B8%80%E4%B8%AA%E4%BD%9C%E4%B8%BAexemplar%E3%80%82%E5%9C%A8%E6%8E%A8%E7%90%86%E9%98%B6%E6%AE%B5%EF%BC%8C%E9%87%87%E7%94%A8**%E5%A4%9A%E6%A0%B7%E6%9C%AC%E6%8A%95%E7%A5%A8%E7%AD%96%E7%95%A5**%EF%BC%9A%E4%BB%8E%E5%85%B7%E6%9C%89%E7%9B%B8%E5%90%8Caction%20type%E7%9A%84%E8%AE%AD%E7%BB%83%E6%A0%B7%E6%9C%AC%E4%B8%AD%E9%9A%8F%E6%9C%BA%E9%80%89%E6%8B%A9M%E4%B8%AA%E6%A0%B7%E6%9C%AC%E4%BD%9C%E4%B8%BAM%E4%B8%AAexemplars%E3%80%82%0A%0A-%20**%E5%AE%9E%E9%AA%8C**%0A%0A-%20%E4%B8%8E%E7%8E%B0%E6%9C%89AQA%E6%96%B9%E6%B3%95%E5%9C%A8%E7%B2%BE%E7%BB%86%E6%BD%9C%E6%B0%B4%E6%96%B9%E9%9D%A2%E7%9A%84%E6%80%A7%E8%83%BD%E6%AF%94%E8%BE%83%E3%80%82%EF%BC%88w%5C%2Fo%20DN%EF%BC%89%E8%A1%A8%E7%A4%BA%E9%9A%8F%E6%9C%BA%E9%80%89%E6%8B%A9%E6%A0%B7%E6%9C%AC%EF%BC%9B%EF%BC%88w%5C%2FDN%EF%BC%89%E8%A1%A8%E7%A4%BA%E4%BD%BF%E7%94%A8%E6%BD%9C%E6%B0%B4%E6%95%B0%E5%AD%97%E6%9D%A5%E9%80%89%E6%8B%A9%E7%A4%BA%E4%BE%8B%EF%BC%9B%5C%2F%E8%A1%A8%E7%A4%BA%E6%97%A0%E8%BF%87%E7%A8%8B%E5%88%86%E5%89%B2%0A%20%20%0A%20%20%3Cimg%20title%3D%22%22%20src%3D%22.%2Fpicture%2F2022-08-22-15-08-59-image.png%22%20alt%3D%22%22%20data-align%3D%22center%22%20width%3D%22406%22%3E%0A%0A-%20%E7%B2%BE%E7%BB%86%E6%BD%9C%E6%B0%B4%E6%B6%88%E8%9E%8D%E7%A0%94%E7%A9%B6%E3%80%82%5C%2F%E6%8C%87%E7%A4%BA%E6%B2%A1%E6%9C%89%E5%88%86%E6%AE%B5%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%8C%E4%BB%A5%E5%8F%8A%E2%9C%93%20%E8%A1%A8%E7%A4%BA%E4%BD%BF%E7%94%A8%E5%9F%BA%E6%9C%AC%E7%9C%9F%E5%80%BC%E9%98%B6%E8%B7%83%E8%BD%AC%E6%8D%A2%E6%A0%87%E7%AD%BE%E7%9A%84%E6%96%B9%E6%B3%95%0A%20%20%0A%20%20%EF%BC%88%24%5Cmathcal%7BF%7D%2B%5Cmathcal%7BR%7D%24%E5%9F%BA%E7%BA%BF%E4%BD%BF%E7%94%A8I3D%E6%8F%90%E5%8F%96%E6%AF%8F%E4%B8%AA%E8%BE%93%E5%85%A5%E8%A7%86%E9%A2%91%E7%9A%84%E8%A7%86%E8%A7%89%E7%89%B9%E5%BE%81%EF%BC%8C%E5%B9%B6%E9%80%9A%E8%BF%87%E5%85%B7%E6%9C%89ReLU%E9%9D%9E%E7%BA%BF%E6%80%A7%E7%9A%84%E4%B8%89%E5%B1%82MLP%E9%A2%84%E6%B5%8B%E5%88%86%E6%95%B0%EF%BC%8C%E8%AF%A5%E9%A2%84%E6%B5%8B%E9%80%9A%E8%BF%87%E9%A2%84%E6%B5%8B%E5%92%8C%E5%9C%B0%E9%9D%A2%E5%AE%9E%E5%86%B5%E4%B9%8B%E9%97%B4%E7%9A%84MSE%E6%8D%9F%E5%A4%B1%E8%BF%9B%E8%A1%8C%E4%BC%98%E5%8C%96%E3%80%82%E2%8B%86%20%E6%8C%87%E7%A4%BA%E9%87%87%E7%94%A8%E9%9D%9E%E5%AF%B9%E7%A7%B0%E8%AE%AD%E7%BB%83%E7%AD%96%E7%95%A5%E7%9A%84%E5%9F%BA%E7%BA%BF%EF%BC%8C%E4%BB%A5%E5%8F%8A%E2%99%AF%20%E8%A1%A8%E7%A4%BAdive%20numbers%E5%B0%86%E8%BF%9E%E6%8E%A5%E5%88%B0%E7%89%B9%E5%BE%81%E7%9A%84%E5%9F%BA%E7%BA%BF%E3%80%82%EF%BC%89%0A%20%20%0A%20%20%3Cimg%20title%3D%22%22%20src%3D%22.%2Fpicture%2F2022-08-22-15-10-11-image.png%22%20alt%3D%22%22%20data-align%3D%22center%22%20width%3D%22435%22%3E%0A%0A-%20%E6%8A%95%E7%A5%A8%E6%A0%B7%E6%9C%AC%E6%95%B0%E9%87%8F%E7%9A%84%E5%BD%B1%E5%93%8D%0A%20%20%0A%20%20%E9%9A%8F%E7%9D%80M%E7%9A%84%E5%A2%9E%E5%8A%A0%EF%BC%8C%E6%80%A7%E8%83%BD%E5%8F%98%E5%BE%97%E6%9B%B4%E5%A5%BD%EF%BC%8C%E8%80%8C%E8%AE%A1%E7%AE%97%E6%88%90%E6%9C%AC%E6%9B%B4%E5%A4%A7%E3%80%82%E5%BD%93M%26gt%3B10%E6%97%B6%EF%BC%8CSpearman%E7%A7%A9%E7%9B%B8%E5%85%B3%E6%80%A7%E7%9A%84%E6%94%B9%E5%96%84%E5%8F%98%E5%BE%97%E4%B8%8D%E9%82%A3%E4%B9%88%E6%98%BE%E8%91%97%0A%20%20%0A%20%20%3Cimg%20title%3D%22%22%20src%3D%22.%2Fpicture%2F2022-08-22-15-17-25-image.png%22%20alt%3D%22%22%20data-align%3D%22center%22%20width%3D%22460%22%3E%0A%0A%23%23%23%23%20Domain%20Knowledge-Informed%20Self-Supervised%20Representations%20for%20Workout%20Form%20Assessment%EF%BC%882022%EF%BC%89%0A%0A%23%23%203.%20%E6%95%B0%E6%8D%AE%E9%9B%86%0A%0A%23%23%23%23%20%E6%A6%82%E8%BF%B0%0A%0A-%20**%E5%88%86%E7%B1%BB**%0A%0A%E4%B8%BB%E8%A6%81%E6%9C%89%E8%BF%90%E5%8A%A8%E3%80%81%E5%8C%BB%E7%96%97%E3%80%81%E6%97%A5%E5%B8%B8%E7%94%9F%E6%B4%BB%E4%B8%89%E4%B8%AA%E5%9C%BA%E6%99%AF%0A%0A%3Cimg%20title%3D%22%22%20src%3D%22.%2Fpicture%2Fimage-20220614212341116.png%22%20alt%3D%22image-20220614212341116%22%20style%3D%22zoom%3A80%25%3B%22%20data-align%3D%22center%22%3E%0A%0A-%20**%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C%E4%BB%A5%E5%8F%8A%E5%AF%B9%E5%BA%94%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8E%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87**%0A%0A%3Cimg%20src%3D%22.%2Fpicture%2Fimage-20220614220503128.png%22%20title%3D%22%22%20alt%3D%22image-20220614220503128%22%20data-align%3D%22center%22%3E%0A%0A%23%23%23%23%203.1%20AQA-7%0A%0A%3E%20Action%20Quality%20Assessment%20Across%20Multiple%20Actions%EF%BC%88WACV%202019%EF%BC%89%0A%3E%20%0A%3E%20http%3A%2F%2Frtis.oit.unlv.edu%2Fdatasets.html%0A%0A-%20**%E4%BB%8B%E7%BB%8D**%0A%0AAQA-7%E6%95%B0%E6%8D%AE%E9%9B%86%E5%85%B1%E5%8C%85%E5%90%AB%E6%9D%A5%E8%87%AA**7%E4%B8%AA%E8%BF%90%E5%8A%A8%E9%A1%B9%E7%9B%AE**%E7%9A%84**1189%E4%B8%AA%E8%A7%86%E9%A2%91**%EF%BC%9A370%E4%B8%AA%E6%9D%A5%E8%87%AA%E5%8D%95%E4%BA%BA%E8%B7%B3%E6%B0%B4-10%E7%B1%B3%E8%B7%B3%E5%8F%B0%EF%BC%8C176%E4%B8%AA%E6%9D%A5%E8%87%AA%E4%BD%93%E6%93%8D%E8%B7%B3%E9%A9%AC%EF%BC%8C175%E4%B8%AA%E6%9D%A5%E8%87%AA%E5%A4%A7%E5%9E%8B%E7%A9%BA%E4%B8%AD%E6%BB%91%E9%9B%AA%EF%BC%8C206%E4%B8%AA%E6%9D%A5%E8%87%AA%E5%A4%A7%E5%9E%8B%E7%A9%BA%E4%B8%AD%E6%BB%91%E9%9B%AA%E6%9D%BF%EF%BC%8C88%E4%B8%AA%E6%9D%A5%E8%87%AA%E5%90%8C%E6%AD%A5%E8%B7%B3%E6%B0%B4-3%E7%B1%B3%E8%B7%B3%E6%9D%BF%EF%BC%8C91%E4%B8%AA%E6%9D%A5%E8%87%AA%E5%90%8C%E6%AD%A5%E8%B7%B3%E6%B0%B4-10%E7%B1%B3%E8%B7%B3%E5%8F%B0%EF%BC%8C83%E4%B8%AA%E6%9D%A5%E8%87%AA%E8%B9%A6%E5%BA%8A%E3%80%82%E9%81%B5%E5%BE%AA%E4%BA%86%E6%8F%90%E5%87%BA%E8%AF%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E8%AE%BE%E7%BD%AE%EF%BC%8C%E6%8E%92%E9%99%A4%E4%BA%86%E8%B9%A6%E5%BA%8A%E7%B1%BB%EF%BC%8C%E5%9B%A0%E4%B8%BA%E8%B9%A6%E5%BA%8A%E7%B1%BB%E7%9A%84%E8%A7%86%E9%A2%91%E6%AF%94%E5%85%B6%E4%BB%96%E7%B1%BB%E5%88%AB%E7%9A%84%E8%A7%86%E9%A2%91%E9%95%BF%E5%BE%97%E5%A4%9A%E3%80%82%E5%85%B1%E6%9C%89803%E4%B8%AA%E8%AE%AD%E7%BB%83%E7%89%87%E6%AE%B5%E5%92%8C303%E4%B8%AA%E6%B5%8B%E8%AF%95%E7%89%87%E6%AE%B5%E3%80%82**%E5%BE%97%E5%88%86%E4%B8%80%E8%88%AC%E7%94%B1%E5%A4%9A%E4%B8%AA%E5%9B%A0%E7%B4%A0%E5%86%B3%E5%AE%9A%EF%BC%9A%E5%8A%A8%E4%BD%9C%E9%9A%BE%E5%BA%A6%E3%80%81%E6%89%A7%E8%A1%8C%E6%83%85%E5%86%B5%E7%AD%89%20%E3%80%82**%0A%0A%3Cimg%20title%3D%22%22%20src%3D%22.%2Fpicture%2Fimage-20220615105004910.png%22%20alt%3D%22image-20220615105004910%22%20style%3D%22zoom%3A80%25%3B%22%20data-align%3D%22center%22%3E%0A%0A-%20**%E8%AF%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C**%0A%0A%3Cimg%20src%3D%22.%2Fpicture%2Fimage-20220614195614896.png%22%20title%3D%22%22%20alt%3D%22image-20220614195614896%22%20data-align%3D%22center%22%3E%0A%0A-%20**%E6%95%B0%E6%8D%AE%E6%A0%B7%E6%9C%AC**%0A%0A%3Cimg%20title%3D%22%22%20src%3D%22.%2Fpicture%2Fimage-20220615105753778.png%22%20alt%3D%22image-20220615105753778%22%20style%3D%22zoom%3A80%25%3B%22%20data-align%3D%22center%22%3E%0A%0A%23%23%23%23%203.2%20MTL-AQA%0A%0A%3E%20What%20and%20How%20Well%20You%20Performed%3F%20A%20Multitask%20Learning%20Approach%20to%20Action%20Quality%20Assessment%EF%BC%882019%20CVPR%EF%BC%89%0A%3E%20%0A%3E%20http%3A%2F%2Frtis.oit.unlv.edu%2Fdatasets.html%0A%0A-%20**%E4%BB%8B%E7%BB%8D**%0A%0AMTL-AQA%E6%95%B0%E6%8D%AE%E9%9B%86%E6%98%AF**%E7%9B%AE%E5%89%8DAQA%E6%9C%80%E5%A4%A7%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86**%E3%80%82MTL-AQA%E4%B8%AD%E6%9C%89**1412%E4%B8%AA%E7%BB%86%E7%B2%92%E5%BA%A6%E6%A0%B7%E6%9C%AC**%EF%BC%8C%E6%9D%A5%E8%87%AA16%E4%B8%AA%E4%B8%8D%E5%90%8C%E7%9A%84events%EF%BC%8C%E5%85%B7%E6%9C%89%E4%B8%8D%E5%90%8C%E7%9A%84%E8%A7%86%E5%9B%BE%E3%80%82%E8%AF%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E6%B6%B5%E7%9B%96%E4%BA%86%E4%B8%AA%E4%BA%BA%E5%92%8C%E5%90%8C%E6%AD%A5%E6%BD%9C%E6%B0%B4%E5%91%98%E3%80%81%E7%94%B7%E5%A5%B3%E8%BF%90%E5%8A%A8%E5%91%98%E3%80%813%E7%B1%B3%E8%B7%B3%E6%9D%BF%E5%92%8C10%E7%B1%B3%E8%B7%B3%E5%8F%B0%E8%AE%BE%E7%BD%AE%E7%9A%84%E9%A1%B9%E7%9B%AE%E3%80%82%E5%9C%A8%E8%AF%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%AD%EF%BC%8C%E6%8F%90%E4%BE%9B%E4%BA%86%E4%B8%8D%E5%90%8C%E7%B1%BB%E5%9E%8B%E7%9A%84%E6%B3%A8%E9%87%8A%EF%BC%8C%E4%BB%A5%E6%94%AF%E6%8C%81%E5%AF%B9%E4%B8%8D%E5%90%8C%E4%BB%BB%E5%8A%A1%E7%9A%84%E7%A0%94%E7%A9%B6%EF%BC%8C%E5%8C%85%E6%8B%AC**%E8%A1%8C%E5%8A%A8%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BC%B0%E3%80%81%E8%A1%8C%E5%8A%A8%E8%AF%86%E5%88%AB%E5%92%8C%E8%AF%84%E8%AE%BA%E7%94%9F%E6%88%90**%E3%80%82%E6%AD%A4%E5%A4%96%EF%BC%8C%E4%B8%83%E4%BD%8D%E8%AF%84%E5%A7%94%E7%9A%84%E5%8E%9F%E5%A7%8B%E5%88%86%E6%95%B0%E6%B3%A8%E9%87%8A%E5%92%8C%E9%9A%BE%E5%BA%A6%EF%BC%88DD%EF%BC%89%E5%8F%AF%E7%94%A8%E4%BA%8E%E6%AF%8F%E4%B8%AA%E5%8A%A8%E4%BD%9C%E3%80%82%E6%8C%89%E7%85%A7%E6%8F%90%E5%87%BA%E8%AF%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E8%AE%BA%E6%96%87%E4%B8%AD%E5%BB%BA%E8%AE%AE%E7%9A%84%E8%AF%84%E4%BC%B0%E5%8D%8F%E8%AE%AE%EF%BC%8C%E5%B0%86%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%86%E4%B8%BA**1059%E4%B8%AA%E5%A4%A7%E5%B0%8F%E7%9A%84%E8%AE%AD%E7%BB%83%E9%9B%86%E5%92%8C353%E4%B8%AA%E5%A4%A7%E5%B0%8F%E7%9A%84%E6%B5%8B%E8%AF%95%E9%9B%86**%E3%80%82%20%0A%0A%3Cimg%20title%3D%22%22%20src%3D%22.%2Fpicture%2Fimage-20220615134635954.png%22%20alt%3D%22image-20220615134635954%22%20style%3D%22zoom%3A80%25%3B%22%20data-align%3D%22center%22%3E%0A%0A%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB%E5%8C%85%E6%8B%AC**%E4%BA%94%E4%B8%AA%E7%BB%86%E7%B2%92%E5%BA%A6%E7%9A%84%E8%B7%B3%E6%B0%B4%E5%AD%90%E8%AF%86%E5%88%AB%E4%BB%BB%E5%8A%A1**%EF%BC%9A%E8%AF%86%E5%88%AB%E4%BD%8D%E7%BD%AE%E5%92%8C%E6%97%8B%E8%BD%AC%E7%B1%BB%E5%9E%8B%EF%BC%8C%E6%A3%80%E6%B5%8B%E8%87%82%E6%9E%B6%EF%BC%8C%E8%AE%A1%E7%AE%97%E7%BF%BB%E8%85%BE%E5%92%8C%E6%89%AD%E8%BD%AC%E3%80%82%0A%0A%3Cimg%20title%3D%22%22%20src%3D%22.%2Fpicture%2Fimage-20220615134655747.png%22%20alt%3D%22image-20220615134655747%22%20style%3D%22zoom%3A80%25%3B%22%20data-align%3D%22center%22%3E%0A%0A-%20**%E8%AF%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C**%0A%0A%EF%BC%88%E4%BA%BA%E7%B1%BB%E4%B8%93%E5%AE%B6%E6%B0%B4%E5%B9%B396%25%EF%BC%89%0A%0A%3Cimg%20src%3D%22.%2Fpicture%2Fimage-20220614195810695.png%22%20title%3D%22%22%20alt%3D%22image-20220614195810695%22%20data-align%3D%22center%22%3E%0A%0A-%20**%E6%95%B0%E6%8D%AE%E6%A0%B7%E6%9C%AC**%0A%0A%3Cimg%20title%3D%22%22%20src%3D%22.%2Fpicture%2Fdiving_sample.gif%22%20alt%3D%22diving_video%22%20style%3D%22zoom%3A%2080%25%3B%22%20data-align%3D%22center%22%3E%0A%0A%23%23%23%23%203.3%20FineDiving%0A%0A%3E%20FineDiving%3A%20A%20Fine-grained%20Dataset%20for%20Procedure-aware%20Action%20Quality%20Assessment%EF%BC%882022%20CVPR%EF%BC%89%0A%0A-%20**%E4%BB%8B%E7%BB%8D**%0A%0A-%20%E6%94%B6%E9%9B%86%E4%BA%86%E5%A5%A5%E8%BF%90%E4%BC%9A%E3%80%81%E4%B8%96%E7%95%8C%E6%9D%AF%E3%80%81%E4%B8%96%E9%94%A6%E8%B5%9B%E4%BB%A5%E5%8F%8A%E6%AC%A7%E9%94%A6%E8%B5%9B%E7%9A%84%E8%B7%B3%E6%B0%B4%E9%A1%B9%E7%9B%AE%E6%AF%94%E8%B5%9B%E8%A7%86%E9%A2%91%E3%80%82%20%E6%AF%8F%E4%B8%AA%E6%AF%94%E8%B5%9B%E8%A7%86%E9%A2%91%E9%83%BD%E6%8F%90%E4%BE%9B%E4%BA%86%E4%B8%B0%E5%AF%8C%E7%9A%84%E5%86%85%E5%AE%B9%EF%BC%8C%E5%8C%85%E6%8B%AC%E6%89%80%E6%9C%89%E8%BF%90%E5%8A%A8%E5%91%98%E7%9A%84%E8%B7%B3%E6%B0%B4%E8%AE%B0%E5%BD%95%E3%80%81%E4%B8%8D%E5%90%8C%E8%A7%86%E8%A7%92%E7%9A%84%E6%85%A2%E9%80%9F%E5%9B%9E%E6%94%BE%E7%AD%89%E3%80%82%0A%0A-%20%E6%88%91%E4%BB%AC%E6%9E%84%E5%BB%BA%E4%BA%86%E4%B8%80%E4%B8%AA%E7%94%B1%E8%AF%AD%E4%B9%89%E5%92%8C%E6%97%B6%E9%97%B4%E7%BB%93%E6%9E%84%E7%BB%84%E7%BB%87%E7%9A%84%E7%BB%86%E7%B2%92%E5%BA%A6%E8%A7%86%E9%A2%91%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%8C%E5%85%B6%E4%B8%AD%E6%AF%8F%E4%B8%AA%E7%BB%93%E6%9E%84%E5%8C%85%E5%90%AB%E4%B8%A4%E4%B8%AA%E5%B1%82%E6%AC%A1%E7%9A%84%E6%B3%A8%E9%87%8A%E3%80%82%E7%BB%99%E5%AE%9A%E4%B8%80%E4%B8%AA%E5%8E%9F%E5%A7%8B%E6%BD%9C%E6%B0%B4%E8%A7%86%E9%A2%91%EF%BC%8C%E6%B3%A8%E9%87%8A%E5%99%A8%E4%BD%BF%E7%94%A8%E6%88%91%E4%BB%AC%E5%AE%9A%E4%B9%89%E7%9A%84%E8%AF%8D%E6%B1%87%E6%9D%A5%E6%A0%87%E8%AE%B0%E6%AF%8F%E4%B8%AA%E5%8A%A8%E4%BD%9C%E5%8F%8A%E5%85%B6%E8%BF%87%E7%A8%8B%E3%80%82%E6%88%91%E4%BB%AC%E9%9C%80%E8%A6%81%E5%AE%8C%E6%88%90%E4%BB%8E%E7%B2%97%E7%B2%92%E5%BA%A6%E5%88%B0%E7%BB%86%E7%B2%92%E5%BA%A6%E7%9A%84%E4%B8%A4%E4%B8%AA%E6%B3%A8%E9%87%8A%E9%98%B6%E6%AE%B5%E3%80%82%E7%B2%97%E7%B2%92%E5%BA%A6%E9%98%B6%E6%AE%B5%E6%98%AF%E6%A0%87%E8%AE%B0%E6%AF%8F%E4%B8%AA%E5%8A%A8%E4%BD%9C%E5%AE%9E%E4%BE%8B%E7%9A%84**%E5%8A%A8%E4%BD%9C%E7%B1%BB%E5%9E%8B**%E5%8F%8A%E5%85%B6**%E6%97%B6%E9%97%B4%E8%BE%B9%E7%95%8C**%EF%BC%8C%E5%B9%B6%E9%99%84%E5%B8%A6%E5%AE%98%E6%96%B9**%E5%88%86%E6%95%B0**%E3%80%82%E7%BB%86%E7%B2%92%E5%BA%A6%E9%98%B6%E6%AE%B5%E6%98%AF%E6%A0%87%E8%AE%B0%E5%8A%A8%E4%BD%9C%E8%BF%87%E7%A8%8B%E4%B8%AD%E6%AF%8F%E4%B8%AA**%E6%AD%A5%E9%AA%A4%E7%9A%84%E5%AD%90%E5%8A%A8%E4%BD%9C%E7%B1%BB%E5%9E%8B**%EF%BC%8C%E5%B9%B6%E8%AE%B0%E5%BD%95**%E6%AF%8F%E4%B8%AA%E6%AD%A5%E9%AA%A4%E7%9A%84%E8%B5%B7%E5%A7%8B%E5%B8%A7**%0A%20%20%0A%20%20-%20%E5%9C%A8%E8%AF%AD%E4%B9%89%E7%BB%93%E6%9E%84%E4%B8%AD%EF%BC%8C**%E5%8A%A8%E4%BD%9C%E7%BA%A7%E6%A0%87%E7%AD%BE**%EF%BC%88action-level%20labels%EF%BC%89%E6%8F%8F%E8%BF%B0%E8%BF%90%E5%8A%A8%E5%91%98%E7%9A%84%E5%8A%A8%E4%BD%9C%E7%B1%BB%E5%9E%8B%EF%BC%8C**%E6%AD%A5%E9%AA%A4%E7%BA%A7%E6%A0%87%E7%AD%BE**%EF%BC%88step-level%20labels%EF%BC%89%E6%8F%8F%E8%BF%B0%E7%A8%8B%E5%BA%8F%E4%B8%AD%E8%BF%9E%E7%BB%AD%E6%AD%A5%E9%AA%A4%E7%9A%84%E5%AD%90%E5%8A%A8%E4%BD%9C%E7%B1%BB%E5%9E%8B%EF%BC%8C%E5%85%B6%E4%B8%AD%E6%AF%8F%E4%B8%AA%E5%8A%A8%E4%BD%9C%E7%A8%8B%E5%BA%8F%E4%B8%AD%E7%9A%84%E7%9B%B8%E9%82%BB%E6%AD%A5%E9%AA%A4%E5%B1%9E%E4%BA%8E%E4%B8%8D%E5%90%8C%E7%9A%84%E5%AD%90%E5%8A%A8%E4%BD%9C%E3%80%82%E5%AD%90%E5%8A%A8%E4%BD%9C%E7%B1%BB%E5%9E%8B%E7%9A%84%E7%BB%84%E5%90%88%E7%94%9F%E6%88%90%E5%8A%A8%E4%BD%9C%E7%B1%BB%E5%9E%8B%E3%80%82%E4%BE%8B%E5%A6%82%EF%BC%8C%E5%AF%B9%E4%BA%8E%E5%8A%A8%E4%BD%9C%E7%B1%BB%E5%9E%8B%E2%80%9C5255B%E2%80%9D%EF%BC%8C%E5%B1%9E%E4%BA%8E%E5%AD%90%E5%8A%A8%E4%BD%9C%E7%B1%BB%E2%80%9CBack%E2%80%9D%2C%20%E2%80%9C2.5%20Somersaults%20Pike%E2%80%9D%2C%20%E5%92%8C%E2%80%9C2.5%20Twists%E2%80%9D%E7%9A%84%E6%AD%A5%E9%AA%A4%E4%BE%9D%E6%AC%A1%E6%89%A7%E8%A1%8C%E3%80%82%0A%20%20%20%20%0A%20%20%20%20%3Cimg%20title%3D%22%22%20src%3D%22.%2Fpicture%2F2022-08-22-11-14-41-image.png%22%20alt%3D%22%22%20data-align%3D%22center%22%20width%3D%22522%22%3E%0A%20%20%0A%20%20-%20%E5%9C%A8%E6%97%B6%E9%97%B4%E7%BB%93%E6%9E%84%E4%B8%AD%EF%BC%8C%E5%8A%A8%E4%BD%9C%E7%BA%A7%E5%88%AB%E6%A0%87%E7%AD%BE**%E5%AE%9A%E4%BD%8D%E8%BF%90%E5%8A%A8%E5%91%98%E6%89%A7%E8%A1%8C%E7%9A%84%E5%AE%8C%E6%95%B4%E5%8A%A8%E4%BD%9C%E5%AE%9E%E4%BE%8B%E7%9A%84%E6%97%B6%E9%97%B4%E8%BE%B9%E7%95%8C**%E3%80%82%E5%9C%A8%E8%BF%99%E4%B8%AA%E6%B3%A8%E9%87%8A%E8%BF%87%E7%A8%8B%E4%B8%AD%EF%BC%8C%E6%88%91%E4%BB%AC%E4%B8%A2%E5%BC%83%E6%89%80%E6%9C%89%E4%B8%8D%E5%AE%8C%E6%95%B4%E7%9A%84%E5%8A%A8%E4%BD%9C%E5%AE%9E%E4%BE%8B%EF%BC%8C%E5%B9%B6%E8%BF%87%E6%BB%A4%E6%8E%89%E6%85%A2%E5%9B%9E%E6%94%BE%E3%80%82%E6%AD%A5%E9%AA%A4%E7%BA%A7%E6%A0%87%E7%AD%BE%E6%98%AF%E5%8A%A8%E4%BD%9C%E8%BF%87%E7%A8%8B%E4%B8%AD%E8%BF%9E%E7%BB%AD**%E6%AD%A5%E9%AA%A4%E7%9A%84%E8%B5%B7%E5%A7%8B%E5%B8%A7**%E3%80%82%E4%BE%8B%E5%A6%82%EF%BC%8C%E5%AF%B9%E4%BA%8E%E5%B1%9E%E4%BA%8E%E2%80%9C5152B%E2%80%9D%E7%B1%BB%E5%9E%8B%E7%9A%84%E5%8A%A8%E4%BD%9C%EF%BC%8C%E8%BF%9E%E7%BB%AD%E6%AD%A5%E9%AA%A4%E7%9A%84%E8%B5%B7%E5%A7%8B%E5%B8%A7%E5%88%86%E5%88%AB%E4%B8%BA18930%E3%80%8118943%E3%80%8118957%E3%80%8118967%E5%92%8C18978%E3%80%82%0A%20%20%20%20%0A%20%20%20%20%3Cimg%20src%3D%22.%2Fpicture%2F2022-08-22-11-15-15-image.png%22%20title%3D%22%22%20alt%3D%22%22%20data-align%3D%22center%22%3E%0A%0A-%20**%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BB%9F%E8%AE%A1**%0A%0A%E7%B2%BE%E7%BB%86%E8%B7%B3%E6%B0%B4%E6%95%B0%E6%8D%AE%E9%9B%86%E7%94%B13000%E4%B8%AA%E8%A7%86%E9%A2%91%E6%A0%B7%E6%9C%AC%E7%BB%84%E6%88%90%EF%BC%8C%E6%B6%B5%E7%9B%9652%E4%B8%AA%E5%8A%A8%E4%BD%9C%E7%B1%BB%E5%9E%8B%E3%80%8129%E4%B8%AA%E5%AD%90%E5%8A%A8%E4%BD%9C%E7%B1%BB%E5%9E%8B%E5%92%8C23%E4%B8%AA%E9%9A%BE%E5%BA%A6%E7%B1%BB%E5%9E%8B%E3%80%82%E8%BF%99%E4%BA%9B%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E5%B0%86%E6%9C%89%E5%8A%A9%E4%BA%8E%E8%AE%BE%E8%AE%A1%E7%AB%9E%E8%B5%9B%E7%AD%96%E7%95%A5%EF%BC%8C%E6%9B%B4%E5%A5%BD%E5%9C%B0%E5%8F%91%E6%8C%A5%E8%BF%90%E5%8A%A8%E5%91%98%E7%9A%84%E4%BC%98%E5%8A%BF%E3%80%82%0A%0A%3Cimg%20title%3D%22%22%20src%3D%22.%2Fpicture%2F2022-08-22-11-21-24-image.png%22%20alt%3D%22%22%20data-align%3D%22center%22%20width%3D%22464%22%3E%0A%0A%E4%B8%8B%E8%A1%A8%E6%8A%A5%E5%91%8A%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E6%9B%B4%E8%AF%A6%E7%BB%86%E4%BF%A1%E6%81%AF%EF%BC%8C%E5%B9%B6%E5%B0%86%E5%85%B6%E4%B8%8E%E7%8E%B0%E6%9C%89%E7%9A%84AQA%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%A5%E5%8F%8A%E5%85%B6%E4%BB%96%E7%BB%86%E7%B2%92%E5%BA%A6%E4%BD%93%E8%82%B2%E6%95%B0%E6%8D%AE%E9%9B%86%E8%BF%9B%E8%A1%8C%E4%BA%86%E6%AF%94%E8%BE%83%E3%80%82%E8%AF%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E5%9C%A8%E6%B3%A8%E9%87%8A%E7%B1%BB%E5%9E%8B%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86%E8%A7%84%E6%A8%A1%E6%96%B9%E9%9D%A2%E4%B8%8E%E7%8E%B0%E6%9C%89%E7%9A%84AQA%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8D%E5%90%8C%E3%80%82%E4%BE%8B%E5%A6%82%EF%BC%8CMIT%20Dive%E3%80%81UNLV%E5%92%8CAQA-7Dive%E4%BB%85%E6%8F%90%E4%BE%9B%E5%8A%A8%E4%BD%9C%E5%88%86%E6%95%B0%EF%BC%8C%E8%80%8C%E8%AF%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E6%8F%90%E4%BE%9B%E7%BB%86%E7%B2%92%E5%BA%A6%E6%B3%A8%E9%87%8A%EF%BC%8C%E5%8C%85%E6%8B%AC%E5%8A%A8%E4%BD%9C%E7%B1%BB%E5%9E%8B%E3%80%81%E5%AD%90%E5%8A%A8%E4%BD%9C%E7%B1%BB%E5%9E%8B%E3%80%81%E7%B2%97%E7%B2%92%E5%BA%A6%E5%92%8C%E7%BB%86%E7%B2%92%E5%BA%A6%E6%97%B6%E9%97%B4%E8%BE%B9%E7%95%8C%E4%BB%A5%E5%8F%8A%E5%8A%A8%E4%BD%9C%E5%88%86%E6%95%B0%E3%80%82MTL-AQA%E6%8F%90%E4%BE%9B%E7%B2%97%E7%B2%92%E5%BA%A6%E6%B3%A8%E9%87%8A%EF%BC%8C%E5%8D%B3%E5%8A%A8%E4%BD%9C%E7%B1%BB%E5%9E%8B%E5%92%8C%E6%97%B6%E9%97%B4%E8%BE%B9%E7%95%8C%E3%80%82%E7%94%B1%E4%BA%8E%E7%BC%BA%E4%B9%8F%E5%8A%A8%E4%BD%9C%E5%BE%97%E5%88%86%EF%BC%8C%E5%85%B6%E4%BB%96%E7%BB%86%E7%B2%92%E5%BA%A6%E8%BF%90%E5%8A%A8%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8D%E8%83%BD%E7%94%A8%E4%BA%8E%E8%AF%84%E4%BC%B0%E5%8A%A8%E4%BD%9C%E8%B4%A8%E9%87%8F%E3%80%82%E6%88%91%E4%BB%AC%E7%9C%8B%E5%88%B0**FineDiving%E6%98%AFAQA%E4%BB%BB%E5%8A%A1%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%BB%86%E7%B2%92%E5%BA%A6%E8%BF%90%E5%8A%A8%E8%A7%86%E9%A2%91%E6%95%B0%E6%8D%AE%E9%9B%86**%EF%BC%8C%E5%A1%AB%E8%A1%A5%E4%BA%86AQA%E4%B8%AD%E7%BB%86%E7%B2%92%E5%BA%A6%E6%B3%A8%E9%87%8A%E7%9A%84%E7%A9%BA%E7%99%BD%E3%80%82%0A%0A%3Cimg%20title%3D%22%22%20src%3D%22.%2Fpicture%2F2022-08-22-11-23-33-image.png%22%20alt%3D%22%22%20data-align%3D%22center%22%20width%3D%22529%22%3E%0A%0A-%20**%E8%AF%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C**%0A%0A%3Cimg%20title%3D%22%22%20src%3D%22.%2Fpicture%2Fimage-20220615130926213.png%22%20alt%3D%22image-20220615130926213%22%20data-align%3D%22center%22%20width%3D%22389%22%3E%0A%0A-%20**%E6%95%B0%E6%8D%AE%E6%A0%B7%E6%9C%AC**%0A%0A%3Cimg%20src%3D%22.%2Fpicture%2F2022-08-22-11-24-30-image.png%22%20title%3D%22%22%20alt%3D%22%22%20data-align%3D%22center%22%3E%0A%0A%23%23%23%23%203.4%20Squat%20Dataset%0A%0A%3E%20Temporal%20Distance%20Matrices%20for%20Squat%20Classification%EF%BC%882019%20CVPRW%EF%BC%89%0A%0A-%20**%E4%BB%8B%E7%BB%8D**%0A%0A%3Cmark%3E%E5%88%86%E7%B1%BB%E3%80%81%E5%9F%BA%E4%BA%8E%E9%AA%A8%E9%AA%BC%3C%2Fmark%3E%0A%0A%E6%8F%90%E4%BE%9B%E4%BA%86%E5%9B%9B%E4%B8%AA%E8%A7%86%E9%A2%91%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%8C%E7%94%A8%E4%BA%8E%E5%88%86%E7%B1%BB%E5%A5%BD%E7%9A%84%E8%B9%B2%E5%A7%BF%E5%92%8C%E4%B8%8D%E5%90%8C%E7%9A%84%E5%9D%8F%E8%B9%B2%E5%A7%BF%EF%BC%9A%E5%8D%95%E4%B8%AA%E4%B8%AA%E4%BD%93%E3%80%81%E5%A4%9A%E4%B8%AA%E4%B8%AA%E4%BD%93%E3%80%81%E8%83%8C%E6%99%AF%E5%8F%98%E5%8C%96%E5%92%8CYouTube%E6%95%B0%E6%8D%AE%E9%9B%86%E3%80%82%E5%A6%82%E4%B8%8B%E6%80%BB%E7%BB%93%E4%BA%86%E5%9B%9B%E8%80%85%E4%B9%8B%E9%97%B4%E7%9A%84%E5%B7%AE%E5%BC%82%E3%80%82%20%0A%0A%3Cimg%20title%3D%22%22%20src%3D%22.%2Fpicture%2Fimage-20220615141833172.png%22%20alt%3D%22image-20220615141833172%22%20style%3D%22zoom%3A80%25%3B%22%20data-align%3D%22center%22%3E%0A%0A**Single%20Individual%20Dataset**%EF%BC%9A%E6%AF%8F%E4%B8%AA%E6%A0%87%E7%AD%BE%E7%9A%84%E8%A7%86%E9%A2%91%E6%95%B0%E9%87%8F%EF%BC%8C%E5%88%86%E4%B8%BA%E6%B5%8B%E8%AF%95%E3%80%81%E5%9F%B9%E8%AE%AD%E5%92%8C%E9%AA%8C%E8%AF%81%E5%AD%90%E9%9B%86%E3%80%82%20%0A%0A%3Cimg%20title%3D%22%22%20src%3D%22.%2Fpicture%2Fimage-20220615141937715.png%22%20alt%3D%22image-20220615141937715%22%20style%3D%22zoom%3A80%25%3B%22%20data-align%3D%22center%22%3E%0A%0A-%20**%E6%95%B0%E6%8D%AE%E6%A0%B7%E6%9C%AC**%0A%0A%3Cimg%20title%3D%22%22%20src%3D%22.%2Fpicture%2Fimage-20220615141740241.png%22%20alt%3D%22image-20220615141740241%22%20style%3D%22zoom%3A80%25%3B%22%20data-align%3D%22center%22%3E%0A%0A%23%23%23%23%203.5%20Fitness-AQA%EF%BC%88%E6%9C%AA%E5%85%AC%E5%BC%80%EF%BC%89%0A%0A%3E%20Domain%20Knowledge-Informed%20Self-Supervised%20Representations%20for%20Workout%20Form%20Assessment%EF%BC%882022%EF%BC%89%0A%0A-%20**%E4%BB%8B%E7%BB%8D**%0A%0A%3Cimg%20title%3D%22%22%20src%3D%22.%2Fpicture%2Fimage-20220615144246407.png%22%20alt%3D%22image-20220615144246407%22%20style%3D%22zoom%3A80%25%3B%22%20data-align%3D%22center%22%3E%0A%0A%23%23%204.%20%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87%0A%0A%23%23%23%23%204.1%20Spearman's%20rank%20correlation%20coefficient%0A%0A%E6%96%AF%E7%9A%AE%E5%B0%94%E6%9B%BC%E6%8E%92%E5%BA%8F%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0%E7%9B%B8%E5%85%B3%E4%BB%8B%E7%BB%8D%EF%BC%9A%5B%E6%96%AF%E7%9A%AE%E5%B0%94%E6%9B%BC%E7%AD%89%E7%BA%A7%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0Spearman's%20rank%20correlation%20coefficient%20-%20%E7%9F%A5%E4%B9%8E%20%28zhihu.com%29%5D(https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F339079547)%0A%0A"></p><p>{\color{Black} \rho=\frac{\sum\left(p_{i}-\bar{p}\right)\left(q_{i}-\bar{q}\right)}{\sqrt{\sum\left(p_{i}-\bar{p}\right)^{2} \sum\left(q_{i}-\bar{q}\right)^{2}}}}</p> <p><img src="https://math.now.sh?from=%0A**p%E5%92%8Cq**%E5%88%86%E5%88%AB%E8%A1%A8%E7%A4%BA%E7%9C%9F%E5%AE%9E%E5%92%8C%E9%A2%84%E6%B5%8B%E5%BE%97%E5%88%86%E5%BA%8F%E5%88%97%E7%9A%84%E6%8E%92%E5%90%8D%E3%80%82**%CF%81**%E5%8F%96%E5%80%BC%E8%8C%83%E5%9B%B4%E4%B8%BA%20**%5B-1%2C1%5D**%EF%BC%8C%E8%B6%8A%E5%A4%A7%E6%95%88%E6%9E%9C%E8%B6%8A%E5%A5%BD%E3%80%82**Fisher%E2%80%99s%20z-value**%E8%A2%AB%E7%94%A8%E4%BA%8E%E5%8E%BB%E8%A1%A1%E9%87%8F%E5%A4%9A%E4%B8%AA%E8%A1%8C%E4%B8%BA%E7%9A%84%E5%B9%B3%E5%9D%87%E6%95%88%E6%9E%9C%E3%80%82%20%C2%A0%C2%A0%C2%A0%C2%A0%0A%0A**Fisher%20transformation**%0A%0A%3E%20%5B%E8%B4%B9%E9%9B%AA%E5%8F%98%E6%8D%A2_%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91%20%28baidu.com%29%5D(https%3A%2F%2Fbaike.baidu.com%2Fitem%2F%E8%B4%B9%E9%9B%AA%E5%8F%98%E6%8D%A2%2F23156580)%0A%0A"></p><p>{\color{Black} z:={\frac{1}{2}}\ln\left({\frac{1+r}{1-r}}\right)=\operatorname{arctanh}(r)}</p> <p><img src="https://math.now.sh?from=%0A%E5%B9%B3%E5%9D%87%E5%90%8E%E5%86%8D%E9%80%86%E5%8F%98%E6%8D%A2%EF%BC%9A%0A%0A"></p><p>{\color{Black} r={\frac{\exp(2z)-1}{\exp(2z)+1}}=\operatorname{tanh}(z)}</p> <p><img src="https://math.now.sh?from=%0A%23%23%23%23%204.2%20%20relative%20L2-distance%20%28R-l2%29%0A%0A%3E%20Group-aware%20Contrastive%20Regression%20for%20Action%20Quality%20Assessment%EF%BC%882021%20ICCV%EF%BC%89%0A%0A"></p><p>{\color{Black} {\mathrm R}-{\it\ell}<em>{2}(\theta)={\frac{1}{K}}\sum</em>{k=1}^{K}\left({\frac{|s_{k}-\hat{s}<em>{k}|}{s</em>{\mathrm{max}}-s_{\mathrm{min}}}}\right)^{2},}</p> <p><img src="https://math.now.sh?from=%0A%E5%85%B6%E4%B8%AD%24s_k%24%E5%92%8C%24%CB%86s_k%24%E5%88%86%E5%88%AB%E8%A1%A8%E7%A4%BA%E7%AC%ACk%E4%B8%AA%E6%A0%B7%E6%9C%AC%E7%9A%84%E5%9C%B0%E9%9D%A2%E7%9C%9F%E5%80%BC%E5%BE%97%E5%88%86%E5%92%8C%E9%A2%84%E6%B5%8B%E3%80%82%E6%88%91%E4%BB%AC%E4%BD%BF%E7%94%A8%24R-l_2%24%E4%BB%A3%E6%9B%BF%E4%BC%A0%E7%BB%9F%E7%9A%84L2%E8%B7%9D%E7%A6%BB%EF%BC%8C**%E5%9B%A0%E4%B8%BA%E4%B8%8D%E5%90%8C%E7%9A%84%E5%8A%A8%E4%BD%9C%E6%9C%89%E4%B8%8D%E5%90%8C%E7%9A%84%E5%BE%97%E5%88%86%E9%97%B4%E9%9A%94**%E3%80%82%E6%AF%94%E8%BE%83%E5%92%8C%E5%B9%B3%E5%9D%87%E4%B8%8D%E5%90%8C%E7%B1%BB%E5%88%AB%E5%8A%A8%E4%BD%9C%E4%B9%8B%E9%97%B4%E7%9A%84%E8%B7%9D%E7%A6%BB%E6%98%AF%E6%AF%AB%E6%97%A0%E6%84%8F%E4%B9%89%E5%92%8C%E4%BB%A4%E4%BA%BA%E5%9B%B0%E6%83%91%E7%9A%84%E3%80%82%24R-l_2%24%E4%B8%8E%E6%96%AF%E7%9A%AE%E5%B0%94%E6%9B%BC%E7%9A%84%E7%9B%B8%E5%85%B3%E6%80%A7%E4%B8%8D%E5%90%8C%EF%BC%9A%E6%96%AF%E7%9A%AE%E5%B0%94%E6%9B%BC%E7%9A%84%E7%9B%B8%E5%85%B3%E6%80%A7%E6%9B%B4%E5%85%B3%E6%B3%A8%E9%A2%84%E6%B5%8B%E5%BE%97%E5%88%86%E7%9A%84%E6%8E%92%E5%BA%8F%EF%BC%8C%E8%80%8C%24R-l_2%24%E5%85%B3%E6%B3%A8%E7%9A%84%E6%98%AF%E6%95%B0%E5%80%BC%E3%80%82%20%0A"></p></div></div> <!----> <div class="page-edit"><div class="edit-link"><a href="https://github.com/xzhouzeng/vuepress-theme-vdoing/edit/master/docs/01.文档/02.动作质量评估/01.动作质量评估.md" target="_blank" rel="noopener noreferrer">编辑</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <div class="tags"><a href="/tags/?tag=%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90" title="标签">#行为分析</a><a href="/tags/?tag=CV" title="标签">#CV</a><a href="/tags/?tag=%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB" title="标签">#论文解读</a></div> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2023/05/26, 03:03:27</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/pages/1186a5/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">2D-3D-Lifting</div></a> <a href="/pages/1aa734/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">基于RGBD视觉信息的异常行为识别</div></a></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/pages/1186a5/" class="prev">2D-3D-Lifting</a></span> <span class="next"><a href="/pages/1aa734/">基于RGBD视觉信息的异常行为识别</a>→
      </span></p></div></div></div> <div class="article-list"><div class="article-title"><a href="/archives/" class="iconfont icon-bi">最近更新</a></div> <div class="article-wrapper"><dl><dd>01</dd> <dt><a href="/pages/e5511e/"><div>
            VideoLLMs
            <!----></div></a> <span class="date">03-20</span></dt></dl><dl><dd>02</dd> <dt><a href="/pages/6b730d/"><div>
            Video2Script
            <!----></div></a> <span class="date">12-07</span></dt></dl><dl><dd>03</dd> <dt><a href="/pages/ca7dd0/"><div>
            多模态
            <!----></div></a> <span class="date">11-09</span></dt></dl> <dl><dd></dd> <dt><a href="/archives/" class="more">更多文章&gt;</a></dt></dl></div></div></main></div> <div class="footer"><div class="icons"><a href="xzhouzeng@zju.edu.cn" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://github.com/xzhouzeng" title="GitHub" target="_blank" class="iconfont icon-github"></a><a href="/pages/b94dba/" title="听音乐" target="_blank" class="iconfont icon-erji"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2022-2024
    <span>xzhouzeng | <a href="https://github.com/xzhouzeng/vuepress-theme-vdoing/blob/master/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <!----> <!----> <!----></div><div class="global-ui"><div></div></div></div>
    <script src="/assets/js/app.9749b4f6.js" defer></script><script src="/assets/js/4.5da4c663.js" defer></script><script src="/assets/js/6.2484af9c.js" defer></script>
  </body>
</html>

<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>大模型应用 | XZ Blog</title>
    <meta name="generator" content="VuePress 1.9.5">
    <link rel="icon" href="/img/favicon.ico">
    <meta name="description" content="Learning notes and sharing">
    <meta name="keywords" content="后端开发,技术文档,论文解读,AI,个人博客,markdown">
    <meta name="baidu-site-verification" content="7F55weZDDc">
    <meta name="theme-color" content="#11a8cd">
    
    <link rel="preload" href="/assets/css/0.styles.b7b742a8.css" as="style"><link rel="preload" href="/assets/js/app.9749b4f6.js" as="script"><link rel="preload" href="/assets/js/4.5da4c663.js" as="script"><link rel="preload" href="/assets/js/9.3f132d53.js" as="script"><link rel="prefetch" href="/assets/js/10.e0104b9a.js"><link rel="prefetch" href="/assets/js/11.63bdaef5.js"><link rel="prefetch" href="/assets/js/12.25c3ab74.js"><link rel="prefetch" href="/assets/js/13.248068ad.js"><link rel="prefetch" href="/assets/js/14.f4e65a02.js"><link rel="prefetch" href="/assets/js/15.3efbd5c7.js"><link rel="prefetch" href="/assets/js/16.a996b679.js"><link rel="prefetch" href="/assets/js/17.b929c3be.js"><link rel="prefetch" href="/assets/js/18.f56edfb3.js"><link rel="prefetch" href="/assets/js/19.42d6d76e.js"><link rel="prefetch" href="/assets/js/2.85cbe54a.js"><link rel="prefetch" href="/assets/js/20.4105fdfa.js"><link rel="prefetch" href="/assets/js/21.9f4fbcc5.js"><link rel="prefetch" href="/assets/js/22.0d2b66df.js"><link rel="prefetch" href="/assets/js/23.5201c1d6.js"><link rel="prefetch" href="/assets/js/24.ca69a44f.js"><link rel="prefetch" href="/assets/js/25.4a9fa551.js"><link rel="prefetch" href="/assets/js/26.355da810.js"><link rel="prefetch" href="/assets/js/27.1c48e68c.js"><link rel="prefetch" href="/assets/js/28.6f08e3bc.js"><link rel="prefetch" href="/assets/js/29.4c6ebaf4.js"><link rel="prefetch" href="/assets/js/3.cbaeda42.js"><link rel="prefetch" href="/assets/js/30.54b4d1bf.js"><link rel="prefetch" href="/assets/js/31.47c1a1ac.js"><link rel="prefetch" href="/assets/js/32.2c62c259.js"><link rel="prefetch" href="/assets/js/33.2099844d.js"><link rel="prefetch" href="/assets/js/34.4522b286.js"><link rel="prefetch" href="/assets/js/35.0f453b5e.js"><link rel="prefetch" href="/assets/js/36.04de70cf.js"><link rel="prefetch" href="/assets/js/37.306cc701.js"><link rel="prefetch" href="/assets/js/38.2f2b954f.js"><link rel="prefetch" href="/assets/js/39.a68ff272.js"><link rel="prefetch" href="/assets/js/40.ef51d7b3.js"><link rel="prefetch" href="/assets/js/41.8afb9f7b.js"><link rel="prefetch" href="/assets/js/42.a36d7260.js"><link rel="prefetch" href="/assets/js/43.760640e8.js"><link rel="prefetch" href="/assets/js/44.4d7de229.js"><link rel="prefetch" href="/assets/js/45.3d4a4655.js"><link rel="prefetch" href="/assets/js/46.1f8d2647.js"><link rel="prefetch" href="/assets/js/47.6a66d324.js"><link rel="prefetch" href="/assets/js/48.fc036730.js"><link rel="prefetch" href="/assets/js/49.cae162d4.js"><link rel="prefetch" href="/assets/js/5.11d967e7.js"><link rel="prefetch" href="/assets/js/50.7ea76738.js"><link rel="prefetch" href="/assets/js/51.9625347d.js"><link rel="prefetch" href="/assets/js/52.69c0feed.js"><link rel="prefetch" href="/assets/js/53.499bf52f.js"><link rel="prefetch" href="/assets/js/54.dd1da291.js"><link rel="prefetch" href="/assets/js/55.f079715b.js"><link rel="prefetch" href="/assets/js/56.2bb9ddbe.js"><link rel="prefetch" href="/assets/js/57.5004934c.js"><link rel="prefetch" href="/assets/js/58.3a179660.js"><link rel="prefetch" href="/assets/js/59.b72a3457.js"><link rel="prefetch" href="/assets/js/6.2484af9c.js"><link rel="prefetch" href="/assets/js/60.a3b13775.js"><link rel="prefetch" href="/assets/js/61.ec0c69b2.js"><link rel="prefetch" href="/assets/js/62.32e82d5c.js"><link rel="prefetch" href="/assets/js/63.2a6bb04f.js"><link rel="prefetch" href="/assets/js/64.bc571e83.js"><link rel="prefetch" href="/assets/js/65.a44199ce.js"><link rel="prefetch" href="/assets/js/66.2a054dfd.js"><link rel="prefetch" href="/assets/js/67.79b393ac.js"><link rel="prefetch" href="/assets/js/68.a63686a0.js"><link rel="prefetch" href="/assets/js/69.cb9f18aa.js"><link rel="prefetch" href="/assets/js/7.38e3dd4a.js"><link rel="prefetch" href="/assets/js/70.55002538.js"><link rel="prefetch" href="/assets/js/71.99c1bbec.js"><link rel="prefetch" href="/assets/js/72.60daec4d.js"><link rel="prefetch" href="/assets/js/8.21fcb95f.js">
    <link rel="stylesheet" href="/assets/css/0.styles.b7b742a8.css">
  </head>
  <body class="theme-mode-light">
    <div id="app" data-server-rendered="true"><div class="theme-container sidebar-open have-rightmenu"><header class="navbar blur"><div title="目录" class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><img src="/img/logo.png" alt="XZ Blog" class="logo"> <span class="site-name can-hide">XZ Blog</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link">首页</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="文档" class="dropdown-title"><a href="/doc/" class="link-title">文档</a> <span class="title" style="display:none;">文档</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/pages/b406c0/" class="nav-link">人体姿态估计</a></li><li class="dropdown-item"><!----> <a href="/pages/1186a5/" class="nav-link">2D-3D-Lifting</a></li><li class="dropdown-item"><!----> <a href="/pages/f0fb64/" class="nav-link">动作质量评估</a></li><li class="dropdown-item"><!----> <a href="/pages/1aa734/" class="nav-link">基于RGBD视觉信息的异常行为识别</a></li><li class="dropdown-item"><!----> <a href="/pages/ce88a1/" class="nav-link">基于RGB视频的行为识别</a></li><li class="dropdown-item"><!----> <a href="/pages/0dsfd5/" aria-current="page" class="nav-link router-link-exact-active router-link-active">大模型应用</a></li><li class="dropdown-item"><h4>网络结构</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/pages/xxx/" class="nav-link">Transformer</a></li><li class="dropdown-subitem"><a href="/pages/xxx/" class="nav-link">GCN</a></li><li class="dropdown-subitem"><a href="/pages/0e6e45/" class="nav-link">Graph Transformers</a></li><li class="dropdown-subitem"><a href="/pages/413fa2/" class="nav-link">Diffusion Model</a></li></ul></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="算法" class="dropdown-title"><a href="/ai/" class="link-title">算法</a> <span class="title" style="display:none;">算法</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/pages/e4e08c/" class="nav-link">深度学习</a></li><li class="dropdown-item"><!----> <a href="/pages/61f973/" class="nav-link">论文解读</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="技术" class="dropdown-title"><a href="/technology/" class="link-title">技术</a> <span class="title" style="display:none;">技术</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/pages/t6068a3/" class="nav-link">后端开发</a></li><li class="dropdown-item"><!----> <a href="/pages/1fa324/" class="nav-link">Git</a></li><li class="dropdown-item"><!----> <a href="/pages/bed179/" class="nav-link">博客搭建</a></li><li class="dropdown-item"><!----> <a href="/pages/f56caf/" class="nav-link">Debug</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="更多" class="dropdown-title"><a href="/more/" class="link-title">更多</a> <span class="title" style="display:none;">更多</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/pages/1db558/" class="nav-link">面试</a></li><li class="dropdown-item"><!----> <a href="/pages/434772/" class="nav-link">实用技巧</a></li><li class="dropdown-item"><!----> <a href="/friends/" class="nav-link">友情链接</a></li></ul></div></div><div class="nav-item"><a href="/about/" class="nav-link">关于</a></div><div class="nav-item"><a href="/pages/5c4f4b/" class="nav-link">收藏</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="索引" class="dropdown-title"><a href="/archives/" class="link-title">索引</a> <span class="title" style="display:none;">索引</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/categories/" class="nav-link">分类</a></li><li class="dropdown-item"><!----> <a href="/tags/" class="nav-link">标签</a></li><li class="dropdown-item"><!----> <a href="/archives/" class="nav-link">归档</a></li></ul></div></div> <a href="https://github.com/xzhouzeng/vuepress-theme-vdoing" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar-hover-trigger"></div> <aside class="sidebar" style="display:none;"><div class="blogger"><img src="/img/head.jpg"> <div class="blogger-info"><h3>xzhouzeng</h3> <span>@渐行。</span></div></div> <nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link">首页</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="文档" class="dropdown-title"><a href="/doc/" class="link-title">文档</a> <span class="title" style="display:none;">文档</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/pages/b406c0/" class="nav-link">人体姿态估计</a></li><li class="dropdown-item"><!----> <a href="/pages/1186a5/" class="nav-link">2D-3D-Lifting</a></li><li class="dropdown-item"><!----> <a href="/pages/f0fb64/" class="nav-link">动作质量评估</a></li><li class="dropdown-item"><!----> <a href="/pages/1aa734/" class="nav-link">基于RGBD视觉信息的异常行为识别</a></li><li class="dropdown-item"><!----> <a href="/pages/ce88a1/" class="nav-link">基于RGB视频的行为识别</a></li><li class="dropdown-item"><!----> <a href="/pages/0dsfd5/" aria-current="page" class="nav-link router-link-exact-active router-link-active">大模型应用</a></li><li class="dropdown-item"><h4>网络结构</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/pages/xxx/" class="nav-link">Transformer</a></li><li class="dropdown-subitem"><a href="/pages/xxx/" class="nav-link">GCN</a></li><li class="dropdown-subitem"><a href="/pages/0e6e45/" class="nav-link">Graph Transformers</a></li><li class="dropdown-subitem"><a href="/pages/413fa2/" class="nav-link">Diffusion Model</a></li></ul></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="算法" class="dropdown-title"><a href="/ai/" class="link-title">算法</a> <span class="title" style="display:none;">算法</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/pages/e4e08c/" class="nav-link">深度学习</a></li><li class="dropdown-item"><!----> <a href="/pages/61f973/" class="nav-link">论文解读</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="技术" class="dropdown-title"><a href="/technology/" class="link-title">技术</a> <span class="title" style="display:none;">技术</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/pages/t6068a3/" class="nav-link">后端开发</a></li><li class="dropdown-item"><!----> <a href="/pages/1fa324/" class="nav-link">Git</a></li><li class="dropdown-item"><!----> <a href="/pages/bed179/" class="nav-link">博客搭建</a></li><li class="dropdown-item"><!----> <a href="/pages/f56caf/" class="nav-link">Debug</a></li></ul></div></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="更多" class="dropdown-title"><a href="/more/" class="link-title">更多</a> <span class="title" style="display:none;">更多</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/pages/1db558/" class="nav-link">面试</a></li><li class="dropdown-item"><!----> <a href="/pages/434772/" class="nav-link">实用技巧</a></li><li class="dropdown-item"><!----> <a href="/friends/" class="nav-link">友情链接</a></li></ul></div></div><div class="nav-item"><a href="/about/" class="nav-link">关于</a></div><div class="nav-item"><a href="/pages/5c4f4b/" class="nav-link">收藏</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="索引" class="dropdown-title"><a href="/archives/" class="link-title">索引</a> <span class="title" style="display:none;">索引</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/categories/" class="nav-link">分类</a></li><li class="dropdown-item"><!----> <a href="/tags/" class="nav-link">标签</a></li><li class="dropdown-item"><!----> <a href="/archives/" class="nav-link">归档</a></li></ul></div></div> <a href="https://github.com/xzhouzeng/vuepress-theme-vdoing" target="_blank" rel="noopener noreferrer" class="repo-link">
    GitHub
    <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>人体姿态估计</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>动作质量评估</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>基于骨骼的行为识别</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>基于RGB视频的行为识别</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>大模型应用</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/pages/0dsfd5/" aria-current="page" class="active sidebar-link">大模型应用</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level2"><a href="/pages/0dsfd5/#基础知识" class="sidebar-link">基础知识</a></li><li class="sidebar-sub-header level2"><a href="/pages/0dsfd5/#大型语言模型应用-llms" class="sidebar-link">大型语言模型应用（LLMs）</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/pages/0dsfd5/#chatgpt" class="sidebar-link">ChatGPT</a></li><li class="sidebar-sub-header level3"><a href="/pages/0dsfd5/#autogpt" class="sidebar-link">AutoGPT</a></li><li class="sidebar-sub-header level3"><a href="/pages/0dsfd5/#hugginggpt" class="sidebar-link">HuggingGPT</a></li><li class="sidebar-sub-header level3"><a href="/pages/0dsfd5/#visualgpt" class="sidebar-link">VisualGPT</a></li><li class="sidebar-sub-header level3"><a href="/pages/0dsfd5/#toolformer" class="sidebar-link">Toolformer</a></li><li class="sidebar-sub-header level3"><a href="/pages/0dsfd5/#chatlaw-2023" class="sidebar-link">ChatLaw（2023）</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/pages/0dsfd5/#图像生成" class="sidebar-link">图像生成</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level4"><a href="/pages/0dsfd5/#应用平台" class="sidebar-link">应用平台：</a></li><li class="sidebar-sub-header level4"><a href="/pages/0dsfd5/#发展" class="sidebar-link">发展：</a></li><li class="sidebar-sub-header level4"><a href="/pages/0dsfd5/#参考资料" class="sidebar-link">参考资料</a></li><li class="sidebar-sub-header level4"><a href="/pages/0dsfd5/#评价指标" class="sidebar-link">评价指标</a></li><li class="sidebar-sub-header level5"><a href="/pages/0dsfd5/#_1-定性评估" class="sidebar-link">1. 定性评估</a></li><li class="sidebar-sub-header level5"><a href="/pages/0dsfd5/#_2-is" class="sidebar-link">2. IS</a></li><li class="sidebar-sub-header level5"><a href="/pages/0dsfd5/#_3-fid" class="sidebar-link">3. FID</a></li><li class="sidebar-sub-header level5"><a href="/pages/0dsfd5/#文本到图像评估" class="sidebar-link">文本到图像评估</a></li><li class="sidebar-sub-header level3"><a href="/pages/0dsfd5/#stable-diffusion" class="sidebar-link">Stable Diffusion</a></li><li class="sidebar-sub-header level3"><a href="/pages/0dsfd5/#dall·e系列" class="sidebar-link">DALL·E系列</a></li><li class="sidebar-sub-header level3"><a href="/pages/0dsfd5/#imagen" class="sidebar-link">Imagen</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/pages/0dsfd5/#prompt工程-技巧" class="sidebar-link">Prompt工程/技巧</a></li><li class="sidebar-sub-header level2"><a href="/pages/0dsfd5/#视频生成" class="sidebar-link">视频生成</a></li><li class="sidebar-sub-header level2"><a href="/pages/0dsfd5/#相关模型" class="sidebar-link">相关模型</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header level3"><a href="/pages/0dsfd5/#lora-2021" class="sidebar-link">LoRA（2021）</a></li><li class="sidebar-sub-header level3"><a href="/pages/0dsfd5/#controlnet-2023" class="sidebar-link">ControlNet（2023）</a></li><li class="sidebar-sub-header level3"><a href="/pages/0dsfd5/#blip" class="sidebar-link">BLIP</a></li><li class="sidebar-sub-header level3"><a href="/pages/0dsfd5/#repaint-2022-cvpr" class="sidebar-link">RePaint（2022 CVPR）</a></li><li class="sidebar-sub-header level3"><a href="/pages/0dsfd5/#sdedit-2022-iclr" class="sidebar-link">SDEdit（2022 ICLR）</a></li><li class="sidebar-sub-header level3"><a href="/pages/0dsfd5/#vqgan" class="sidebar-link">VQGAN</a></li><li class="sidebar-sub-header level3"><a href="/pages/0dsfd5/#vqvae" class="sidebar-link">VQVAE</a></li><li class="sidebar-sub-header level3"><a href="/pages/0dsfd5/#guided-diffusion" class="sidebar-link">Guided Diffusion</a></li><li class="sidebar-sub-header level3"><a href="/pages/0dsfd5/#vq-diffusion-2022-cvpr" class="sidebar-link">VQ-Diffusion（2022 CVPR）</a></li><li class="sidebar-sub-header level3"><a href="/pages/0dsfd5/#glide" class="sidebar-link">GLIDE</a></li><li class="sidebar-sub-header level3"><a href="/pages/0dsfd5/#draggan" class="sidebar-link">DragGAN</a></li><li class="sidebar-sub-header level3"><a href="/pages/0dsfd5/#条件控制扩散模型" class="sidebar-link">条件控制扩散模型</a></li><li class="sidebar-sub-header level3"><a href="/pages/0dsfd5/#sr3-2023-tpami" class="sidebar-link">SR3（2023 TPAMI）</a></li><li class="sidebar-sub-header level3"><a href="/pages/0dsfd5/#improved-ddpm" class="sidebar-link">Improved DDPM</a></li><li class="sidebar-sub-header level3"><a href="/pages/0dsfd5/#stable-diffusion、lora、controlnet之间的联系与区别" class="sidebar-link">Stable Diffusion、LORA、ControlNet之间的联系与区别？</a></li><li class="sidebar-sub-header level3"><a href="/pages/0dsfd5/#fatezero-2023-iccv" class="sidebar-link">FateZero（2023 ICCV）</a></li></ul></li><li class="sidebar-sub-header level2"><a href="/pages/0dsfd5/#参考资料-相关链接" class="sidebar-link">参考资料&amp;相关链接</a></li></ul></li><li><a href="/pages/574fcf/" class="sidebar-link">大模型应用开发及落地</a></li><li><a href="/pages/21e4c7/" class="sidebar-link">大模型用于数据分析</a></li><li><a href="/pages/f3001c/" class="sidebar-link">Copilot</a></li><li><a href="/pages/cd39e4/" class="sidebar-link">AIGeo</a></li><li><a href="/pages/b19df9/" class="sidebar-link">提示工程</a></li><li><a href="/pages/d46d50/" class="sidebar-link">大模型智能体</a></li><li><a href="/pages/7644e0/" class="sidebar-link">LLM-Agents</a></li></ul></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>网络架构</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>视频生成</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>NLP</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>多模态</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>视频理解</span> <span class="arrow right"></span></p> <!----></section></li></ul> </aside> <div><main class="page"><div class="theme-vdoing-wrapper "><div class="articleInfo-wrap" data-v-06225672><div class="articleInfo" data-v-06225672><ul class="breadcrumbs" data-v-06225672><li data-v-06225672><a href="/" title="首页" class="iconfont icon-home router-link-active" data-v-06225672></a></li> <li data-v-06225672><a href="/doc/#文档" data-v-06225672>文档</a></li><li data-v-06225672><a href="/doc/#大模型应用" data-v-06225672>大模型应用</a></li></ul> <div class="info" data-v-06225672><div title="作者" class="author iconfont icon-touxiang" data-v-06225672><a href="https://github.com/xzhouzeng" target="_blank" title="作者" class="beLink" data-v-06225672>xzhouzeng</a></div> <div title="创建时间" class="date iconfont icon-riqi" data-v-06225672><a href="javascript:;" data-v-06225672>2023-05-24</a></div> <!----></div></div></div> <!----> <div class="content-wrapper"><div class="right-menu-wrapper"><div class="right-menu-margin"><div class="right-menu-title">目录</div> <div class="right-menu-content"></div></div></div> <h1><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAYAAAA7MK6iAAAAAXNSR0IArs4c6QAABKFJREFUSA3tVl1oFVcQnrMbrak3QUgkya1akpJYcrUtIqW1JvFBE9LiQ5v6JmJpolbMg32rVrhgoYK0QiMY6i9Y6EMaW5D+xFJaTYItIuK2Kr3+BJNwkxBj05sQY3b3nM6cs2dv9t7NT/vQJw/sndk5M/PNzJkzewGerP+pAmy+ON8lLzUJgA8ZYxYIYZmGYRnctDaWvJJAmTtfP1pvXsBCCPP8QFcCaRkZYACgDZFO4stNIcBCajEOlmmC9XpJ9bAGCaPaPmzPl32dvLSVu3BWCTQs0XQQ6g0DYgwLIoAZbBCdW/i+781o1VVlm/410mw4h06Y7bIPHNyWDyL4FHkX03Q8SrzNhZTZriieckWt7cL6MM85YcLpsi/7O9/iXFT6MswI0DmmpkSaJ0qLxFIm3+i1THHB3zmBH3PYx9CcykcLOeQVVa7QtdxTgQgEleX2AjHYfwA+2ddV77ruGoJUbhGDI09YSNXyMpUt5ylOzxgbUmtOp7NmbNt8v3arjTBfYELmLUV+M+nSawNNAUqpT3ClJWg5I3BLT+cGW/DXNGCa6tx1aakCGEigArTn4TDIPdrXXYKCZNrHLMCOEPvHBlLQ99s9eHB7EB6NTki73CVPQ2F5MSx/uRQixfmq7rK0wYD8w8E905bnPDfwoWs/rfv93NWN/ZfvwsLIU7A09gxECyISeGJkHAau98L97tuw7NXnoPyNF8FcYGLGKsOs0mN3OEyec9esGW/ZEl945dTP34wlR2FZVQWU1q0Cw8Tr7p+hgLLNL0FPxx/Q35mA8aEUrH6nCgwEl0tn7wUiZYJnNRh6DK4UH/k0lfyrsBKdPVv/AriGIQcEDQZ65LBAGe2Rzui9Ybjz7XUppz1/uKBbyVPGkN3ZAeC6hr0x7Nr38N5+EqkoOm17xpoqR9ohQF55ERSvr4Dkr3chNfC3DMzGJlNBElW8w9nsGQvhNGIzDkXzCg8cLK951xHsFBlTJspJNi3ZFIMF2AeDV3q8DNOB+YHi6QTrChDIWDBRi5U5f+ZMfJLu3ccrqxtdxk4SKH336LFxSmkqefwU5T8fhdSdQf9IVKD6aNiwI/hnmcAZ91isYMJIaCUCx9W098+LgruikeTqzqqxKPUwqJyCPJiyemVVZBOijDGjD38Os0jOiSPL1z3SPjXNANbiNPXAdzTfukjjuknNBbyz3nwgTd3AVFqUJ5hpHlq9MveLnWwttUfoygBmvVjuikxND3znrhsELnZk7k+OjIGxeNEkomyLVta0xxn+HZhjBc4YZ/AFjHjz9u3xRZl2BN4aq9nFwWh16IrQ1aHHEd3j1+4/dB9OtH4e29A2H1DyHQRmOSfQZ1Fy7MHBTGB6J/Djq6p3OxyO2cB+4Car7v/o3GXgfAkj23+x9ID1Teoamo/SXcbvSf2PX7Vc8DdCmE1vN9di+32P9/5YR3vLnhCVGUWBjEkr3yh4H8v9CzmsbdhzOKzsJKM90iFdaTMjRPhGVsakRvOaRidljo6H6G7j+ctrJpsP+4COhDIl0La2+FS4+5mlocBaXY5QnGZysIBYoeSsl5qQzrSj/cgNrfuEzlWBfwA+EjrZyWUvpAAAAABJRU5ErkJggg==">大模型应用<!----></h1> <!----> <div class="theme-vdoing-content content__default"><h1 id="大模型应用"><a href="#大模型应用" class="header-anchor">#</a> 大模型应用</h1> <p><strong>“AI时代就是大模型时代”</strong></p> <p><strong>“AGI（Artificial General Intelligence）是指一种能够像人类一样具有广泛认知能力的人工智能技术。与弱人工智能只能解决特定问题不同，AGI能够处理各种不同的任务，并具有学习、推理、判断和自我改进等能力，类似于人类的智能。尽管AIGC和AGI都是人工智能领域中的重要概念，但它们之间存在着明显的区别。AIGC通常专注于某个特定的任务或领域，而AGI具有更广泛的应用范围和更强的认知能力。”</strong></p> <h2 id="基础知识"><a href="#基础知识" class="header-anchor">#</a> 基础知识</h2> <p>AI两种期待：</p> <ul><li><p>专用模型：对预训练模型改造（finetune、Adapter：减少微调参数）</p></li> <li><p>通用模型：</p> <ul><li><p>hard prompt：通过范例学习（In-context Learning）、Instruction-tuning、CoT-Prompting（范例提供推理过程）Few-shot-CoT、Zero-shot-CoT（不够例子，Let's think step by step)、人类反馈的强化学习（RLHF）</p></li> <li><p>soft prompt：可学习参数</p></li></ul></li></ul> <p>将大型语言模型（LLMs）的范围扩大到文本生成之外，当代研究已经研究了两种主要方法：</p> <ol><li><p>一些工作设计了统一的<strong>多模态语言模型</strong>，如BLIP-2，它利用Q-former来协调语言和视觉语义，以及Kosmos-1，它将视觉输入纳入文本序列以合并语言和视觉输入。</p></li> <li><p>其他研究集中在<strong>外部工具或模型的整合</strong>上:</p> <ol><li><p>首创的Toolformer在文本序列中引入了外部API标签，促进LLM对外部工具的访问。因此，许多工作都将LLMs扩展到了视觉模态中。</p></li> <li><p>Visual ChatGPT将视觉基础模型，如BLIP和ControlNet，与LLMs融合。</p></li> <li><p>视觉编程和ViperGPT通过采用编程语言将LLMs应用于视觉对象，将视觉查询解析为以Python代码表达的可解释步骤。</p></li> <li><p>研究人员还努力使这些LLM适用于专门的视觉任务。例如，Prophet和ChatCaptioner分别将LLMs纳入视觉问答和图像字幕任务。</p></li></ol></li></ol> <h2 id="大型语言模型应用-llms"><a href="#大型语言模型应用-llms" class="header-anchor">#</a> 大型语言模型应用（LLMs）</h2> <h3 id="chatgpt"><a href="#chatgpt" class="header-anchor">#</a> ChatGPT</h3> <blockquote><p><a href="https://zhuanlan.zhihu.com/p/589961989" target="_blank" rel="noopener noreferrer">ChatGPT是什么，一文读懂ChatGPT - 知乎 (zhihu.com)<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></blockquote> <h3 id="autogpt"><a href="#autogpt" class="header-anchor">#</a> AutoGPT</h3> <blockquote><p><a href="https://autogpt.cn/setup" target="_blank" rel="noopener noreferrer">AutoGPT：安装 - AutoGPT中文<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p>AutoGPT 运行原理解析 - 郑海波的文章 - 知乎
https://zhuanlan.zhihu.com/p/625094476</p></blockquote> <p>不过ChatGPT的API都是无状态的，没有对话管理的功能。</p> <p><a href="https://zhuanlan.zhihu.com/p/618911413" target="_blank" rel="noopener noreferrer">手把手教会你如何通过ChatGPT API实现上下文对话 - 知乎 (zhihu.com)<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p><a href="https://zhuanlan.zhihu.com/p/625094476" target="_blank" rel="noopener noreferrer">AutoGPT 运行原理解析 - 知乎 (zhihu.com)<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p><a href="https://zhuanlan.zhihu.com/p/623045493" target="_blank" rel="noopener noreferrer">详解下最近被吹爆的 AutoGPT - 知乎 (zhihu.com)<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p>Auto-GPT 是一个实验性的开源应用程序，展示了 GPT-4 语言模型的功能。该程序由 GPT-4 驱动，将 LLM“思想”链接在一起，以<strong>自主实现您设定的任何目标</strong>。作为 GPT-4 完全自主运行的首批示例之一，Auto-GPT 突破了 AI 可能的界限。</p> <ul><li>🌐用于搜索和信息收集的互联网接入</li> <li>💾长期和短期记忆管理</li> <li>🧠用于文本生成的 GPT-4 实例</li> <li>🔗访问热门网站和平台</li> <li>🗃️使用 GPT-3.5 进行文件存储和摘要</li> <li>🔌插件的可扩展性</li></ul> <h3 id="hugginggpt"><a href="#hugginggpt" class="header-anchor">#</a> HuggingGPT</h3> <p>（<strong>HuggingFace+ChatGPT</strong>）</p> <blockquote><p>HuggingGPT： Solving AI Tasks with ChatGPT and its Friends in Hugging Face（2023）</p> <p><a href="https://zhuanlan.zhihu.com/p/619763221" target="_blank" rel="noopener noreferrer">HuggingGPT： 用ChatGPT和它的朋友在HuggingFace中解决AI任务 - 知乎 (zhihu.com)<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></blockquote> <p>利用LLM（如ChatGPT）来连接机器学习社区（如Hugging Face）中各种人工智能模型以解决人工智能任务，HuggingGPT的整个过程可以分为四个阶段：</p> <ol><li><p>一个LLM（如ChatGPT）首先解析用户请求，将其分解为多个任务，并根据其知识规划<strong>任务顺序和依赖性</strong>；</p> <p>为了提示大语言模型进行有效的任务规划，HuggingGPT在其提示设计中同时采用了基于规范的指令(将任务规范作为高级指令提供给大语言模型，用于分析用户的请求并相应地解析任务)和基于演示的解析法(通过在提示中注入几个演示使大型语言模型能够更好地理解任务规划的意图和标准)。</p></li> <li><p>LLM根据Hugging Face中的模型描述将解析后的任务分配给专家模型；</p> <p>将任务和模型的分配视为单项选择问题，其中潜在的模型在给定的上下文中被作为选项提出。</p></li> <li><p>专家模型在推理端点上执行分配的任务，并将执行信息和推理结果记录到LLM；</p> <p>为了提高速度和计算稳定性，HuggingGPT在混合推理端点上运行这些模型。通过将任务参数作为输入，模型计算推理结果，然后将其送回大语言模型。</p></li> <li><p>最后，LLM总结执行过程记录和推理结果，将总结返回给用户。</p> <p>将前三个阶段（任务规划、模型选择和任务执行）的所有信息整合成一个简明的摘要，包括计划的任务列表、为任务选择的模型以及模型的推理结果。</p></li></ol> <img title="" src="/assets/img/2023-05-21-14-18-54-image.1118b1dc.png" alt="" data-align="center"> <p>为了处理复杂的人工智能任务，LLM应该能够与外部模型协调，利用它们的力量。因此，关键点在于如何选择合适的中间件来连接LLM和AI模型。为了解决这个问题，我们注意到每个人工智能模型可以通过总结其模型功能来表示为一种语言形式。因此，我们引入一个概念： &quot;语言是LLMs连接AI模型的通用接口&quot;。</p> <p>即HuggingGPT是一个协作系统，由一个大型语言模型（LLM）作为控制器，众多专家模型作为协作执行者组成。</p> <img src="/assets/img/2023-05-21-16-18-20-image.9dfdffef.png" title="" alt="" data-align="center"> <p>HuggingGPT中提示设计的详细信息。在提示中，我们设置了一些可注射的插槽，如和{Candidate Models}}。这些槽在被馈送到LLM之前被统一地替换为相应的文本。</p> <img src="/assets/img/2023-05-21-16-22-19-image.87679543.png" title="" alt="" data-align="center"> <p><strong>限制：</strong></p> <ul><li><p>效率。效率的瓶颈在于大型语言模型的推理。对于每一轮的用户请求，HuggingGPT在任务规划、模型选择和响应生成阶段都需要与大型语言模型进行至少一次互动。这些互动大大增加了响应延迟，导致用户体验的下降。</p></li> <li><p>最大上下文长度的限制。受LLM可以接受的最大token数的限制，HuggingGPT也面临着最大上下文长度的限制。我们使用了对话窗口，并且只在任务规划阶段跟踪对话背景，以缓解它。</p></li> <li><p>系统的稳定性，这包括两个方面。一个是大型语言模型推理过程中出现的反叛现象。大型语言模型在推理时偶尔会不符合指令，输出格式也可能违背预期，导致程序工作流程出现异常。其次是Hugging Face推理端点上托管的专家模型的不可控状态。Hugging Face上的专家模型可能受到网络延迟或服务状态的影响，导致任务执行阶段的错误。</p></li></ul> <h3 id="visualgpt"><a href="#visualgpt" class="header-anchor">#</a> VisualGPT</h3> <h3 id="toolformer"><a href="#toolformer" class="header-anchor">#</a> Toolformer</h3> <h3 id="chatlaw-2023"><a href="#chatlaw-2023" class="header-anchor">#</a> ChatLaw（2023）</h3> <blockquote><p><a href="https://arxiv.org/abs/2306.16092" target="_blank" rel="noopener noreferrer">[2306.16092] ChatLaw: Open-Source Legal Large Language Model with Integrated External Knowledge Bases (arxiv.org)<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></blockquote> <h2 id="图像生成"><a href="#图像生成" class="header-anchor">#</a> 图像生成</h2> <h4 id="应用平台"><a href="#应用平台" class="header-anchor">#</a> 应用平台：</h4> <ol><li><p>Midjourney(收费)</p></li> <li><p>SD: <a href="https://zhuanlan.zhihu.com/p/622238031" target="_blank" rel="noopener noreferrer">Stable Diffusion超详细教程！从0-1入门到进阶 - 知乎 (zhihu.com)<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></li> <li><p><a href="https://github.com/AbdBarho/stable-diffusion-webui-docker" target="_blank" rel="noopener noreferrer">AbdBarho/stable-diffusion-webui-docker: Easy Docker setup for Stable Diffusion with user-friendly UI (github.com)<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></li> <li><p>Deforum</p> <p>Deforum插件是一种基于stable diffusion的动画生成工具，它可以根据文本描述或参考视频生成连续的图像序列，并将它们拼接成视频。Deforum插件使用了一种叫做image-to-image function的技术，它可以对图像帧进行微小的变换，并用stable diffusion生成下一帧。由于帧之间的变化很小，因此会产生连续视频的感觉。</p></li></ol> <h4 id="发展"><a href="#发展" class="header-anchor">#</a> 发展：</h4> <p>文本生成图像</p> <p>DALL·E(2021/1)-&gt;CogView(2021/5)-&gt;NvWA(2021/11)-&gt;GLIDE(2021/12)-&gt;ERNIE-ViLG(2021/12)-&gt;DALL·E2(2022/4)-&gt;CogView2(2022/4)-&gt;CogVideo(2022/5)-&gt;Imagen(2022/5)</p> <h4 id="参考资料"><a href="#参考资料" class="header-anchor">#</a> 参考资料</h4> <p><a href="https://zhuanlan.zhihu.com/p/62746494" target="_blank" rel="noopener noreferrer">GAN生成图像综述 - 知乎 (zhihu.com)<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <h4 id="评价指标"><a href="#评价指标" class="header-anchor">#</a> 评价指标</h4> <p><a href="https://zhuanlan.zhihu.com/p/109342043" target="_blank" rel="noopener noreferrer">GAN评价指标最全汇总 - 知乎 (zhihu.com)<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p><a href="https://blog.csdn.net/air__Heaven/article/details/124074376?spm=1001.2101.3001.6650.15&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-15-124074376-blog-128194122.235%5Ev36%5Epc_relevant_default_base3&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-15-124074376-blog-128194122.235%5Ev36%5Epc_relevant_default_base3&amp;utm_relevant_index=20" target="_blank" rel="noopener noreferrer">Text to Image 文本生成图像定量评价指标分析笔记 Metric Value总结 IS、FID、R-prec等_图像生成评价指标_中杯可乐多加冰的博客-CSDN博客<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <h5 id="_1-定性评估"><a href="#_1-定性评估" class="header-anchor">#</a> 1. 定性评估</h5> <p>该方法主要还是靠人的眼睛来进行判断。一般的做法是将真实图像和生成图像对上传到众包平台上让人来判断图像的真假，并且给出两者的相似程度，最后根据打分的结果统计一个最终的指标。</p> <h5 id="_2-is"><a href="#_2-is" class="header-anchor">#</a> 2. IS</h5> <h5 id="_3-fid"><a href="#_3-fid" class="header-anchor">#</a> 3. FID</h5> <h5 id="文本到图像评估"><a href="#文本到图像评估" class="header-anchor">#</a> 文本到图像评估</h5> <h3 id="stable-diffusion"><a href="#stable-diffusion" class="header-anchor">#</a> Stable Diffusion</h3> <blockquote><p>High-Resolution Image Synthesis with Latent Diffusion Models（2022 CVPR）</p></blockquote> <p>通过将图像形成过程分解为顺序应用 降噪自动编码器、扩散模型（DM）实现最先进的技术 图像数据及其他方面的合成结果。此外，它们的配方 允许使用引导机制来控制图像生成过程，而无需培训。但是，由于这些模型通常直接以像素为单位运行空间，强大的DM的优化通常需要消耗数百天的GPU和 由于顺序评估，推理成本很高。启用DM训练有限的计算资源，同时保持其质量和灵活性， 我们<strong>将它们应用于强大的预训练自动编码器的潜在空间中</strong>。在与以前的工作相比，在这种表示上训练扩散模型 允许首次达到复杂度之间的接近最佳点减少和细节保留，大大提高视觉保真度。由在模型架构中引入交叉关注层，我们转向将模型扩散到强大而灵活的发生器中，用于一般调节文本或边界框等输入以及高分辨率合成变为可能以卷积方式。我们的潜扩散模型 （LDM） 可实现 图像修复和极具竞争力的性能的新技术水平 各种任务，包括无条件图像生成、语义场景合成和超分辨率，同时显著减少计算 与基于像素的 DM 相比的要求。</p> <img src="/assets/img/2023-05-22-13-05-49-image.a734b0d0.png" title="" alt="" data-align="center"> <p><strong>感知压缩（Perceptual Compression）</strong>:首先需要训练好一个自编码模型（AutoEncoder，包括一个编码器和一个解码器 ）。这样一来，我们就可以利用编码器对图片进行压缩，然后在潜在表示空间上做diffusion操作，最后我们再用解码器恢复到原始像素空间即可。（512,512 ,3)-&gt;(64 ,64 ,4）</p> <p><strong>条件机制（Conditioning Mechanisms）</strong>：除了无条件图片生成外，我们也可以进行条件图片生成，具体来说，论文通过在UNet主干网络上增加cross-attention机制来实现。</p> <h3 id="dall·e系列"><a href="#dall·e系列" class="header-anchor">#</a> DALL·E系列</h3> <blockquote><p>Zero-Shot Text-to-Image Generation（2021 ICML）
Hierarchical Text-Conditional Image Generation with CLIP Latents</p> <p><a href="https://zhuanlan.zhihu.com/p/502389739" target="_blank" rel="noopener noreferrer">DALL-E 2的工作原理原来是这样！ - 知乎 (zhihu.com)<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></blockquote> <p>DALL-E 2的工作是训练两个模型。第一个是Prior，接受文本标签并创建CLIP图像嵌入。第二个是Decoder，其接受CLIP图像嵌入并生成图像。模型训练完成之后，推理的流程如下：</p> <img title="" src="/assets/img/2023-05-22-14-14-16-image.f22c3a22.png" alt="" data-align="center" width="622"> <ul><li><p>输入的文本被转化为使用神经网络的CLIP文本嵌入。</p></li> <li><p>使用文本嵌入创建图像嵌入。(先验模型，扩散模型效果及效率更优)Transformer/U-Net+attension</p></li> <li><p>进入Decoder步骤后，扩散模型被用来将图像嵌入转化为图像。（GLIDE修改版）</p> <p>两个Diffusion Model: 网络结构是U-Net。图像被从64×64放大到256×256，最后放大到1024×1024。</p> <p>我们使用ADMNet体系结构作为上采样器。在第一个上采样阶段，我们使用余弦噪声处理计划，在ADMNet内使用320个通道和每个分辨率3个resblock的深度。我们还应用了高斯模糊（核大小3，西格玛0.6）。在第二个上采样阶段，我们使用线性噪声计划，192个通道，每个分辨率2个resblock的深度，并根据Rombach等人[42]的BSR退化进行训练。<strong>两个上采样器都没有使用注意力。</strong> 为了减少推理时间，我们使用DDIM并手动调整步数，256×256模型为27步，1024×1024模型为15步。</p></li></ul> <h3 id="imagen"><a href="#imagen" class="header-anchor">#</a> Imagen</h3> <blockquote><p>Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding</p></blockquote> <p>我们展示了Imagen，这是一个文本到图像的扩散模型，具有前所未有的真实感和深入的语言理解。Imagen建立在大型转换语言模型理解文本的能力之上，并取决于高保真图像生成中扩散模型的强度。我们的关键发现是，在纯文本语料库上预训练的通用大型语言模型（例如T5）在编码用于图像合成的文本方面出奇地有效：增加Imagen中语言模型的大小比增加图像扩散模型的大小更能提高样本保真度和图像文本对齐。
Imagen在COCO数据集上获得了7.27的新的最先进的FID分数，而从未对COCO进行过训练，并且人类评分者发现Imagen样本在图像-文本对齐方面与COCO数据本身不相上下。为了更深入地评估文本到图像模型，我们引入了DrawBench，这是一个全面且具有挑战性的文本到图像建模基准。使用DrawBench，我们将Imagen与最近的方法进行了比较，包括VQ-GAN+CLIP、Latent Diffusion Models,、GLIDE和DALL-E2，并发现在并排比较中，无论是在样本质量还是图像文本对齐方面，人类评分者都更喜欢Imagen而不是其他模型。</p> <h2 id="prompt工程-技巧"><a href="#prompt工程-技巧" class="header-anchor">#</a> Prompt工程/技巧</h2> <blockquote><p>Prompting: Prefix-Tuning: Optimizing Continuous Prompts for Generation</p></blockquote> <p><a href="https://zhuanlan.zhihu.com/p/626819858" target="_blank" rel="noopener noreferrer">《ChatGPT Prompt Engineering for Developers》课程笔记 - 知乎 (zhihu.com)<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <h2 id="视频生成"><a href="#视频生成" class="header-anchor">#</a> 视频生成</h2> <h2 id="相关模型"><a href="#相关模型" class="header-anchor">#</a> 相关模型</h2> <p>参考资料：</p> <p><a href="https://zhuanlan.zhihu.com/p/626346656" target="_blank" rel="noopener noreferrer">基于Diffusion的精细化可控图片生成【论文+效果分析】 - 知乎 (zhihu.com)<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <h3 id="lora-2021"><a href="#lora-2021" class="header-anchor">#</a> LoRA（2021）</h3> <blockquote><p>LoRA: Low-Rank Adaptation of Large Language Models</p> <p><a href="https://zhuanlan.zhihu.com/p/618073170" target="_blank" rel="noopener noreferrer">【自然语言处理】【大模型】极低资源微调大模型方法LoRA以及BLOOM-LORA实现代码 - 知乎 (zhihu.com)<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></blockquote> <h3 id="controlnet-2023"><a href="#controlnet-2023" class="header-anchor">#</a> ControlNet（2023）</h3> <blockquote><p>Adding Conditional Control to Text-to-Image Diffusion Models</p> <p><a href="https://juejin.cn/post/7210369671656505399" target="_blank" rel="noopener noreferrer">ControlNet原理解析 | 读论文 - 掘金 (juejin.cn)<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></blockquote> <p>我们提出了一种神经网络结构ControlNet，用于控制预先训练的大型扩散模型，以支持额外的输入条件。ControlNet以端到端的方式学习特定任务的条件，即使训练数据集很小（&lt;50k），学习也很稳健。此外，训练ControlNet与微调扩散模型一样快，并且可以在个人设备上训练该模型。
或者，如果有强大的计算集群可用，该模型可以扩展到大量（数百万到数十亿）的数据。我们报告说，像stable diffusion这样的大型扩散模型可以用ControlNets来增强，以实现条件输入，如边缘图、分割图、关键点等。这可能会丰富控制大型扩散模型的方法，并进一步促进相关应用。</p> <img title="" src="/assets/img/2023-05-22-13-25-12-image.7e2c406e.png" alt="" width="587" data-align="center"> <p>ControlNet in Stable Diffusion</p> <img title="" src="/assets/img/2023-05-22-13-27-00-image.9269c81d.png" alt="" width="627" data-align="center"> <h3 id="blip"><a href="#blip" class="header-anchor">#</a> BLIP</h3> <h3 id="repaint-2022-cvpr"><a href="#repaint-2022-cvpr" class="header-anchor">#</a> RePaint（2022 CVPR）</h3> <blockquote><p>RePaint: Inpainting using Denoising Diffusion Probabilistic Models</p></blockquote> <p>RePaint修改了标准去噪过程，以便对给定的图像内容进行调节。在每一步中，我们对输入的已知区域（顶部）和DDPM输出的修复部分（底部）进行采样。</p> <img src="/assets/img/2023-05-22-12-43-09-image.e1059aba.png" title="" alt="" data-align="center"> <p>当直接应用描述的方法时，我们观察到只有内容类型与已知区域匹配。例如下图 n=1中，绘制区域是一个与狗的毛发相匹配的毛茸茸的纹理。尽管修复的区域与相邻区域的纹理匹配，但在语义上是不正确的。因此，DDPM利用了已知区域的上下文，<strong>但它并没有很好地将其与图像的其余部分协调起来。</strong>
该模型使用xt预测xt−1，xt包括DDPM的输出和来自已知区域的样本。然而，对已知像素进行采样是在不考虑图像的生成部分的情况下进行的，这会引入不和谐。尽管模型在每一步都试图再次协调图像，但它永远无法完全收敛，因为下一步会出现同样的问题。此外，在每个反向步骤中，由于βt的变化时间表，图像的最大变化逐渐减少。因此，由于灵活性有限，该方法无法纠正导致后续步骤中不和谐界限的错误。因此，<strong>在进入下一个去噪步骤之前，模型需要更多的时间在一个步骤中协调条件信息x已知t−1和生成的信息x未知t−1。</strong></p> <p>由于DDPM被训练来生成位于数据分布中的图像，因此它自然旨在产生一致的结构。在我们的重采样方法中，我们使用这种DDPM特性通过多次迭代来协调模型的输入。</p> <img title="" src="/assets/img/2023-05-22-12-44-18-image.a4bcdc50.png" alt="" data-align="center" width="668"> <img title="" src="/assets/img/2023-05-22-12-43-21-image.9382fe34.png" alt="" width="541" data-align="center"> <h3 id="sdedit-2022-iclr"><a href="#sdedit-2022-iclr" class="header-anchor">#</a> SDEdit（2022 ICLR）</h3> <blockquote><p>SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations</p></blockquote> <p>使用SDEdit从笔划合成图像。蓝点说明了我们方法的编辑过程。绿色和蓝色轮廓图分别表示图像和笔划绘画的分布。给定一个笔划绘制，我们首先用高斯噪声对其进行扰动，并通过模拟反向SDE逐渐去除噪声。这个过程逐渐将一幅不切实际的笔触画投射到自然图像的流形上。</p> <img src="/assets/img/2023-05-22-12-40-22-image.30ca8b72.png" title="" alt="" data-align="center"> <img src="/assets/img/2023-05-22-12-41-01-image.804ed843.png" title="" alt="" data-align="center"> <h3 id="vqgan"><a href="#vqgan" class="header-anchor">#</a> VQGAN</h3> <h3 id="vqvae"><a href="#vqvae" class="header-anchor">#</a> VQVAE</h3> <h3 id="guided-diffusion"><a href="#guided-diffusion" class="header-anchor">#</a> Guided Diffusion</h3> <h3 id="vq-diffusion-2022-cvpr"><a href="#vq-diffusion-2022-cvpr" class="header-anchor">#</a> VQ-Diffusion（2022 CVPR）</h3> <blockquote><p>Vector quantized diffusion model for text-to-image synthesis</p></blockquote> <h3 id="glide"><a href="#glide" class="header-anchor">#</a> GLIDE</h3> <blockquote><p>GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models</p></blockquote> <p>扩散模型最近被证明可以生成高质量的合成图像，<strong>尤其是当与guidance技术相结合时，以牺牲多样性来换取保真度</strong>。我们探索了文本条件图像合成问题的扩散模型，并比较了两种不同的引导策略：CLIP guidance and classifier-free guidance。我们发现，后者在照片真实性和帽子相似性方面都是人类评估者的首选，并且经常产生照片真实性样本。与DALL-E的样本相比，使用classifier-free guidance的35亿参数文本条件扩散模型的样本更受人类评估者的青睐，即使后者使用昂贵的CLIP重新排序。此外，我们发现我们的模型可以进行微调以执行图像修复，从而实现强大的文本驱动图像编辑。</p> <h3 id="draggan"><a href="#draggan" class="header-anchor">#</a> DragGAN</h3> <h3 id="条件控制扩散模型"><a href="#条件控制扩散模型" class="header-anchor">#</a> 条件控制扩散模型</h3> <blockquote><p>Classifier-Guidance: Diffusion Models Beat GANs on Image Synthesis（2022 NIPS）</p> <p>Classifier-Free: Classifier Free Diffusion Guidance（2021 NIPS）</p> <p>DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation</p> <p><a href="https://zhuanlan.zhihu.com/p/582880086" target="_blank" rel="noopener noreferrer">浅谈扩散模型的有分类器引导和无分类器引导 - 知乎 (zhihu.com)<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p><a href="https://spaces.ac.cn/archives/9257" target="_blank" rel="noopener noreferrer">生成扩散模型漫谈（九）：条件控制生成结果 - 科学空间|Scientific Spaces<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></blockquote> <p>扩散模型的引导生成的三种做法的区别。</p> <ul><li><p>用显式分类器引导生成的做法</p></li> <li><p>用隐式无分类器引导的做法</p></li> <li><p>用CLIP计算跨模态间的损失来引导生成的做法。</p></li></ul> <h3 id="sr3-2023-tpami"><a href="#sr3-2023-tpami" class="header-anchor">#</a> SR3（2023 TPAMI）</h3> <blockquote><p>Image Super-Resolution via Iterative Refinement</p></blockquote> <h3 id="improved-ddpm"><a href="#improved-ddpm" class="header-anchor">#</a> Improved DDPM</h3> <blockquote><p>Improved denoising diffusion probabilistic models</p></blockquote> <p>贡献：</p> <ul><li><p>损失函数的改进：DDPM中的方差是很难学习的，因此其忽略了方差项，令方差为一个常数，使其方差不是一个可学习的对象。相反，IDDPM引入方差项的学习，损失项加上了一项惩罚。</p></li> <li><p>噪声机制使用cos：DDPM的方差是一个linear schedule，<img src="https://math.now.sh?inline=%5Chat%7B%5Calpha%20_t%7D" style="display:inline-block;margin:0;">的值变化如下图所示。可以看到，linear 的方式使得图片加噪的太快。因此为了让扩散过程中能够保证更多的细节，IDDPM改进了schedule，采用cosine schedule。</p> <p>（<strong>线性噪声机制对于高分辨率图像很好</strong>，在分辨率小的图像上结果次优。ddpm中的前向加噪过程对采样过程没有太大的贡献。）</p> <img title="" src="/assets/img/2023-05-26-22-42-27-image.21dd585c.png" alt="" data-align="center" width="355"></li></ul> <h3 id="stable-diffusion、lora、controlnet之间的联系与区别"><a href="#stable-diffusion、lora、controlnet之间的联系与区别" class="header-anchor">#</a> Stable Diffusion、LORA、ControlNet之间的联系与区别？</h3> <p>Stable Diffusion是一种基于扩散模型的图像生成方法，可以通过文本等条件来控制图像的内容和风格<a href="https://github.com/HighCWu/ControlLoRA" target="_blank" rel="noopener noreferrer">¹<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>。ControlNet是一种神经网络结构，可以通过添加额外的条件来控制扩散模型的空间信息，例如边缘检测或人体姿态检测（<a href="https://github.com/lllyasviel/ControlNet" target="_blank" rel="noopener noreferrer">参考<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>、<a href="https://stable-diffusion-art.com/controlnet/" target="_blank" rel="noopener noreferrer">ControlNet v1.1<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>）。LORA是一种轻量级的网络，可以对扩散模型的CLIP模型和Unet的CrossAttention的线性层进行微调，从而提高图像生成的质量和多样性（<a href="https://zhuanlan.zhihu.com/p/611310582" target="_blank" rel="noopener noreferrer">参考<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>）。</p> <p>简单来说，stable diffusion可以接受文本等条件去生成图像，controlnet可以让用户更精确地控制图像的布局和结构，LORA可以提升图像生成的效果和效率。这三者都是在扩散模型的基础上进行改进和拓展的方法。</p> <p><strong>举例说明：</strong></p> <p>stable diffusion可以根据文本生成图像，例如输入“a bird”就可以生成一只鸟的图像。但是stable diffusion不能控制图像的具体形状和位置，只能随机生成。</p> <p>controlnet可以在stable diffusion的基础上添加额外的条件，例如一个边缘检测或人体姿态检测的图像，来控制生成图像的空间信息。例如，如果输入“a bird”和一个边缘检测图像，controlnet就可以生成一只鸟的图像，并且让它的形状和位置与边缘检测图像相匹配。</p> <p>LORA可以在stable diffusion的基础上对CLIP模型和Unet的CrossAttention的线性层进行微调，从而提高图像生成的质量和多样性。例如，如果输入“a bird”，LORA就可以生成一只鸟的图像，并且让它更清晰和逼真。</p> <p>更具体的说，LORA可以通过使用不同风格的图像微调来改变或控制图像生成的风格，例如“毕加索风格的鸟”<a href="https://www.shruggingface.com/blog/self-portraits-with-stable-diffusion-and-lora" target="_blank" rel="noopener noreferrer">¹<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>。这样可以使得stable diffusion可以生成更多有意思的图像。</p> <p>其他一些使用LORA进行风格转换的例子：</p> <ul><li>使用水彩风格的LORA生成水彩画效果的图像<a href="https://aituts.com/stable-diffusion-lora/" target="_blank" rel="noopener noreferrer">²<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li>使用漫画线稿风格的LORA生成线稿或填色本效果的图像<a href="https://aituts.com/stable-diffusion-lora/" target="_blank" rel="noopener noreferrer">²<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li>使用塔罗牌风格的LORA生成复杂细致的塔罗牌插画效果的图像<a href="https://aituts.com/stable-diffusion-lora/" target="_blank" rel="noopener noreferrer">²<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li>使用多个不同风格的LORA混合生成自定义风格的图像<sup><a href="https://huggingface.co/blog/lora" target="_blank" rel="noopener noreferrer">3<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></sup></li></ul> <p>这些例子只是展示了LORA的一部分功能，还可以尝试使用其他风格或主题的LORA来创造您想要的图像。</p> <h3 id="fatezero-2023-iccv"><a href="#fatezero-2023-iccv" class="header-anchor">#</a> FateZero（2023 ICCV）</h3> <blockquote><p>FateZero: Fusing Attentions for Zero-shot Text-based Video Editing</p></blockquote> <h2 id="参考资料-相关链接"><a href="#参考资料-相关链接" class="header-anchor">#</a> 参考资料&amp;相关链接</h2> <ul><li><p><a href="https://blog.csdn.net/lovechris00/article/details/130192469?spm=1001.2101.3001.6661.1&amp;utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EYuanLiJiHua%7EPayColumn-1-130192469-blog-128194122.235%5Ev36%5Epc_relevant_default_base3&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EYuanLiJiHua%7EPayColumn-1-130192469-blog-128194122.235%5Ev36%5Epc_relevant_default_base3&amp;utm_relevant_index=1" target="_blank" rel="noopener noreferrer">大模型、AIGC 资源记录_伊织code的博客-CSDN博客<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></li> <li><p><a href="https://github.com/altryne/awesome-ai-art-image-synthesis" target="_blank" rel="noopener noreferrer">altryne/awesome-ai-art-image-synthesis<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></li> <li><p><a href="https://github.com/Yutong-Zhou-cv/Awesome-Text-to-Image" target="_blank" rel="noopener noreferrer">GitHub - Yutong-Zhou-cv/Awesome-Text-to-Image: (ෆ`꒳´ෆ) A Survey on Text-to-Image Generation/Synthesis.<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></li></ul></div></div> <!----> <div class="page-edit"><div class="edit-link"><a href="https://github.com/xzhouzeng/vuepress-theme-vdoing/edit/master/docs/01.文档/05.大模型应用/01.大模型应用.md" target="_blank" rel="noopener noreferrer">编辑</a> <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></div> <div class="tags"><a href="/tags/?tag=DM" title="标签">#DM</a><a href="/tags/?tag=ChatGPT" title="标签">#ChatGPT</a><a href="/tags/?tag=%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90" title="标签">#图像生成</a><a href="/tags/?tag=%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB" title="标签">#论文解读</a></div> <div class="last-updated"><span class="prefix">上次更新:</span> <span class="time">2023/06/22, 13:30:30</span></div></div> <div class="page-nav-wapper"><div class="page-nav-centre-wrap"><a href="/pages/ce88a1/" class="page-nav-centre page-nav-centre-prev"><div class="tooltip">基于RGB视频的行为识别</div></a> <a href="/pages/574fcf/" class="page-nav-centre page-nav-centre-next"><div class="tooltip">大模型应用开发及落地</div></a></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/pages/ce88a1/" class="prev">基于RGB视频的行为识别</a></span> <span class="next"><a href="/pages/574fcf/">大模型应用开发及落地</a>→
      </span></p></div></div></div> <div class="article-list"><div class="article-title"><a href="/archives/" class="iconfont icon-bi">最近更新</a></div> <div class="article-wrapper"><dl><dd>01</dd> <dt><a href="/pages/e5511e/"><div>
            VideoLLMs
            <!----></div></a> <span class="date">03-20</span></dt></dl><dl><dd>02</dd> <dt><a href="/pages/6b730d/"><div>
            Video2Script
            <!----></div></a> <span class="date">12-07</span></dt></dl><dl><dd>03</dd> <dt><a href="/pages/ca7dd0/"><div>
            多模态
            <!----></div></a> <span class="date">11-09</span></dt></dl> <dl><dd></dd> <dt><a href="/archives/" class="more">更多文章&gt;</a></dt></dl></div></div></main></div> <div class="footer"><div class="icons"><a href="xzhouzeng@zju.edu.cn" title="发邮件" target="_blank" class="iconfont icon-youjian"></a><a href="https://github.com/xzhouzeng" title="GitHub" target="_blank" class="iconfont icon-github"></a><a href="/pages/b94dba/" title="听音乐" target="_blank" class="iconfont icon-erji"></a></div> 
  Theme by
  <a href="https://github.com/xugaoyi/vuepress-theme-vdoing" target="_blank" title="本站主题">Vdoing</a> 
    | Copyright © 2022-2024
    <span>xzhouzeng | <a href="https://github.com/xzhouzeng/vuepress-theme-vdoing/blob/master/LICENSE" target="_blank">MIT License</a></span></div> <div class="buttons"><div title="返回顶部" class="button blur go-to-top iconfont icon-fanhuidingbu" style="display:none;"></div> <div title="去评论" class="button blur go-to-comment iconfont icon-pinglun" style="display:none;"></div> <div title="主题模式" class="button blur theme-mode-but iconfont icon-zhuti"><ul class="select-box" style="display:none;"><li class="iconfont icon-zidong">
          跟随系统
        </li><li class="iconfont icon-rijianmoshi">
          浅色模式
        </li><li class="iconfont icon-yejianmoshi">
          深色模式
        </li><li class="iconfont icon-yuedu">
          阅读模式
        </li></ul></div></div> <!----> <!----> <!----></div><div class="global-ui"><div></div></div></div>
    <script src="/assets/js/app.9749b4f6.js" defer></script><script src="/assets/js/4.5da4c663.js" defer></script><script src="/assets/js/9.3f132d53.js" defer></script>
  </body>
</html>

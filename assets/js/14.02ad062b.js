(window.webpackJsonp=window.webpackJsonp||[]).push([[14],{565:function(a,t,r){a.exports=r.p+"assets/img/2023-06-08-14-30-13-image.7b865dc2.png"},566:function(a,t,r){a.exports=r.p+"assets/img/2023-06-08-13-55-02-image.7492ec4c.png"},725:function(a,t,r){"use strict";r.r(t);var e=r(4),s=Object(e.a)({},(function(){var a=this,t=a._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[t("h1",{attrs:{id:"大型语言模型-llm"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#大型语言模型-llm"}},[a._v("#")]),a._v(" 大型语言模型 (LLM)")]),a._v(" "),t("h2",{attrs:{id:"基础知识"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#基础知识"}},[a._v("#")]),a._v(" 基础知识")]),a._v(" "),t("h3",{attrs:{id:"分类"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#分类"}},[a._v("#")]),a._v(" 分类")]),a._v(" "),t("p",[a._v("预训练的LLM的类别。黑线表示双向模型中的信息流，而灰线表示从左到右的信息流。")]),a._v(" "),t("ul",[t("li",[t("p",[t("strong",[a._v("Encoder models")]),a._v("，例如BERT，是用上下文感知目标进行训练的。")])]),a._v(" "),t("li",[t("p",[t("strong",[a._v("Decoder models")]),a._v("，例如GPT，是用"),t("strong",[a._v("自回归目标训练的")]),a._v("。")])]),a._v(" "),t("li",[t("p",[t("strong",[a._v("Encoder-decoder models")]),a._v("，例如T5和BART，将两者结合起来，使用上下文感知结构作为编码器，使用从左到右结构作为解码器。")])])]),a._v(" "),t("img",{attrs:{src:r(565),title:"",alt:"","data-align":"center"}}),a._v(" "),t("h3",{attrs:{id:"transformer"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#transformer"}},[a._v("#")]),a._v(" Transformer")]),a._v(" "),t("blockquote",[t("p",[t("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/338817680",target:"_blank",rel:"noopener noreferrer"}},[a._v("Transformer模型详解（图解最完整版） - 知乎 (zhihu.com)"),t("OutboundLink")],1)])]),a._v(" "),t("h2",{attrs:{id:"综述"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#综述"}},[a._v("#")]),a._v(" 综述")]),a._v(" "),t("h3",{attrs:{id:"a-survey-of-large-language-models-2023"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#a-survey-of-large-language-models-2023"}},[a._v("#")]),a._v(" A Survey of Large Language Models（2023）")]),a._v(" "),t("p",[a._v("近年来现有的大型语言模型（大小大于10B）的时间表。时间表主要是根据模型技术文件的发布日期（例如提交给arXiv的日期）确定的。如果没有相应的论文，我们将模型的发布日期设置为其公开发布或公告的最早时间。"),t("strong",[a._v("我们用黄色的公开可用的模型检查点标记LLM")]),a._v("。由于该图的篇幅限制，我们只包括公开报告评估结果的LLM。")]),a._v(" "),t("img",{attrs:{src:r(566),title:"",alt:"","data-align":"center"}}),a._v(" "),t("h2",{attrs:{id:"相关工作"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#相关工作"}},[a._v("#")]),a._v(" 相关工作")]),a._v(" "),t("h3",{attrs:{id:"gpt3-2020-neurips"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#gpt3-2020-neurips"}},[a._v("#")]),a._v(" GPT3（2020 NeurIPS）")]),a._v(" "),t("blockquote",[t("p",[a._v("Language Models are Few-Shot Learners")]),a._v(" "),t("p",[t("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/174782647",target:"_blank",rel:"noopener noreferrer"}},[a._v("最新最全GPT-3模型网络结构详细解析 - 知乎 (zhihu.com)"),t("OutboundLink")],1)])]),a._v(" "),t("h3",{attrs:{id:"palm-2022"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#palm-2022"}},[a._v("#")]),a._v(" PaLM（2022）")]),a._v(" "),t("h3",{attrs:{id:"llama-2023"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#llama-2023"}},[a._v("#")]),a._v(" LLaMa（2023）")]),a._v(" "),t("blockquote",[t("p",[a._v("Large Language Model Meta AI")])])])}),[],!1,null,null,null);t.default=s.exports}}]);
(window.webpackJsonp=window.webpackJsonp||[]).push([[11],{454:function(t,e,o){t.exports=o.p+"assets/img/image-20220316201750798.8ecaec34.png"},455:function(t,e,o){t.exports=o.p+"assets/img/image-20220315201728223.2df6fedb.png"},456:function(t,e,o){t.exports=o.p+"assets/img/image-20220315205819378.9d8fb7e1.png"},457:function(t,e){t.exports="data:image/png;base64,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"},458:function(t,e){t.exports="data:image/png;base64,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"},736:function(t,e,o){"use strict";o.r(e);var i=o(1),r=Object(i.a)({},(function(){var t=this,e=t.$createElement,i=t._self._c||e;return i("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[i("h1",{attrs:{id:"基于rgb视频的行为识别"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#基于rgb视频的行为识别"}},[t._v("#")]),t._v(" 基于RGB视频的行为识别")]),t._v(" "),i("h2",{attrs:{id:"_1-综述"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#_1-综述"}},[t._v("#")]),t._v(" 1. 综述")]),t._v(" "),i("p",[i("strong",[t._v("行为识别Action Recognition")]),t._v("是指对视频中人的行为行为进行识别，即读懂视频。")]),t._v(" "),i("ul",[i("li",[i("p",[t._v("类型划分：")]),t._v(" "),i("ul",[i("li",[i("strong",[t._v("Hand gesture")]),t._v("：集中于处理视频片段中单人的手势")]),t._v(" "),i("li",[i("strong",[t._v("Action")]),t._v("：短时间的行为行为，场景往往是短视频片段的单人行为，比如Throw，catch，clap等")]),t._v(" "),i("li",[i("strong",[t._v("Activity")]),t._v("：持续时间较长的行为，场景往往是较长视频中的单人或多人行为")])])]),t._v(" "),i("li",[i("p",[t._v("任务划分：")]),t._v(" "),i("ul",[i("li",[i("strong",[t._v("Classification")]),t._v("：给定预先裁剪好的视频片段，预测其所属的行为类别✨")]),t._v(" "),i("li",[t._v("**Detection：**视频是未经过裁剪的，需要先进行人的检测where和行为定位（分析行为的始末时间）when，再进行行为的分类what。（行为检测）")])])]),t._v(" "),i("li",[i("p",[t._v("解读：")]),t._v(" "),i("p",[i("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/103566134",target:"_blank",rel:"noopener noreferrer"}},[t._v("一文了解通用行为识别ActionRecognition：了解及分类 - 知乎 (zhihu.com)"),i("OutboundLink")],1),i("img",{attrs:{src:"picture/v2-d1df4c0d34ccb1cda0a5627ac80966ed_r.jpg",alt:"preview"}})])]),t._v(" "),i("li",[i("p",[t._v("框架：")]),t._v(" "),i("p",[i("a",{attrs:{href:"https://github.com/open-mmlab/mmaction2",target:"_blank",rel:"noopener noreferrer"}},[t._v("open-mmlab/mmaction2: OpenMMLab's Next Generation Video Understanding Toolbox and Benchmark (github.com)"),i("OutboundLink")],1)])]),t._v(" "),i("li",[i("p",[t._v("3D-Conv与CNN+LSTM算法参考代码（PyTorch）")]),t._v(" "),i("p",[i("a",{attrs:{href:"https://github.com/MRzzm/action-recognition-models-pytorch",target:"_blank",rel:"noopener noreferrer"}},[t._v("MRzzm/action-recognition-models-pytorch: The models of action recognition with pytorch (github.com)"),i("OutboundLink")],1)])]),t._v(" "),i("li",[i("p",[t._v("基于RGB-D的行为识别综述")]),t._v(" "),i("p",[i("a",{attrs:{href:"https://deepai.org/publication/rgb-d-based-human-motion-recognition-with-deep-learning-a-survey",target:"_blank",rel:"noopener noreferrer"}},[t._v("基于RGB-D的深度学习人体运动识别：|调查深爱 (deepai.org)"),i("OutboundLink")],1)])]),t._v(" "),i("li",[i("p",[t._v("本文档分类。深度学习分类不绝对，涉及到融合模型🍳")]),t._v(" "),i("p",[i("img",{attrs:{src:"picture/%E8%A1%8C%E4%B8%BA%E8%AF%86%E5%88%AB.png",alt:"行为识别"}})])])]),t._v(" "),i("h3",{attrs:{id:"_1-1-a-comprehensive-study-of-deep-video-action-recognition-2020-arxiv"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#_1-1-a-comprehensive-study-of-deep-video-action-recognition-2020-arxiv"}},[t._v("#")]),t._v(" 1.1 A Comprehensive Study of Deep Video Action Recognition（2020 ArXiv）")]),t._v(" "),i("h4",{attrs:{id:"行为识别模型在深度学习方向的三种趋势"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#行为识别模型在深度学习方向的三种趋势"}},[t._v("#")]),t._v(" 行为识别模型在深度学习方向的三种趋势：")]),t._v(" "),i("ol",[i("li",[i("p",[t._v("第一种趋势始于关于"),i("strong",[t._v("双流网络")]),t._v("的开创性论文，它增加了第二条路径，通过在光流上训练卷积神经网络来学习视频中的时间信息。它的巨大成功激发了大量后续论文。")])]),t._v(" "),i("li",[i("p",[t._v("第二个趋势是使用"),i("strong",[t._v("3D卷积")]),t._v("核对视频时间信息建模。")])]),t._v(" "),i("li",[i("p",[t._v("第三种趋势"),i("strong",[t._v("侧重于计算效率")]),t._v("，以扩展到更大的数据集，以便在实际应用中采用。")])])]),t._v(" "),i("ul",[i("li",[t._v("介绍17种数据集，部分总结在"),i("strong",[t._v("5.")])])]),t._v(" "),i("h4",{attrs:{id:"在开发有效的视频行为识别算法方面有几个主要挑战"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#在开发有效的视频行为识别算法方面有几个主要挑战"}},[t._v("#")]),t._v(" 在开发有效的视频行为识别算法方面有几个主要挑战:")]),t._v(" "),i("ul",[i("li",[i("h5",{attrs:{id:"在数据集方面"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#在数据集方面"}},[t._v("#")]),t._v(" 在数据集方面：")]),t._v(" "),i("ul",[i("li",[t._v("首先，定义用于训练行为识别模型的标签空间非常重要。这是因为"),i("strong",[t._v("人类行为通常是复合概念")]),t._v("，这些概念的层次结构没有很好的定义。")]),t._v(" "),i("li",[t._v("其次，"),i("strong",[t._v("为行为识别添加视频注释是一项艰巨的任务")]),t._v("（例如，需要观看所有视频帧），而且模棱两可。（例如，很难确定行动的确切开始和结束）。")]),t._v(" "),i("li",[t._v("第三，一些流行的基准数据集（如Kinetics系列）只发布供用户下载的视频链接，而不是实际的视频，这导致了在不同数据上评估方法的情况。在方法之间进行公平的比较并获得见解是不可能的。")])])]),t._v(" "),i("li",[i("h5",{attrs:{id:"在建模方面"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#在建模方面"}},[t._v("#")]),t._v(" 在建模方面：")]),t._v(" "),i("ul",[i("li",[t._v("首先，"),i("strong",[t._v("捕捉人类行为的视频具有强烈的类内和类间差异")]),t._v("。人们可以在不同的视角下以不同的速度执行相同的行为。此外，有些行为有着相似的行为模式，很难区分。")]),t._v(" "),i("li",[t._v("其次，识别人类行为需要**同时理解短期特定行为的运动信息和长期时间信息。**我们可能需要一个的模型来处理不同的视角（perspectives），而不是使用单一的卷积神经网络。")]),t._v(" "),i("li",[t._v("第三，"),i("strong",[t._v("训练和推理的计算成本都很高")]),t._v("，阻碍了行为识别模型的开发和部署。")])])])]),t._v(" "),i("h4",{attrs:{id:"深度学习在视频行为识别中的应用-🐱‍🏍"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#深度学习在视频行为识别中的应用-🐱‍🏍"}},[t._v("#")]),t._v(" "),i("strong",[t._v("深度学习在视频行为识别中的应用")]),t._v(" 🐱‍🏍")]),t._v(" "),i("img",{staticStyle:{zoom:"80%"},attrs:{src:o(454),alt:"image-20220316201750798"}}),t._v(" "),i("p",[t._v("这一部分回顾了2014年至2020基于深度学习的视频行为识别方法，并介绍了相关的早期工作。")]),t._v(" "),i("h5",{attrs:{id:"_1-from-hand-crafted-features-to-cnns"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#_1-from-hand-crafted-features-to-cnns"}},[t._v("#")]),t._v(" 1. From hand-crafted features to CNNs")]),t._v(" "),i("ol",[i("li",[i("p",[t._v("**IDT：**手工制作的特征，尤其是改进的密集轨迹，由于其高精度和良好的鲁棒性，在2015年之前主导了视频理解文献。")]),t._v(" "),i("blockquote",[i("p",[t._v("Action Recognition with Improved Trajectories（2013 ICCV）")])])]),t._v(" "),i("li",[i("p",[t._v("**DeepVideo：**深度学习在行为识别开创性工作提出在每个视频帧上单独使用一个2D CNN模型，效果较差。")]),t._v(" "),i("blockquote",[i("p",[t._v("Large-Scale Video Classification with Convolutional Neural Networks（2014 CVPR）")])])])]),t._v(" "),i("h5",{attrs:{id:"_2-two-stream-networks"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#_2-two-stream-networks"}},[t._v("#")]),t._v(" 2. Two-stream networks")]),t._v(" "),i("p",[i("strong",[t._v("TwoStreamCNN：")]),t._v("，包括空间流和时间流，空间流以原始视频帧作为输入来捕获视觉外观信息。时间流以一堆光流图像作为输入，以捕获视频帧之间的运动信息。"),i("strong",[t._v("基于CNN的方法首次实现了与之前最好的手工制作功能IDT类似的性能。")])]),t._v(" "),i("p",[t._v("自此出现了许多关于twostream网络的后续论文，极大地推动了视频行为识别的发展。将它们分为几个类别：")]),t._v(" "),i("ul",[i("li",[i("p",[i("strong",[t._v("Using deeper network architectures")])]),t._v(" "),i("p",[t._v("双流网络使用了相对较浅的网络架构。因此，"),i("strong",[t._v("对双流网络的自然扩展需要使用更深的网络。")])]),t._v(" "),i("ol",[i("li",[i("p",[t._v("**DeepTwoStream ：**引入了一系列良好的实践，包括跨模态初始化、同步批处理规范化、角点裁剪和多尺度裁剪数据增强、大Dropout率等，以防止更深层次的网络过度拟合。通过这些良好的实践，能够使用VGG16模型来训练双流网络，在UCF101上的表现大大优于常规双流。")]),t._v(" "),i("blockquote",[i("p",[t._v("Towards Good Practices for Very Deep Two-Stream ConvNets（2015 arXiv preprint）")])])]),t._v(" "),i("li",[i("p",[t._v("**TSN：**时间段网络对网络体系结构进行了彻底的研究，如VGG16、ResNet、Inception，并证明更深的网络通常可以实现更高的视频行为识别准确率。")]),t._v(" "),i("blockquote",[i("p",[t._v("Temporal Segment Networks: Towards Good Practices for Deep Action Recognition（2016 ECCV）")])])])])]),t._v(" "),i("li",[i("p",[i("strong",[t._v("Two-stream fusion")])]),t._v(" "),i("p",[t._v("由于双流网络中有两个流，**早期融合有助于两个流学习更丰富的功能，**并比后期融合提高性能。")]),t._v(" "),i("ol",[i("li",[i("p",[t._v("**TwostreamFusion：**研究早期融合范例的几篇论文中的第一篇，包括如何执行空间融合（例如，使用sum、max、双线性、卷积和级联等运算符），在何处融合网络（例如，早期交互发生的网络层），以及如何执行时间融合（例如，在网络的后期使用2D或3D卷积融合）。表明，早期融合有助于两个流学习更丰富的功能，并比后期融合提高性能。")]),t._v(" "),i("blockquote",[i("p",[t._v("Convolutional Two-Stream Network Fusion for Video Action Recognition（2016 CVPR）")])])]),t._v(" "),i("li",[i("p",[t._v("**ST-ResNet + IDT：**通过引入两条流之间的剩余连接，将ResNet推广到了时空域。")]),t._v(" "),i("blockquote",[i("p",[t._v("Spatiotemporal Residual Networks for Video Action Recognition（2016 NeurIPS）")])])]),t._v(" "),i("li",[i("p",[t._v("**STM Network+IDT：**进一步提出了一种用于剩余网络的乘法选通函数，以更好地学习时空特征。")]),t._v(" "),i("blockquote",[i("p",[t._v("Spatiotemporal Multiplier Networks for Video Action Recognition（2017 CVPR）")])])]),t._v(" "),i("li",[i("p",[t._v("**STPN：**采用时空金字塔来执行两个流之间的分层早期融合。")]),t._v(" "),i("blockquote",[i("p",[t._v("Spatiotemporal Pyramid Network for Video Action Recognition（2017 CVPR）")])])])])]),t._v(" "),i("li",[i("p",[i("strong",[t._v("Recurrent neural networks")])]),t._v(" "),i("p",[t._v("由于视频本质上是一个时间序列，研究人员探索了用于视频内部时间建模的递归神经网络（RNN），特别是长短时记忆（LSTM）的使用。")]),t._v(" "),i("ol",[i("li",[i("p",[t._v("**LRCN和Beyond Short Snippets：**是在双流网络环境下使用LSTM进行视频行为识别的几篇论文中的第一篇。他们将CNN的特征图作为深层LSTM网络的输入，并将帧级CNN特征聚合为视频级预测。请注意，它们分别在两个流上使用LSTM，最终结果仍然是通过后期融合获得的。")])]),t._v(" "),i("li",[i("p",[t._v("**分层多粒度LSTM网络：**根据CNN-LSTM框架，提出了几种变体，分层多粒度LSTM网络是其中一种。（还有双向LSTM、CNN-LSTM融合）")]),t._v(" "),i("blockquote",[i("p",[t._v("Action Recognition by Learning Deep MultiGranular Spatio-Temporal Video Representation（2016 ICMR）")])])]),t._v(" "),i("li",[i("p",[t._v("**VideoLSTM：**包括基于相关性的空间注意机制和基于轻量级运动的注意机制。不仅展示了改进的行为识别结果，还展示了如何通过仅依赖行为类标签将学习到的注意力用于行为定位。")]),t._v(" "),i("blockquote",[i("p",[t._v("VideoLSTM Convolves, Attends and Flows for Action Recognition（2018 CVIU）")])])]),t._v(" "),i("li",[i("p",[t._v("**Lattice LSTM：**通过学习单个空间位置的记忆单元的独立隐态转换来扩展LSTM，因此它可以精确地模拟长期和复杂的运动。")]),t._v(" "),i("blockquote",[i("p",[t._v("Lattice Long Short-Term Memory for Human Action Recognition（2017 ICCV）")])])]),t._v(" "),i("li",[i("p",[t._v("**ShuttleNet：**是一项并行工作，它考虑RNN中的前馈和反馈连接，以了解长期依赖关系。")]),t._v(" "),i("blockquote",[i("p",[t._v("Learning Long-Term Dependencies for Action Recognition With a Biologically-Inspired Deep Network（2017 ICCV）")])])]),t._v(" "),i("li",[i("p",[t._v("**FAST-GRU：**从昂贵的主干网和廉价的主干网聚合剪辑级功能。该策略降低了冗余片段的处理成本，从而加快了推理速度。")])])])]),t._v(" "),i("li",[i("p",[i("strong",[t._v("Segment-based methods")])]),t._v(" "),i("p",[t._v("由于光流，双流网络能够推断帧之间的短期运动信息。然而，它们仍然无法捕获远程时间信息。专注长期时间运动建模")]),t._v(" "),i("ol",[i("li",[i("p",[t._v("**TSN：**能够建模长期时间结构，因为模型可以看到整个视频中的内容。此外，这种稀疏采样策略降低了长视频序列的训练成本，但保留了相关信息。")])]),t._v(" "),i("li",[i("p",[t._v("**DVOF：**深度局部视频特征建议将在局部输入上训练的深度网络作为特征提取器，并训练另一个编码函数，以将全局特征映射到全局标签。")]),t._v(" "),i("blockquote",[i("p",[t._v("Deep Local Video Feature for Action Recognition（2017 CVPRW）")])])]),t._v(" "),i("li",[i("p",[t._v("**TLE：**时间线性编码网络与DVOF同时出现，但编码层被嵌入网络中，因此整个管道可以进行端到端的训练。")]),t._v(" "),i("blockquote",[i("p",[t._v("Deep Temporal Linear Encoding Networks（2017 CVPR）")])])]),t._v(" "),i("li",[i("p",[t._v("**VLAD3和ActionVLAD：**他们将NetVLAD层扩展到视频域，以执行视频级编码，而不是使用紧凑双线性编码。")]),t._v(" "),i("blockquote",[i("p",[t._v("VLAD3: Encoding Dynamics of Deep Features for Action Recognition（2016 CVPR）")]),t._v(" "),i("p",[t._v("ActionVLAD: Learning SpatioTemporal Aggregation for Action Classification（2017 CVPR）")])])]),t._v(" "),i("li",[i("p",[t._v("**TRN：**为了提高TSN的时间推理能力，时间关系网络TRN来学习和推理多时间尺度下视频帧之间的时间依赖关系。")]),t._v(" "),i("blockquote",[i("p",[t._v("Temporal Relational Reasoning in Videos（2018 ECCV）")])])]),t._v(" "),i("li",[i("p",[t._v("**TSM：**最新最先进的高效模型，也是基于分段的，将在后续讨论。")])])])]),t._v(" "),i("li",[i("p",[i("strong",[t._v("Multi-stream networks")])]),t._v(" "),i("p",[t._v("从本质上讲，多流网络是一种多模态学习方法，它使用不同的线索作为输入信号来帮助视频行为识别。")])])]),t._v(" "),i("h5",{attrs:{id:"_3-the-rise-of-3d-cnns"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#_3-the-rise-of-3d-cnns"}},[t._v("#")]),t._v(" 3. The rise of 3D CNNs")]),t._v(" "),i("p",[t._v("预计算光流计算量大，存储要求高，不利于大规模训练或实时部署。从概念上讲，理解视频的一种简单方法是将其视为具有两个空间维度和一个时间维度的3D张量。因此，这导致使用3D CNN作为处理单元来模拟视频中的时间信息。")]),t._v(" "),i("ol",[i("li",[i("p",[t._v("**开创性工作：**使用3D CNN进行行为识别的开创性工作。尽管令人振奋，但该网络的深度还不足以展示其潜力。")]),t._v(" "),i("blockquote",[i("p",[t._v("3D Convolutional Neural Networks for Human Action Recognition（2012 PAMI）")])])]),t._v(" "),i("li",[i("p",[i("strong",[t._v("C3D："),i("strong",[t._v("扩展到一个更深层的3D网络，遵循模块化设计，可以将其视为VGG16网络的3D版本。它在标准基准测试中的性能并不令人满意，但显示出")]),t._v("强大的泛化能力")]),t._v("，可以用作各种视频任务的通用特征提取器。然而，3D网络很难优化。为了更好地训练3D卷积滤波器，人们需要一个具有不同视频内容和行为类别的大规模数据集。幸运的是，有一个数据集Sports1M，它足够大，可以支持深度3D网络的训练。然而，**C3D的训练需要数周时间才能完成。**尽管C3D很受欢迎，但大多数用户只是将其用作不同用例的功能提取器，而不是修改/微调网络。这也是基于2D CNN的双流网络在2014年至2017年间主导视频行为识别领域的部分原因。")])]),t._v(" "),i("li",[i("p",[t._v("**I3D：**将视频剪辑作为输入，并通过堆叠的3D卷积层将其转发。视频剪辑是一系列视频帧，通常使用16或32帧。I3D的主要贡献是：1）它采用了成熟的图像分类体系结构，用于3D CNN；2） 对于模型权重，它采用了为初始化光流网络而开发的方法，将ImageNet预先训练的2D模型权重膨胀到3D模型中的对应权重。因此，"),i("strong",[t._v("I3D绕过了3D CNN必须从头开始训练的困境。"),i("strong",[t._v("通过对一个新的大规模数据集Kinetics400进行预训练，I3D在UCF101和HMDB51上的得分分别为95.6%和74.8%。I3D终结了不同方法在UCF101和HMDB512等小型数据集上报告数字的时代。")]),t._v("（最终的I3D模型是3D CNN和twostream网络的组合。）")])]),t._v(" "),i("blockquote",[i("p",[t._v("Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset（2017 CVPR）")])])])]),t._v(" "),i("p",[t._v("**3D CNN并没有取代两个流网络，它们也不是相互排斥的。**他们只是用不同的方法来模拟视频中的时间关系。")]),t._v(" "),i("ul",[i("li",[i("p",[i("strong",[t._v("Mapping from 2D to 3D CNNs")])]),t._v(" "),i("p",[t._v("2D CNN享受着大规模图像数据集（如ImageNet和Places205）带来的预训练的好处，这些数据集甚至无法与当今最大的视频数据集相匹配。大量的工作致力于寻找更准确、更通用的2D CNN架构并借鉴优点用于3D-CNN.")]),t._v(" "),i("ol",[i("li",[i("p",[t._v("**ResNet3D：**直接采用2D ResNet，并用3D内核替换所有2D卷积滤波器。他们相信，通过将深度3D CNN与大规模数据集结合使用，可以利用ImageNet上2D CNN的成功。")]),t._v(" "),i("blockquote",[i("p",[t._v("Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet? （2018 CVPR）")])])]),t._v(" "),i("li",[i("p",[t._v("**MFNet：**受ResNeXt的启发，提出了一种多光纤结构，将复杂的神经网络分割成一个轻量级网络（光纤）的集合，以促进光纤之间的信息流动，同时降低计算成本。")]),t._v(" "),i("blockquote",[i("p",[t._v("Multi-Fiber Networks for Video Recognition（2018 ECCV）")])])]),t._v(" "),i("li",[i("p",[t._v("**STCNet：**受SENet的启发，提出在3D块内集成信道信息，以捕获整个网络中的空间信道和时间信道相关信息。")]),t._v(" "),i("blockquote",[i("p",[t._v("Spatio-Temporal Channel Correlation Networks for Action Classification（2018 ECCV）")])])])])]),t._v(" "),i("li",[i("p",[i("strong",[t._v("Unifying 2D and 3D CNNs")])]),t._v(" "),i("p",[t._v("主要是为了降低3D网络训练的复杂性")]),t._v(" "),i("ol",[i("li",[i("p",[t._v("**P3D和R2+1D：**探索了3D分解的概念。具体来说，一个3D内核（例如3×3×3）可以分解为两个独立的操作，一个2D空间卷积（例如1×3×3）和一个1D时间卷积（例如3×1×1）。P3D和R2+1D之间的区别在于它们如何安排两个因式分解操作，以及如何形成每个剩余块。")])]),t._v(" "),i("li",[i("p",[t._v("**TrajectoryCNN：**遵循这一思想，但对时间分量使用可变形卷积来更好地处理运动。")])]),t._v(" "),i("li",[i("p",[i("strong",[t._v("MiCT："),i("strong",[t._v("简化3D CNN的另一种方法是在单个网络中")]),t._v("混合2D和3D卷积")]),t._v("，MiCTNet集成了2D和3D CNN，以生成更深入、信息更丰富的特征地图，同时降低了每一轮时空融合的训练复杂性。")]),t._v(" "),i("blockquote",[i("p",[t._v("MiCT: Mixed 3D/2D Convolutional Tube for Human Action Recognition（2018 CVPR）")])])]),t._v(" "),i("li",[i("p",[t._v("**ARTNet：**通过使用新的构建块引入了一个外观和关系网络。构建块由使用二维CNN的空间分支和使用三维CNN的关系分支组成。")]),t._v(" "),i("blockquote",[i("p",[t._v("Appearance-and-Relation Networks for Video Classification（2018 CVPR）")])])]),t._v(" "),i("li",[i("p",[t._v("**S3D：**结合了上述方法的优点。首先用二维核函数代替网络底部的三维卷积，发现这种顶重网络具有更高的识别精度。然后S3D将剩余的3D核分解为P3D和R2+1D，以进一步减小模型大小和训练复杂度。")]),t._v(" "),i("blockquote",[i("p",[t._v("Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification（2018 ECCV）")])])]),t._v(" "),i("li",[i("p",[t._v("**ECO：**也采用了这样一个重上加重的网络来实现在线视频理解。")]),t._v(" "),i("blockquote",[i("p",[t._v("ECO: Efficient Convolutional Network for Online Video Understanding（2018 ECCV）")])])])])]),t._v(" "),i("li",[i("p",[i("strong",[t._v("Long-range temporal modeling")])]),t._v(" "),i("p",[t._v("在3D CNN中，可以通过叠加多个短时间卷积（例如，3×3×3滤波器）来实现远程时间连接。然而，有用的时间信息可能会在深度网络的后期丢失，尤其是对于相距较远的帧。为了进行长期时间建模，")]),t._v(" "),i("ol",[i("li",[i("p",[t._v("**LTC：**引入并评估了大量视频帧上的长期时间卷积。然而，由于GPU内存的限制，他们不得不牺牲输入分辨率来使用更多的帧。")]),t._v(" "),i("blockquote",[i("p",[t._v("Longterm Temporal Convolutions for Action Recognition（2018 PAMI）")])])]),t._v(" "),i("li",[i("p",[t._v("**T3D：**采用了一种紧密连接的结构，以尽可能完整地保留原始时间信息，从而进行最终预测。")]),t._v(" "),i("blockquote",[i("p",[t._v("Temporal 3D ConvNets: New Architecture and Transfer Learning for Video Classification（2017 arXiv preprint）")])])]),t._v(" "),i("li",[i("p",[t._v("**Non-Local："),i("strong",[t._v("引入了一种新的构造块，“非局部”是一种类似于“自我注意”的通用操作，它可以以即插即用的方式用于许多计算机视觉任务。他们在随后的残差块之后使用")]),t._v("时空非局部模块来捕获空间和时间域中的长期依赖性，**并在没有钟声和哨声的情况下实现了基线性能的改进。")]),t._v(" "),i("blockquote",[i("p",[t._v("Non-Local Neural Networks（2018 CVPR）")])])]),t._v(" "),i("li",[i("p",[t._v("**LFB：**提出了一种特征库表示法，将整个视频的信息嵌入到一个存储单元中，以进行上下文感知预测。")]),t._v(" "),i("blockquote",[i("p",[t._v("Long-Term Feature Banks for Detailed Video Understanding（2019 CVPR）")])])]),t._v(" "),i("li",[i("p",[t._v("**V4D：**提出了视频级4D CNN，用4D卷积来模拟长距离时空表示的演化。")]),t._v(" "),i("blockquote",[i("p",[t._v("V4D:4D Convolutional Neural Networks for Video-level Representation Learning（2020 ICLR）")])])])])]),t._v(" "),i("li",[i("p",[i("strong",[t._v("Enhancing 3D efficiency")])]),t._v(" "),i("p",[t._v("为了进一步提高3D CNN的效率（即在GFLOP、模型参数和延迟方面），3D CNN的许多变体开始出现。受高效2D网络发展的推动，研究人员开始采用通道可分离卷积，并将其扩展到视频分类。")]),t._v(" "),i("ol",[i("li",[i("p",[t._v("**CSN：**通过分离通道相互作用和时空相互作用来分解3D卷积是一种很好的做法，并且能够获得最先进的性能，同时比以前的最佳方法快2到3倍。")]),t._v(" "),i("blockquote",[i("p",[t._v("Video Classification With Channel-Separated Convolutional Networks（2019 ICCV）")])])]),t._v(" "),i("li",[i("p",[t._v("**SlowFast：**是一种具有慢路径和快路径的高效网络。慢速路径以低帧速率运行以捕获详细的语义信息，而快速路径以高时间分辨率运行以捕获快速变化的运动。为了整合运动信息，例如在两个流网络中，SlowFast采用横向连接来融合每个路径学习到的表示。由于快速通道可以通过降低其通道容量而变得非常轻量级，因此SlowFast的整体效率大大提高。"),i("strong",[t._v("尽管SlowFast有两条路径，但它不同于两条流网络，因为这两条路径旨在模拟不同的时间速度，而不是空间和时间建模。")])]),t._v(" "),i("blockquote",[i("p",[t._v("SlowFast Networks for Video Recognition（2019 ICCV）")])])]),t._v(" "),i("li",[i("p",[t._v("**X3D：**同时使用多种途径来平衡准确性和效率，它沿着多个网络轴逐步扩展2D图像分类体系结构，如时间持续时间、帧速率、空间分辨率、宽度、瓶颈宽度和深度。X3D将3D模型修改/因子分解推到了极致，是一系列高效的视频网络，以满足目标复杂性的不同要求。")]),t._v(" "),i("blockquote",[i("p",[t._v("X3D: Expanding Architectures for Efficient Video Recognition（2020 CVPR）")])])]),t._v(" "),i("li",[i("p",[t._v("**A3D：**也利用了多种网络配置。然而，A3D联合训练这些配置，并且在推理过程中只部署一个模型。这最终使模型更加有效。")]),t._v(" "),i("blockquote",[i("p",[t._v("A3d: Adaptive 3d networks for video action recognition（2020）")])])])])])]),t._v(" "),i("p",[t._v("（下一节继续讨论高效的视频建模，但不基于3D卷积。）")]),t._v(" "),i("h5",{attrs:{id:"_4-efficient-video-modeling"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#_4-efficient-video-modeling"}},[t._v("#")]),t._v(" 4. Efficient Video Modeling")]),t._v(" "),i("p",[t._v("随着数据集大小的增加和部署的需要，效率成为一个重要的关注点。基于双流网络的方法中，"),i("strong",[t._v("光流量的预计算很大")]),t._v("，"),i("strong",[t._v("同时存储所有光流图像需要巨大的空间")]),t._v("，这将使I/O成为训练过程中最紧密的瓶颈；基于3D CNN的方法中，**3D CNN很难训练和部署。**在训练方面，需要很长的训练周期和巨大的计算成本，在部署方面，不同平台对3D卷积的支持不如2D卷积。此外，3D CNN需要更多的视频帧作为输入，这增加了额外的IO成本。")]),t._v(" "),i("p",[t._v("因此，从2018年开始，研究人员开始研究其他替代方法，以了解它们如何能够同时提高视频行为识别的准确性和效率。我们将在下面的几个类别中回顾最近有效的视频建模方法：")]),t._v(" "),i("ul",[i("li",[i("p",[i("strong",[t._v("Flow-mimic approaches")])]),t._v(" "),i("p",[t._v("双流网络的主要缺点之一是需要光流。预计算光流的计算成本很高，存储要求很高，对于视频行为识别来说，它不是端到端可训练的。如果我们能找到一种"),i("strong",[t._v("不使用光流编码运动信息的方法")]),t._v("，至少在推理过程中，这是很有吸引力的。")]),t._v(" "),i("ol",[i("li",[i("p",[t._v("**ActionFlowNet：**是学习估计网络内部光流以进行视频行为识别的早期尝试。虽然这种方法在推理过程中不需要光流，但它在训练过程中需要光流来训练流量估计网络。")]),t._v(" "),i("blockquote",[i("p",[t._v("ActionFlowNet: Learning Motion Representation for Action Recognition（2018 WACV）")])])]),t._v(" "),i("li",[i("p",[t._v("**Hidden Two-Stream：**提出了MotionNet来取代传统的光流计算。MotionNet是一个轻量级的网络，用于以无监督的方式学习运动信息，当与时间流连接时，它是端到端可训练的。因此，隐藏的双流CNN只将原始视频帧作为输入，直接预测行为类别，而无需显式计算光流，无论是在训练阶段还是推理阶段。")]),t._v(" "),i("blockquote",[i("p",[t._v("Hidden Two-Stream Convolutional Networks for Action Recognition（2018 ACCV）")])])]),t._v(" "),i("li",[i("p",[t._v("**PAN：**通过计算连续特征图之间的差异来模拟光流特征。")]),t._v(" "),i("blockquote",[i("p",[t._v("Pan:Towards fast action recognition via learning persistence of appearance（2020）")])])]),t._v(" "),i("li",[i("p",[t._v("按照这个方向，下述论文继续研究端到端可训练CNN，**从数据中学习类似opticalflow的特征。**他们直接从光流的定义中得出这些特征。")]),t._v(" "),i("blockquote",[i("p",[t._v("Optical Flow Guided Feature: A Fast and Robust Motion Representation for Video Action Recognition（2018 CVPR）")]),t._v(" "),i("p",[t._v("End-to-End Learning of Motion Representation for Video Understanding（2018 CVPR）")]),t._v(" "),i("p",[t._v("Motion Feature Network: Fixed Motion Filter for Action Recognition（2018 ECCV）")]),t._v(" "),i("p",[t._v("Representation Flow for Action Recognition（2019 CVPR）")])])]),t._v(" "),i("li",[i("p",[t._v("**MARS和D3D：**使用知识蒸馏将两个流网络组合成一个流，例如，通过调整空间流来预测时间流的输出。")]),t._v(" "),i("blockquote",[i("p",[t._v("MARS: Motion-Augmented RGB Stream for Action Recognition（2019 CVPR）")]),t._v(" "),i("p",[t._v("D3D: Distilled 3D Networks for Video Action Recognition（2020 WACV）")])])]),t._v(" "),i("li",[i("p",[t._v("**MotionSqueeze：**提出MotionSqueeze模块来估计运动特征。提议的模块是端到端可训练的，可以插入任何网络。")]),t._v(" "),i("blockquote",[i("p",[t._v("Motionsqueeze: Neural motion feature learning for video understanding（2020 ECCV）")])])])])]),t._v(" "),i("li",[i("p",[i("strong",[t._v("Temporal modeling without 3D convolution")])]),t._v(" "),i("p",[t._v("一个简单而自然的选择是使用3D卷积来模拟帧之间的时间关系。然而，要实现这一目标，还有很多选择。在这里，我们将回顾一些最近的工作，这些"),i("strong",[t._v("工作在没有三维卷积的情况下执行时间建模。")])]),t._v(" "),i("ol",[i("li",[i("p",[t._v("**TSM："),i("strong",[t._v("称为时间移位模块。TSM")]),t._v("将移位操作扩展到视频理解。**它沿着时间维度移动部分通道，从而促进相邻帧之间的信息交换。为了保持空间特征的学习能力，他们在残差块的残差分支中加入了时间移位模块。因此，在通过身份映射进行时间转移后，原始激活中的所有信息仍然可以访问。TSM的最大优点是，它可以插入2D CNN中，以实现零计算和零参数的时间建模。")]),t._v(" "),i("blockquote",[i("p",[t._v("TSM: Temporal Shift Module for Efficient Video Understanding（2019 ICCV）")])])]),t._v(" "),i("li",[i("p",[t._v("**TIN：**与TSM类似，TIN引入了一个时间隔行模块来模拟时间卷积。")]),t._v(" "),i("blockquote",[i("p",[t._v("Temporal interlacing network（2020）")])]),t._v(" "),i("p",[t._v("📢最近有几种2D CNN方法利用"),i("strong",[t._v("注意力进行长期时间建模")]),t._v("，注意，这些基于注意的方法不同于"),i("strong",[t._v("Non-Local")]),t._v("，因为它们使用通道注意，而非局部使用空间注意。")])]),t._v(" "),i("li",[i("p",[t._v("**STM：**提出了一个通道方向的时空模块来呈现时空特征，以及一个通道方向的运动模块来高效地编码运动特征。")]),t._v(" "),i("blockquote",[i("p",[t._v("STM: SpatioTemporal and Motion Encoding for Action Recognition（2019 ICCV）")])])]),t._v(" "),i("li",[i("p",[t._v("**TEA：**与STM类似，但受SENet的启发，TEA使用运动特征来重新校准时空特征，以增强运动模式。具体来说，TEA有两个组件：运动激励和多时间聚集，第一个组件处理短距离运动建模，第二个组件有效地扩大了长距离时间建模的时间感受野。它们是互补的，重量都很轻，因此TEA能够在保持尽可能多的2D CNN的同时，与以前的最佳方法取得竞争性结果。")]),t._v(" "),i("blockquote",[i("p",[t._v("TEA: Temporal Excitation and Aggregation for Action Recognition（2020 CVPR）")])])]),t._v(" "),i("li",[i("p",[t._v("**TEINet：**也开始关注增强时间建模。")]),t._v(" "),i("blockquote",[i("p",[t._v("TEINet: Towards an Efficient Architecture for Video Recognition（2020 AAAI）")])])])])])]),t._v(" "),i("h5",{attrs:{id:"_5-miscellaneous"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#_5-miscellaneous"}},[t._v("#")]),t._v(" 5. Miscellaneous")]),t._v(" "),i("p",[t._v("在本节中，我们将展示过去十年中视频行为识别的几个"),i("strong",[t._v("流行方向")]),t._v("：")]),t._v(" "),i("ul",[i("li",[i("p",[i("strong",[t._v("Trajectory-based methods")])]),t._v(" "),i("p",[t._v("虽然基于CNN的方法已经显示出其优越性，并逐渐取代了传统的手工制作方法，但传统的局部特征管道仍有其不可忽视的优点，如轨迹的使用。这里，"),i("strong",[t._v("轨迹被定义为在时间维度上跟踪像素的路径。")])]),t._v(" "),i("ol",[i("li",[i("p",[t._v("**Trajectory-Pooled：**轨迹约束池，将深度卷积特征聚合为有效的描述符，他们称之为TDD。这种新的视频表现形式既有手工制作的功能，也有深入学习的功能。")]),t._v(" "),i("blockquote",[i("p",[t._v("Action Recognition With Trajectory-Pooled Deep-Convolutional Descriptors（2015 CVPR）")])])]),t._v(" "),i("li",[i("p",[t._v("同时，Lan等人将独立子空间分析（ISA）和密集轨迹结合到标准的双流网络中，并展示了数据独立和数据驱动方法之间的互补性。")])]),t._v(" "),i("li",[i("p",[t._v("**TrajectoryCNN：**没有将CNN视为固定的特征提取器，而是提出了轨迹卷积，利用轨迹沿时间维度学习特征。")]),t._v(" "),i("blockquote",[i("p",[t._v("Trajectory Convolution for Action Recognition（2018 NeurIPS）")])])])])]),t._v(" "),i("li",[i("p",[i("strong",[t._v("Rank pooling")])]),t._v(" "),i("p",[t._v("还有另一种在视频中建模时间信息的方法，称为排名池（又称学习排名）。")]),t._v(" "),i("ol",[i("li",[i("p",[t._v("**VideoDarwin：**这一领域的开创性工作，它使用排序机器来了解外观随时间的演变，并返回一个排序函数。排名函数应该能够在时间上对视频帧进行排序，因此它们使用该排名函数的参数作为新的视频表示。VideoDarwin不是一种基于深度学习的方法，但其性能和效率相当。")]),t._v(" "),i("blockquote",[i("p",[t._v("Modeling Video Evolution For Action Recognition（2015 CVPR）")])])]),t._v(" "),i("li",[i("p",[t._v("**End2endRP：**为了使秩池适应深度学习，引入了一个可微秩池层来实现端到端的特征学习。")]),t._v(" "),i("blockquote",[i("p",[t._v("Learning End-to-end Video Classification with Rank-Pooling（2016 ICML）")])])]),t._v(" "),i("li",[i("p",[t._v("**DINet：**对视频的原始图像像素应用秩池，为每个视频生成单个RGB图像，称为动态图像。")]),t._v(" "),i("blockquote",[i("p",[t._v("Dynamic Image Networks for Action Recognition（2016 CVPR）")])])]),t._v(" "),i("li",[i("p",[t._v("**DHRP：**通过叠加多级时间编码，将秩池扩展到分层秩池。")]),t._v(" "),i("blockquote",[i("p",[t._v("Discriminative Hierarchical Rank Pooling for Activity Recognition（2016 CVPR）")])])]),t._v(" "),i("li",[i("p",[t._v("**GRP：**使用子空间表示法对原始Rank Pooling公式进行了推广，并表明它可以显著更好地表示行为的动态演化，同时计算成本较低。")]),t._v(" "),i("blockquote",[i("p",[t._v("Generalized Rank Pooling for Activity Recognition（2016 CVPR）")])])])])]),t._v(" "),i("li",[i("p",[i("strong",[t._v("Compressed video action recognition")])]),t._v(" "),i("p",[t._v("大多数视频行为识别方法使用原始视频（或解码视频帧）作为输入。然而，使用原始视频有几个缺点，例如数据量大和时间冗余度高。视频压缩方法通常通过重用另一帧（即i帧）的内容来存储一个帧，并且由于相邻帧相似，因此仅存储差异（即P帧和B帧）。这里，"),i("strong",[t._v("I帧是原始RGB视频帧，P帧和B帧包括运动矢量和残差，"),i("strong",[t._v("用于存储差异。受视频压缩领域发展的推动，研究人员开始")]),t._v("采用压缩视频表示作为输入来训练有效的视频模型。")])]),t._v(" "),i("ol",[i("li",[i("p",[i("strong",[t._v("EMV-CNN："),i("strong",[t._v("由于运动矢量结构粗糙，可能包含不准确的运动，EMV-CNN采用了")]),t._v("知识提取")]),t._v("来帮助基于运动矢量的时间流模拟基于光流的时间流。 然而，他们的方法需要提取和处理每一帧。他们获得了与标准双流网络相当的识别精度，但速度快了27倍。")]),t._v(" "),i("blockquote",[i("p",[t._v("Real-time Action Recognition with Enhanced Motion Vector CNNs（2016 CVPR）")])])]),t._v(" "),i("li",[i("p",[t._v("**CoViAR：**在I帧中使用了重量级CNN，在P帧中使用了轻量级CNN。这要求通过累加将每个P帧的运动矢量和残差引用回I帧。")]),t._v(" "),i("blockquote",[i("p",[t._v("Compressed Video Action Recognition（2018 CVPR）")])])]),t._v(" "),i("li",[i("p",[t._v("**DMC-Net：**是CoViAR使用对抗性损失的后续工作。它采用了一个轻量级的生成器网络来帮助运动矢量捕捉精细的运动细节，而不是像[256]中那样进行知识提取。")]),t._v(" "),i("blockquote",[i("p",[t._v("DMC-Net: Generating Discriminative Motion Cues for Fast Compressed Video Action Recognition（2019 CVPR）")])])]),t._v(" "),i("li",[i("p",[i("strong",[t._v("SCSampler："),i("strong",[t._v("也采用压缩视频表示法对显著片段进行采样，我们将在")]),t._v("Frame/Clip sampling")]),t._v("中讨论。")]),t._v(" "),i("p",[t._v("👀"),i("strong",[t._v("由于增加了复杂性，目前还没有一种压缩方法能够处理B帧。")])])])])]),t._v(" "),i("li",[i("p",[i("strong",[t._v("Frame/Clip sampling")])]),t._v(" "),i("p",[t._v("前面提到的大多数深度学习方法都会平等地对待每一个视频帧/片段进行最终预测。然而，有判别性行为只会在几分钟内发生，而且大多数其他视频内容与标记的行为类别无关或弱相关。这种模式有几个缺点。首先，使用大量不相关的视频帧进行训练可能会影响性能。第二，这种均匀抽样在推理过程中是无效的。在一定程度上，受人类在整个视频中只需几次浏览就能理解视频的启发，人们提出了许多方法"),i("strong",[t._v("来对信息量最大的视频帧/片段进行采样，以提高性能，并在推理过程中提高模型的效率。")])]),t._v(" "),i("ol",[i("li",[i("p",[t._v("**KVM：**是最早提出端到端框架的尝试之一，该框架可以同时识别关键卷并进行操作分类。")]),t._v(" "),i("blockquote",[i("p",[t._v("A Key V olume Mining Deep Framework for Action Recognition（2016 CVPR）")])])]),t._v(" "),i("li",[i("p",[t._v("**AdaScan：**以在线方式预测每个视频帧的重要性分数，他们称之为自适应时间池。")]),t._v(" "),i("blockquote",[i("p",[t._v("AdaScan: Adaptive Scan Pooling in Deep Convolutional Neural Networks for Human Action Recognition in Videos（2017 CVPR）")])]),t._v(" "),i("p",[t._v("这两种方法都提高了性能，但它们仍然采用标准的评估方案，在推理过程中没有显示出效率。最近的方法更注重效率：")])]),t._v(" "),i("li",[i("p",[t._v("**AdaFrame：**使用基于强化学习的方法来搜索信息量更大的视频剪辑。")]),t._v(" "),i("blockquote",[i("p",[t._v("AdaFrame: Adaptive Frame Selection for Fast Video Recognition（2019 CVPR）")])])]),t._v(" "),i("li",[i("p",[t._v("**使用教师-学生框架，**也就是说，一个全视老师可以用来训练一个计算效率很低的学生。他们证明，高效的学生网络可以减少30%的推理时间和大约90%的失败次数，而性能下降可以忽略不计。")]),t._v(" "),i("blockquote",[i("p",[t._v("Efficient Video Classification Using Fewer Frames（2019 CVPR）")])])]),t._v(" "),i("li",[i("p",[t._v("**SCSampler：**训练了一个轻量级网络，以基于压缩视频表示对最显著的视频片段进行采样，并在Kinetics400和Sports1M数据集上实现最先进的性能。他们还从经验上证明，这种基于显著性的采样不仅有效，而且比使用所有视频帧具有更高的准确性。")]),t._v(" "),i("blockquote",[i("p",[t._v("SCSampler: Sampling Salient Clips From Video for Efficient Action Recognition（2019 ICCV）")])])])])]),t._v(" "),i("li",[i("p",[i("strong",[t._v("Visual tempo")])]),t._v(" "),i("p",[t._v("视觉节奏是一个**描述行为速度的概念。"),i("strong",[t._v("许多行为类有不同的视觉节奏。在大多数情况下，区分它们的关键是它们的视觉节奏，因为它们在视觉外观上可能有着高度的相似性，比如走路、慢跑和跑步。有几篇论文探讨了")]),t._v("不同的时间速率（节拍）**来改进时间建模。")]),t._v(" "),i("ol",[i("li",[i("p",[t._v("最初的尝试通常通过以"),i("strong",[t._v("多种速率对原始视频进行采样并构建输入级帧金字塔")]),t._v("来捕捉视频节奏。")]),t._v(" "),i("blockquote",[i("p",[t._v("Bidirectional Multirate Reconstruction for Temporal Modeling in Videos（2017 CVPR）")]),t._v(" "),i("p",[t._v("Temporal Difference Networks for Video Action Recognition（2018 WACV）")]),t._v(" "),i("p",[t._v("Random Temporal Skipping for Multirate Video Analysis（2018 ACCV）")])])]),t._v(" "),i("li",[i("p",[t._v("**SlowFast：**利用视觉节奏的特点设计了一个双通道网络，以实现更好的准确性和效率权衡。")])]),t._v(" "),i("li",[i("p",[t._v("**CIDC：**提出了方向性时间建模以及用于视频时间建模的本地主干。")]),t._v(" "),i("blockquote",[i("p",[t._v("Directional temporal modeling for action recognition（2020 ECCV）")])])]),t._v(" "),i("li",[i("p",[t._v("**TPN：**将节奏建模扩展到了特征级别，并显示出与之前方法相比的持续改进。")]),t._v(" "),i("blockquote",[i("p",[t._v("Temporal Pyramid Network for Action Recognition（2020 CVPR）")])])])])])]),t._v(" "),i("h4",{attrs:{id:"评价和基准"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#评价和基准"}},[t._v("#")]),t._v(" 评价和基准")]),t._v(" "),i("p",[t._v("**评价策略：**在模型训练过程中，我们通常随机选取一个视频帧/片段，形成小批量样本。然而，为了进行评估，我们需要一个标准化的管道，以便进行公平的比较。")]),t._v(" "),i("ul",[i("li",[t._v("对于2D CNN，一种广泛采用的评估方案是从以下视频中"),i("strong",[t._v("均匀采样25帧")]),t._v("。对于每一帧，我们通过裁剪4个角点和1个中心点，水平翻转它们，并在样本的所有裁剪上平均预测分数（在softmax操作之前），来执行"),i("strong",[t._v("10个裁剪数据增强")]),t._v("，也就是说，这意味着我们每个视频使用250帧进行推断。")]),t._v(" "),i("li",[t._v("对于3D CNN，一种被广泛采用的评估方案称为30视图策略，每个视频中"),i("strong",[t._v("均匀采样10个片段")]),t._v("。对于每个视频剪辑，我们执行"),i("strong",[t._v("三次裁剪数据增强")]),t._v("。具体来说，我们将较短的空间侧缩放到256像素，并采用三种256×256的裁剪来覆盖空间维度，并平均预测分数。")]),t._v(" "),i("li",[t._v("**评估方案并非固定不变。**它们正在进化并适应新的网络架构和不同的数据集。例如，TSM对于小型数据集每个视频只使用两个剪辑，并且对每个剪辑执行三次裁剪数据增强，尽管它是2D CNN。")])]),t._v(" "),i("p",[t._v("在评估指标方面，我们报告了单标签动作识别的准确度，以及多标签动作识别的mAP（平均准确度）。")]),t._v(" "),i("p",[t._v("我们将常见基准分为三类：场景聚焦（UCF101、HMDB51和Kinetics400）、运动聚焦（Sth V1和V2）和多标签（Charades），三个实验：")]),t._v(" "),i("ul",[i("li",[i("p",[i("strong",[t._v("以场景为中心的数据集")])])]),t._v(" "),i("li",[i("p",[i("strong",[t._v("运动聚焦数据集")])]),t._v(" "),i("p",[t._v("我们发现使用更长的输入（例如16帧）通常更好。此外，"),i("strong",[t._v("专注于时间建模的方法")]),t._v("比堆叠的3D内核工作得更好。例如，TSM、TEA和MSNet将一个显式的时态推理模块插入到2D ResNet主干中，并获得最先进的结果。这意味着Sth数据集需要强大的时间运动推理以及空间语义信息。")])]),t._v(" "),i("li",[i("p",[t._v("**多标签数据集 **")]),t._v(" "),i("p",[t._v("从实验中我们得出以下观察结果。首先，3D模型的性能通常优于2D模型和带有光流输入的2D模型。这表明"),i("strong",[t._v("时空推理对于长期复杂的并发行为理解至关重要")]),t._v("。第二，**较长的输入有助于识别，**可能是因为有些动作需要长期的特征才能识别。第三，在更大的数据集上预先训练的具有强大骨干的模型一般表现更好。")])])]),t._v(" "),i("h4",{attrs:{id:"行为识别可探讨的方向"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#行为识别可探讨的方向"}},[t._v("#")]),t._v(" 行为识别可探讨的方向")]),t._v(" "),i("ol",[i("li",[i("strong",[t._v("分析和见解（综述）")])]),t._v(" "),i("li",[i("strong",[t._v("数据增强")])]),t._v(" "),i("li",[i("strong",[t._v("视频域适配")])]),t._v(" "),i("li",[i("strong",[t._v("神经架构搜索")])]),t._v(" "),i("li",[i("strong",[t._v("高效的模型开发")])]),t._v(" "),i("li",[i("strong",[t._v("新数据集")])]),t._v(" "),i("li",[i("strong",[t._v("视频对抗攻击")])]),t._v(" "),i("li",[i("strong",[t._v("zero-shot 行为识别")])]),t._v(" "),i("li",[i("strong",[t._v("弱监督视频动作识别")])]),t._v(" "),i("li",[i("strong",[t._v("细粒度视频动作识别")])]),t._v(" "),i("li",[i("strong",[t._v("自我中心行为识别")])]),t._v(" "),i("li",[i("strong",[t._v("多模态")])]),t._v(" "),i("li",[i("strong",[t._v("自监督视频表征学习")])])]),t._v(" "),i("h2",{attrs:{id:"_2-传统算法"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#_2-传统算法"}},[t._v("#")]),t._v(" 2. 传统算法")]),t._v(" "),i("h3",{attrs:{id:"_2-1-dt-2013-ijcv"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-dt-2013-ijcv"}},[t._v("#")]),t._v(" 2.1 DT（2013 IJCV）")]),t._v(" "),i("blockquote",[i("p",[t._v("Dense Trajectories and Motion Boundary Descriptors for Action Recognition")])]),t._v(" "),i("p",[i("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/27528934",target:"_blank",rel:"noopener noreferrer"}},[t._v("行为识别笔记：improved dense trajectories算法（iDT算法） - 知乎 (zhihu.com)"),i("OutboundLink")],1)]),t._v(" "),i("ul",[i("li",[i("p",[i("strong",[t._v("解读：")])]),t._v(" "),i("p",[t._v("框架包括密集采样点特征、特征点轨迹跟踪和基于轨迹的特征提取三部分，后续再进行特征编码和分类。")]),t._v(" "),i("p",[t._v("在得到视频对应的特征后，DT算法采用SVM分类器进行分类，采用one-against-rest策略训练多类分类器。")])]),t._v(" "),i("li",[i("p",[i("strong",[t._v("模型：")])]),t._v(" "),i("p",[i("img",{attrs:{src:"picture/image-20220122131931570.png",alt:"image-20220122131931570"}})])])]),t._v(" "),i("h3",{attrs:{id:"_2-2-idt-2013-iccv"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-idt-2013-iccv"}},[t._v("#")]),t._v(" 2.2 iDT（2013 ICCV）")]),t._v(" "),i("blockquote",[i("p",[t._v("Action Recognition with Improved Trajectories")])]),t._v(" "),i("ul",[i("li",[t._v("iDT算法的基本框架和DT算法相同，主要改进在于对光流图像的优化，特征正则化方式的改进以及特征编码方式的改进。")]),t._v(" "),i("li",[t._v("通过估计相机运动估计来消除背景上的光流以及轨迹\n"),i("ul",[i("li",[t._v("对于HOF,HOG和MBH特征采取了与DT算法（L2范数归一化）不同的方式——L1正则化后再对特征的每个维度开平方")])])]),t._v(" "),i("li",[t._v("使用效果更好的Fisher Vector特征编码")])]),t._v(" "),i("h2",{attrs:{id:"_3-深度学习方法"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#_3-深度学习方法"}},[t._v("#")]),t._v(" 3. 深度学习方法")]),t._v(" "),i("h3",{attrs:{id:"_3-1-two-stream"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-two-stream"}},[t._v("#")]),t._v(" 3.1 Two-Stream")]),t._v(" "),i("p",[t._v("Two-Stream将行为识别中的特征提取分为两个分支，一个是RGB分支提取空间特征，另一个是光流分支提取时间上的光流特征，最后结合两种特征进行行为识别。")]),t._v(" "),i("ul",[i("li",[i("p",[t._v("解读：")]),t._v(" "),i("p",[i("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/40964492",target:"_blank",rel:"noopener noreferrer"}},[t._v("论文笔记——基于深度学习的视频行为识别/行为识别（一） - 知乎 (zhihu.com)"),i("OutboundLink")],1)])])]),t._v(" "),i("h4",{attrs:{id:"twostreamcnn-2014-neurips"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#twostreamcnn-2014-neurips"}},[t._v("#")]),t._v(" TwoStreamCNN（2014 NeurIPS）")]),t._v(" "),i("blockquote",[i("p",[t._v("Two-stream convolutional networks for action recognition in videos")])]),t._v(" "),i("ul",[i("li",[i("p",[i("strong",[t._v("解读：")])])]),t._v(" "),i("li",[i("p",[t._v("多任务学习")])]),t._v(" "),i("li",[i("p",[i("a",{attrs:{href:"https://blog.csdn.net/liuxiao214/article/details/78377791",target:"_blank",rel:"noopener noreferrer"}},[t._v("(1条消息) 【论文学习】Two-Stream Convolutional Networks for Action Recognition in Videos_I am what i am-CSDN博客"),i("OutboundLink")],1)])])]),t._v(" "),i("h4",{attrs:{id:"lrcn-2015-cvpr"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#lrcn-2015-cvpr"}},[t._v("#")]),t._v(" LRCN（2015 CVPR）")]),t._v(" "),i("blockquote",[i("p",[t._v("Long-term recurrent convolutional networks for visual recognition and description")])]),t._v(" "),i("p",[t._v("这种方法通常使用CNN提取空间特征，使用RNN（如LSTM）提取时序特征，进行行为识别。")]),t._v(" "),i("h4",{attrs:{id:"beyond-short-snippets-2015-cvpr"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#beyond-short-snippets-2015-cvpr"}},[t._v("#")]),t._v(" Beyond short snippets（2015 CVPR）")]),t._v(" "),i("blockquote",[i("p",[t._v("Beyond Short Snippets: Deep Networks for Video Classification")])]),t._v(" "),i("h4",{attrs:{id:"twostreamfusion-2016-cvpr"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#twostreamfusion-2016-cvpr"}},[t._v("#")]),t._v(" TwoStreamFusion（2016 CVPR）")]),t._v(" "),i("blockquote",[i("p",[t._v("Convolutional Two-Stream Network Fusion for Video Action Recognition")])]),t._v(" "),i("ul",[i("li",[i("p",[i("strong",[t._v("解读：")])]),t._v(" "),i("ul",[i("li",[t._v("解决two stream的两个问题，一是不能在空间和时间特征之间学习像素级的对应关系，二是空域卷积只在单RGB帧上时域卷积只在堆叠的L个时序相邻的光流帧上，时间规模非常有限。")]),t._v(" "),i("li",[t._v("该文章通篇谈的是融合(Fusion)，关键阐释的是如何去融合空域卷积网络与时域卷积网络、在哪里融合这两个网络、如何在时域上融合网络三个问题。")]),t._v(" "),i("li",[i("a",{attrs:{href:"https://blog.csdn.net/u013588351/article/details/102074562?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2~default~OPENSEARCH~Rate-1.pc_relevant_aa&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~OPENSEARCH~Rate-1.pc_relevant_aa&utm_relevant_index=2",target:"_blank",rel:"noopener noreferrer"}},[t._v("【论文】Convolutional Two-Stream Network Fusion for Video Action Recognition_安静-CSDN博客"),i("OutboundLink")],1)])])])]),t._v(" "),i("h4",{attrs:{id:"tsn-2016-eccv"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#tsn-2016-eccv"}},[t._v("#")]),t._v(" TSN（2016 ECCV）")]),t._v(" "),i("blockquote",[i("p",[t._v("Temporal segment networks: Towards good practices for deep action recognition")])]),t._v(" "),i("ul",[i("li",[i("p",[i("strong",[t._v("解读：")])]),t._v(" "),i("ul",[i("li",[i("p",[i("a",{attrs:{href:"https://blog.csdn.net/u014380165/article/details/79029309",target:"_blank",rel:"noopener noreferrer"}},[t._v("TSN(Temporal Segment Networks)算法笔记_AI之路-CSDN博客_tsn模型"),i("OutboundLink")],1)])]),t._v(" "),i("li",[i("p",[i("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/84598874",target:"_blank",rel:"noopener noreferrer"}},[t._v("视频理解-Temporal Segment Network TSN - 知乎 (zhihu.com)"),i("OutboundLink")],1)])])])])]),t._v(" "),i("h4",{attrs:{id:"stpn-2017-cvpr"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#stpn-2017-cvpr"}},[t._v("#")]),t._v(" STPN（2017 CVPR）")]),t._v(" "),i("blockquote",[i("p",[t._v("Spatiotemporal Pyramid Network for Video Action Recognition")])]),t._v(" "),i("h4",{attrs:{id:"lattice-lstm-2017-iccv"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#lattice-lstm-2017-iccv"}},[t._v("#")]),t._v(" Lattice LSTM（2017 ICCV）")]),t._v(" "),i("blockquote",[i("p",[t._v("Lattice Long Short-Term Memory for Human Action Recognition")])]),t._v(" "),i("h4",{attrs:{id:"trn-2018-eccv"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#trn-2018-eccv"}},[t._v("#")]),t._v(" TRN（2018 ECCV）")]),t._v(" "),i("blockquote",[i("p",[t._v("Temporal Relational Reasoning in Videos")])]),t._v(" "),i("ul",[i("li",[i("p",[i("strong",[t._v("解读：")])]),t._v(" "),i("ul",[i("li",[t._v("时间关系推理（Temporal relational reasoning）是指理解物体／实体在时间域的变化关系的能力。")]),t._v(" "),i("li",[t._v("本文对TSN最后融合方式做一个改进，TSN每个snippet独立地预测，而TRN在预测前先进行snippet间的特征融合。另外TRN的输入用的是不同帧数的snippet(different scale)。")]),t._v(" "),i("li",[i("a",{attrs:{href:"https://blog.csdn.net/elaine_bao/article/details/80753506",target:"_blank",rel:"noopener noreferrer"}},[t._v("【论文笔记】视频分类系列 Temporal Relational Reasoning in Videos （TRN）_elaine_bao的专栏-CSDN博客"),i("OutboundLink")],1)])])])]),t._v(" "),i("h4",{attrs:{id:"tsm-2019-iccv"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#tsm-2019-iccv"}},[t._v("#")]),t._v(" TSM（2019 ICCV）")]),t._v(" "),i("blockquote",[i("p",[t._v("TSM: Temporal Shift Module for Efficient Video Understanding")])]),t._v(" "),i("h4",{attrs:{id:"lgd-2019-cvpr"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#lgd-2019-cvpr"}},[t._v("#")]),t._v(" LGD（2019 CVPR）")]),t._v(" "),i("blockquote",[i("p",[t._v("Learning Spatio-Temporal Representation with Local and Global Diffusion")])]),t._v(" "),i("h3",{attrs:{id:"_3-2-3d-conv"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#_3-2-3d-conv"}},[t._v("#")]),t._v(" 3.2 3D-Conv")]),t._v(" "),i("p",[t._v("3D convolution 直接将2D卷积扩展到3D（添加了时间维度），直接提取包含时间和空间两方面的特征。")]),t._v(" "),i("ul",[i("li",[i("p",[t._v("解读：")]),t._v(" "),i("p",[i("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/41659502",target:"_blank",rel:"noopener noreferrer"}},[t._v("论文笔记——基于的视频行为识别/行为识别算法笔记(三) - 知乎 (zhihu.com)"),i("OutboundLink")],1)])])]),t._v(" "),i("h4",{attrs:{id:"c3d-2015-iccv"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#c3d-2015-iccv"}},[t._v("#")]),t._v(" C3D（2015 ICCV）")]),t._v(" "),i("blockquote",[i("p",[t._v("Learning spatiotemporal features with 3d convolutional networks")])]),t._v(" "),i("h4",{attrs:{id:"i3d-2017-cvpr"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#i3d-2017-cvpr"}},[t._v("#")]),t._v(" I3D（2017 CVPR）")]),t._v(" "),i("blockquote",[i("p",[t._v("Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset")])]),t._v(" "),i("h4",{attrs:{id:"p3d-2017-iccv"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#p3d-2017-iccv"}},[t._v("#")]),t._v(" P3D（2017 ICCV）")]),t._v(" "),i("blockquote",[i("p",[t._v("Learning spatio-temporal representation with pseudo-3d residual networks")])]),t._v(" "),i("h4",{attrs:{id:"s3d-2018-eccv"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#s3d-2018-eccv"}},[t._v("#")]),t._v(" S3D（2018 ECCV）")]),t._v(" "),i("blockquote",[i("p",[t._v("Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification")])]),t._v(" "),i("h4",{attrs:{id:"r-2-1-d-2018-cvpr"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#r-2-1-d-2018-cvpr"}},[t._v("#")]),t._v(" R(2+1)D（2018 CVPR）")]),t._v(" "),i("blockquote",[i("p",[t._v("A Closer Look at Spatiotemporal Convolutions for Action Recognition")])]),t._v(" "),i("h4",{attrs:{id:"resnet3d-2018-cvpr"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#resnet3d-2018-cvpr"}},[t._v("#")]),t._v(" ResNet3D（2018 CVPR）")]),t._v(" "),i("blockquote",[i("p",[t._v("Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?")])]),t._v(" "),i("h4",{attrs:{id:"non-local-2018-cvpr"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#non-local-2018-cvpr"}},[t._v("#")]),t._v(" Non-Local（2018 CVPR）")]),t._v(" "),i("blockquote",[i("p",[t._v("Non-Local Neural Networks")])]),t._v(" "),i("h4",{attrs:{id:"slowfast-2019-iccv"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#slowfast-2019-iccv"}},[t._v("#")]),t._v(" SlowFast（2019 ICCV）")]),t._v(" "),i("blockquote",[i("p",[t._v("SlowFast Networks for Video Recognition")])]),t._v(" "),i("h4",{attrs:{id:"x3d-2020-cvpr"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#x3d-2020-cvpr"}},[t._v("#")]),t._v(" X3D（2020 CVPR）")]),t._v(" "),i("blockquote",[i("p",[t._v("X3D: Expanding Architectures for Efficient Video Recognition")])]),t._v(" "),i("h3",{attrs:{id:"_3-3-efficient-video-modeling"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#_3-3-efficient-video-modeling"}},[t._v("#")]),t._v(" 3.3 Efficient Video Modeling")]),t._v(" "),i("h4",{attrs:{id:"hidden-tsn-2018-accv"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#hidden-tsn-2018-accv"}},[t._v("#")]),t._v(" Hidden TSN（2018 ACCV）")]),t._v(" "),i("blockquote",[i("p",[t._v("Hidden Two-Stream Convolutional Networks for Action Recognition")])]),t._v(" "),i("h4",{attrs:{id:"mars-2019-cvpr"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#mars-2019-cvpr"}},[t._v("#")]),t._v(" MARS（2019 CVPR）")]),t._v(" "),i("blockquote",[i("p",[t._v("MARS: Motion-Augmented RGB Stream for Action Recognition")])]),t._v(" "),i("h4",{attrs:{id:"tea-2020-cvpr"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#tea-2020-cvpr"}},[t._v("#")]),t._v(" TEA（2020 CVPR）")]),t._v(" "),i("blockquote",[i("p",[t._v("TEA: Temporal Excitation and Aggregation for Action Recognition")])]),t._v(" "),i("h3",{attrs:{id:"_3-4-miscellaneous"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#_3-4-miscellaneous"}},[t._v("#")]),t._v(" 3.4 Miscellaneous")]),t._v(" "),i("h4",{attrs:{id:"grp-2016-cvpr"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#grp-2016-cvpr"}},[t._v("#")]),t._v(" GRP（2016 CVPR）")]),t._v(" "),i("blockquote",[i("p",[t._v("Generalized Rank Pooling for Activity Recognition")])]),t._v(" "),i("h4",{attrs:{id:"adascan-2017-cvpr"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#adascan-2017-cvpr"}},[t._v("#")]),t._v(" AdaScan（2017 CVPR）")]),t._v(" "),i("blockquote",[i("p",[t._v("AdaScan: Adaptive Scan Pooling in Deep Convolutional Neural Networks for Human Action Recognition in Videos")])]),t._v(" "),i("h4",{attrs:{id:"trajectorycnn-2018-neurips"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#trajectorycnn-2018-neurips"}},[t._v("#")]),t._v(" TrajectoryCNN（2018 NeurIPS）")]),t._v(" "),i("blockquote",[i("p",[t._v("Trajectory Convolution for Action Recognition")])]),t._v(" "),i("h4",{attrs:{id:"coviar-2018-cvpr"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#coviar-2018-cvpr"}},[t._v("#")]),t._v(" CoViAR（2018 CVPR）")]),t._v(" "),i("blockquote",[i("p",[t._v("Compressed Video Action Recognition")])]),t._v(" "),i("h4",{attrs:{id:"tpn-2020-cvpr"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#tpn-2020-cvpr"}},[t._v("#")]),t._v(" TPN（2020 CVPR）")]),t._v(" "),i("blockquote",[i("p",[t._v("Temporal Pyramid Network for Action Recognition")])]),t._v(" "),i("h3",{attrs:{id:"_3-5-transformer-based"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#_3-5-transformer-based"}},[t._v("#")]),t._v(" 3.5 Transformer-based")]),t._v(" "),i("h4",{attrs:{id:"vidtr-2021-iccv"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#vidtr-2021-iccv"}},[t._v("#")]),t._v(" VidTr（2021 ICCV）")]),t._v(" "),i("blockquote",[i("p",[t._v("VidTr: Video Transformer Without Convolutions")])]),t._v(" "),i("h4",{attrs:{id:"vivit-2021-iccv"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#vivit-2021-iccv"}},[t._v("#")]),t._v(" ViViT（2021 ICCV）")]),t._v(" "),i("blockquote",[i("p",[t._v("ViViT: A Video Vision Transformer")])]),t._v(" "),i("h4",{attrs:{id:"mvit-b-32x3-2021-iccv"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#mvit-b-32x3-2021-iccv"}},[t._v("#")]),t._v(" MViT-B, 32x3（2021 ICCV）")]),t._v(" "),i("blockquote",[i("p",[t._v("Multiscale Vision Transformers")])]),t._v(" "),i("h4",{attrs:{id:"mformer-hr-2021neurips"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#mformer-hr-2021neurips"}},[t._v("#")]),t._v(" Mformer-HR（2021NeurIPS）")]),t._v(" "),i("blockquote",[i("p",[t._v("Keeping Your Eye on the Ball: Trajectory Attention in Video Transformers")])]),t._v(" "),i("h4",{attrs:{id:"mvit-l-b-2021"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#mvit-l-b-2021"}},[t._v("#")]),t._v(" MViT-L/B（2021）")]),t._v(" "),i("blockquote",[i("p",[t._v("Improved Multiscale Vision Transformers for Classification and Detection")])]),t._v(" "),i("h4",{attrs:{id:"x-vit-2021-neurips"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#x-vit-2021-neurips"}},[t._v("#")]),t._v(" X-Vit（2021 NeurIPS）")]),t._v(" "),i("blockquote",[i("p",[t._v("Space-time Mixing Attention for Video Transformer")])]),t._v(" "),i("h4",{attrs:{id:"rsanet-r50-2021-neurips"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#rsanet-r50-2021-neurips"}},[t._v("#")]),t._v(" RSANet-R50（2021 NeurIPS）")]),t._v(" "),i("blockquote",[i("p",[t._v("Relational Self-Attention: What's Missing in Attention for Video Understanding")])]),t._v(" "),i("h4",{attrs:{id:"uniformer-2022-iclr"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#uniformer-2022-iclr"}},[t._v("#")]),t._v(" UniFormer（2022 ICLR）")]),t._v(" "),i("blockquote",[i("p",[t._v("UniFormer: Unified Transformer for Efficient Spatial-Temporal Representation Learning")])]),t._v(" "),i("h3",{attrs:{id:"_3-6-others-最新"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#_3-6-others-最新"}},[t._v("#")]),t._v(" 3.6 others（最新）")]),t._v(" "),i("h4",{attrs:{id:"omnisource-2020-eccv"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#omnisource-2020-eccv"}},[t._v("#")]),t._v(" OmniSource（2020 ECCV）")]),t._v(" "),i("blockquote",[i("p",[t._v("Omni-sourced Webly-supervised Learning for Video Recognition")])]),t._v(" "),i("h4",{attrs:{id:"hatnet-2020-eccv"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#hatnet-2020-eccv"}},[t._v("#")]),t._v(" HATNet（2020 ECCV）")]),t._v(" "),i("blockquote",[i("p",[t._v("Large Scale Holistic Video Understanding")])]),t._v(" "),i("h4",{attrs:{id:"smart-2021-aaai"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#smart-2021-aaai"}},[t._v("#")]),t._v(" SMART（2021 AAAI）")]),t._v(" "),i("blockquote",[i("p",[t._v("SMART Frame Selection for Action Recognition")])]),t._v(" "),i("h4",{attrs:{id:"morphmlp-2021"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#morphmlp-2021"}},[t._v("#")]),t._v(" MorphMLP（2021）")]),t._v(" "),i("blockquote",[i("p",[t._v("MorphMLP: A Self-Attention Free, MLP-Like Backbone for Image and Video")])]),t._v(" "),i("h4",{attrs:{id:"action-net-2021-cvpr"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#action-net-2021-cvpr"}},[t._v("#")]),t._v(" ACTION-Net（2021 CVPR）")]),t._v(" "),i("blockquote",[i("p",[t._v("ACTION-Net: Multipath Excitation for Action Recognition")])]),t._v(" "),i("h4",{attrs:{id:"movinets-2021-cvpr"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#movinets-2021-cvpr"}},[t._v("#")]),t._v(" MoViNets（2021 CVPR）")]),t._v(" "),i("blockquote",[i("p",[t._v("MoViNets: Mobile Video Networks for Efficient Video Recognition")])]),t._v(" "),i("h4",{attrs:{id:"tdn-2021-cvpr"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#tdn-2021-cvpr"}},[t._v("#")]),t._v(" TDN（2021 CVPR）")]),t._v(" "),i("blockquote",[i("p",[t._v("TDN: Temporal Difference Networks for Efficient Action Recognition")])]),t._v(" "),i("h4",{attrs:{id:"selfynet-tsm-r50en-2021-iccv"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#selfynet-tsm-r50en-2021-iccv"}},[t._v("#")]),t._v(" SELFYNet-TSM-R50En（2021 ICCV）")]),t._v(" "),i("blockquote",[i("p",[t._v("Learning Self-Similarity in Space and Time as Generalized Motion for Video Action Recognition")])]),t._v(" "),i("h4",{attrs:{id:"ct-net-2021-iclr"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#ct-net-2021-iclr"}},[t._v("#")]),t._v(" CT-Net（2021 ICLR）")]),t._v(" "),i("blockquote",[i("p",[t._v("CT-Net: Channel Tensorization Network for Video Classification")])]),t._v(" "),i("h2",{attrs:{id:"_4-自监督-对抗学习-多模态等"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#_4-自监督-对抗学习-多模态等"}},[t._v("#")]),t._v(" 4. 自监督/对抗学习/多模态等")]),t._v(" "),i("h3",{attrs:{id:"_4-1-deep-hal-with-odf-sdf-2021-acm-mm"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#_4-1-deep-hal-with-odf-sdf-2021-acm-mm"}},[t._v("#")]),t._v(" 4.1 DEEP-HAL with ODF+SDF（2021 ACM MM）")]),t._v(" "),i("blockquote",[i("p",[t._v("Self-supervising Action Recognition by Statistical Moment and Subspace Descriptors")])]),t._v(" "),i("h3",{attrs:{id:"_4-2-videomoco-2021-cvpr"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#_4-2-videomoco-2021-cvpr"}},[t._v("#")]),t._v(" 4.2 VideoMoCo（2021 CVPR）")]),t._v(" "),i("blockquote",[i("p",[t._v("VideoMoCo: Contrastive Video Representation Learning with Temporally Adversarial Examples")])]),t._v(" "),i("h3",{attrs:{id:"_4-3-vimpac-2021"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#_4-3-vimpac-2021"}},[t._v("#")]),t._v(" 4.3 VIMPAC（2021）")]),t._v(" "),i("blockquote",[i("p",[t._v("VIMPAC: Video Pre-Training via Masked Token Prediction and Contrastive Learning")])]),t._v(" "),i("h3",{attrs:{id:"_4-4-maskfeat-2021"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#_4-4-maskfeat-2021"}},[t._v("#")]),t._v(" 4.4 MaskFeat（2021）")]),t._v(" "),i("blockquote",[i("p",[t._v("Masked Feature Prediction for Self-Supervised Visual Pre-Training")])]),t._v(" "),i("h2",{attrs:{id:"_5-rgb数据集"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#_5-rgb数据集"}},[t._v("#")]),t._v(" 5. RGB数据集")]),t._v(" "),i("p",[t._v("对于视频行为识别任务，数据集通常是通过以下过程构建的：")]),t._v(" "),i("p",[t._v("（1）定义一个行为列表，将以前行为识别数据集的标签组合起来，并根据用例添加新的类别。")]),t._v(" "),i("p",[t._v("（2） 通过将视频标题/副标题与行为列表匹配，从各种来源获取视频，如YouTube和电影。")]),t._v(" "),i("p",[t._v("（3） 手动提供时间注释，以指示操作的开始和结束位置；")]),t._v(" "),i("p",[t._v("（4）通过重复数据消除和过滤掉嘈杂的类/样本，最终清理数据集。")]),t._v(" "),i("img",{staticStyle:{zoom:"80%"},attrs:{src:o(455),alt:"image-20220315201728223"}}),t._v(" "),i("h3",{attrs:{id:"_5-1-hmdb-51"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#_5-1-hmdb-51"}},[t._v("#")]),t._v(" 5.1 HMDB-51")]),t._v(" "),i("p",[t._v("于2011年推出。它主要来自电影，还有一小部分来自公共数据库，如Prelinger archive、YouTube和谷歌视频。该数据集包含"),i("strong",[t._v("6849")]),t._v("个剪辑，分为"),i("strong",[t._v("51")]),t._v("个行为类别，每个类别至少包含101个剪辑。该数据集有三个官方拆分。大多数以前的论文要么报告"),i("strong",[t._v("top-1精度")]),t._v("，要么报告"),i("strong",[t._v("三次分类的平均精度。")])]),t._v(" "),i("h3",{attrs:{id:"_5-2-ucf-101"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#_5-2-ucf-101"}},[t._v("#")]),t._v(" 5.2 UCF-101")]),t._v(" "),i("p",[t._v("目前行为识别最常使用的数据集之一，于2012年推出，是之前UCF50数据集的扩展。它包含了13320个来自Y ouTube的视频，涵盖101类人类行为。该数据集有三个类似于HMDB51的官方拆分，也以相同的方式进行评估。")]),t._v(" "),i("h3",{attrs:{id:"_5-3-something-something-v1-v2"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#_5-3-something-something-v1-v2"}},[t._v("#")]),t._v(" 5.3 Something-Something V1/V2")]),t._v(" "),i("p",[t._v("2017年推出了V1，2018年推出了V2。这个家族是另一个流行的基准，由"),i("strong",[t._v("174")]),t._v("个行为类组成，描述人类对日常物品执行基本行为。V1中有108499个视频，V2中有220847个视频。注意，Something-Something数据集"),i("strong",[t._v("需要强大的时间建模")]),t._v("，因为大多数活动不能仅基于空间特征推断（例如打开某物、用某物覆盖某物）。")]),t._v(" "),i("h3",{attrs:{id:"_5-4-charades"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#_5-4-charades"}},[t._v("#")]),t._v(" 5.4 Charades")]),t._v(" "),i("p",[t._v("在2016年作为一个数据集引入，用于现实生活中的"),i("strong",[t._v("并发动作理解")]),t._v("。它包含9848个视频，平均长度为30秒。该数据集包括157项多标签日常室内活动，由267名不同的人进行。它有一个正式的训练验证部分，其中7985个视频用于训练，其余1863个用于验证。")]),t._v(" "),i("h3",{attrs:{id:"数据集特征分析"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#数据集特征分析"}},[t._v("#")]),t._v(" 数据集特征分析")]),t._v(" "),i("p",[t._v("流行视频行为数据集的视觉示例")]),t._v(" "),i("img",{staticStyle:{zoom:"80%"},attrs:{src:o(456),alt:"image-20220315205819378"}}),t._v(" "),i("p",[i("strong",[t._v("不同数据集有不同特征：")])]),t._v(" "),i("ul",[i("li",[t._v("在前两行中，我们从"),i("strong",[t._v("UCF101和Kinetics400数据集")]),t._v("中选择行为类。有趣的是，我们发现，这些"),i("strong",[t._v("行为有时可以由上下文或场景单独决定")]),t._v("。例如，该模型可以预测骑自行车的行为，只要它在视频帧中识别出一辆自行车。该模型还可以预测板球保龄球的行为，如果它能识别板球场。因此，对于这些类别，视频行为识别可能成为一个对象/场景分类问题，而不需要推理运动/时间信息。")]),t._v(" "),i("li",[t._v("在中间的两行中，我们从"),i("strong",[t._v("something-something v2数据集")]),t._v("中挑选行为类。该数据集侧重于人机交互，因此更细粒度，**需要强大的时间建模，不能仅通过第一帧来区分这两个行为。**例如，如果我们只看丢东西和捡东西的第一帧，而不看其他视频帧，就不可能区分这两个行为。")]),t._v(" "),i("li",[t._v("在最下面一行中，我们从Moments in time数据集中选择行为类。该数据集不同于大多数视频行为识别数据集，其设计目的是具有较大的类间和类内变化，以表示不同抽象级别的动态事件。即在不同的环境中，同样的行动可能会有不同的参与者。 例如，行为攀岩可以在不同的环境（楼梯或树）中有不同的参与者（人或动物）")])]),t._v(" "),i("h2",{attrs:{id:"_6-其他"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#_6-其他"}},[t._v("#")]),t._v(" 6. 其他")]),t._v(" "),i("h3",{attrs:{id:"_6-1-光流"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#_6-1-光流"}},[t._v("#")]),t._v(" 6.1 光流")]),t._v(" "),i("ul",[i("li",[i("p",[i("strong",[t._v("光流")]),t._v("是空间运动物体在"),i("strong",[t._v("观察成像平面")]),t._v("上的像素运动的"),i("strong",[t._v("瞬时速度")]),t._v("，是利用图像序列中像素在时间域上的变化以及相邻帧之间的"),i("strong",[t._v("相关性")]),t._v("来找到上一帧跟当前帧之间存在的对应关系，从而计算出相邻帧之间物体的运动信息的一种方法。")])]),t._v(" "),i("li",[i("p",[t._v("光流之所以生效是依赖于这几个假设：")]),t._v(" "),i("ol",[i("li",[t._v("物体的像素强度不会在连续帧之间改变；")]),t._v(" "),i("li",[t._v("一张图像中相邻的像素具有相似的运动。")])])]),t._v(" "),i("li",[i("p",[i("strong",[t._v("光流的计算方法")])]),t._v(" "),i("p",[t._v("假设第一帧图像中的像素 "),i("em",[t._v("I(x, y, t)")]),t._v(" 在时间 "),i("em",[t._v("dt")]),t._v(" 后移动到第二帧图像的 "),i("em",[t._v("(x+dx, y+dy)")]),t._v(" 处。根据上述第一条假设：灰度值不变，我们可以得到：")]),t._v(" "),i("img",{staticStyle:{zoom:"80%"},attrs:{src:o(457),alt:"image-20220221153601761"}}),t._v(" "),i("p",[t._v("对等号右侧进行泰勒级数展开，消去相同项，两边都除以 "),i("em",[t._v("dt")]),t._v(" ，得到如下方程：")]),t._v(" "),i("img",{staticStyle:{zoom:"80%"},attrs:{src:o(458),alt:"image-20220221153623441"}}),t._v(" "),i("p",[i("img",{attrs:{src:"picture/image-20220221153635509.png",alt:"image-20220221153635509"}})]),t._v(" "),i("p",[t._v("fx,fy均可由图像数据求得，而**(u,v)即为所求光流矢量**。")]),t._v(" "),i("p",[t._v("上述一个等式中有两个未知数。有几个方法可以解决这个问题，其中的一个是 Lucas-Kanade 法 。增加有一个假设：")]),t._v(" "),i("p",[t._v("这里就要用到上面提到的第二个假设条件，领域内的所有像素点具有相同的运动。Lucas-Kanade法就是利用一个3x3的领域中的9个像素点具有相同的运动，就可以得到9个点的光流方程(即上述公式)，用这些方程来求得*(u, v)* 这两个未知数，显然这是个约束条件过多的方程组，不能解得精确解，一个好的解决方法就是使用最小二乘来拟合。")]),t._v(" "),i("p",[t._v("opencv提供函数计算，参考"),i("a",{attrs:{href:"https://blog.csdn.net/xiao_lxl/article/details/95330541",target:"_blank",rel:"noopener noreferrer"}},[t._v("OpenCV小例程——光流法_xiao_lxl的专栏-CSDN博客_opencv 光流算法"),i("OutboundLink")],1)])])])])}),[],!1,null,null,null);e.default=r.exports}}]);
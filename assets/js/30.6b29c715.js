(window.webpackJsonp=window.webpackJsonp||[]).push([[30],{753:function(r,e,t){"use strict";t.r(e);var a=t(1),n=Object(a.a)({},(function(){var r=this,e=r.$createElement,t=r._self._c||e;return t("ContentSlotsDistributor",{attrs:{"slot-key":r.$parent.slotKey}},[t("h1",{attrs:{id:"video2script"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#video2script"}},[r._v("#")]),r._v(" Video2Script")]),r._v(" "),t("h2",{attrs:{id:"基本概念"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#基本概念"}},[r._v("#")]),r._v(" 基本概念")]),r._v(" "),t("h2",{attrs:{id:"文献调研"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#文献调研"}},[r._v("#")]),r._v(" 文献调研")]),r._v(" "),t("ul",[t("li",[t("p",[r._v("VideoChat")]),r._v(" "),t("p",[t("a",{attrs:{href:"http://arxiv.org/abs/2305.06355",target:"_blank",rel:"noopener noreferrer"}},[r._v("[2305.06355] VideoChat: Chat-Centric Video Understanding"),t("OutboundLink")],1)]),r._v(" "),t("p",[r._v("【双流：逐帧+视频】")])]),r._v(" "),t("li",[t("p",[r._v("MVBench（VideoChat2）")]),r._v(" "),t("p",[t("a",{attrs:{href:"http://arxiv.org/abs/2311.17005",target:"_blank",rel:"noopener noreferrer"}},[r._v("[2311.17005] MVBench: A Comprehensive Multi-modal Video Understanding Benchmark"),t("OutboundLink")],1)])]),r._v(" "),t("li",[t("p",[r._v("Dolphin")]),r._v(" "),t("p",[t("a",{attrs:{href:"https://github.com/kaleido-lab/dolphin",target:"_blank",rel:"noopener noreferrer"}},[r._v("GitHub - kaleido-lab/dolphin: General video interaction platform based on LLMs, including Video ChatGPT"),t("OutboundLink")],1)]),r._v(" "),t("p",[r._v("【开源项目】")])]),r._v(" "),t("li")]),r._v(" "),t("h2",{attrs:{id:"相关工作"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#相关工作"}},[r._v("#")]),r._v(" 相关工作")]),r._v(" "),t("h2",{attrs:{id:"其他工作"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#其他工作"}},[r._v("#")]),r._v(" 其他工作")]),r._v(" "),t("ul",[t("li",[t("p",[r._v("GRiT")]),r._v(" "),t("p",[t("a",{attrs:{href:"https://arxiv.org/abs/2212.00280",target:"_blank",rel:"noopener noreferrer"}},[r._v("[2212.00280] GRiT: A Generative Region-to-text Transformer for Object Understanding (arxiv.org)"),t("OutboundLink")],1)])]),r._v(" "),t("li",[t("p",[r._v("Dense Video Object Captioning from Disjoint Supervision")]),r._v(" "),t("p",[t("a",{attrs:{href:"https://arxiv.org/abs/2306.11729",target:"_blank",rel:"noopener noreferrer"}},[r._v("[2306.11729] Dense Video Object Captioning from Disjoint Supervision (arxiv.org)"),t("OutboundLink")],1)])]),r._v(" "),t("li",[t("p",[r._v("A Challenging Multimodal Video Summary: Simultaneously Extracting and Generating Keyframe-Caption Pairs from Video")]),r._v(" "),t("p",[t("a",{attrs:{href:"https://arxiv.org/abs/2312.01575",target:"_blank",rel:"noopener noreferrer"}},[r._v("[2312.01575] A Challenging Multimodal Video Summary: Simultaneously Extracting and Generating Keyframe-Caption Pairs from Video (arxiv.org)"),t("OutboundLink")],1)])]),r._v(" "),t("li",[t("p",[r._v("Vid2Seq（CVPR-23）")]),r._v(" "),t("p",[t("a",{attrs:{href:"https://arxiv.org/abs/2302.14115",target:"_blank",rel:"noopener noreferrer"}},[r._v("[2302.14115] Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning (arxiv.org)"),t("OutboundLink")],1)])]),r._v(" "),t("li",[t("p",[r._v("VidChapters-7M（NIPS-23）")]),r._v(" "),t("p",[t("a",{attrs:{href:"https://arxiv.org/abs/2309.13952",target:"_blank",rel:"noopener noreferrer"}},[r._v("[2309.13952] VidChapters-7M: Video Chapters at Scale (arxiv.org)"),t("OutboundLink")],1)])]),r._v(" "),t("li")])])}),[],!1,null,null,null);e.default=n.exports}}]);